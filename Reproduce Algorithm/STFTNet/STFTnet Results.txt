

[STFTnet] Initial input shape: torch.Size([32, 120, 7])
[STFTnet] Shape after permuting for conv: torch.Size([32, 7, 120])
  [MFA Block] Input shape: torch.Size([32, 7, 120])
  [MFA Block] After global average pooling shape: torch.Size([32, 7, 1])
  [MFA Block] Attention weights shape: torch.Size([32, 7, 1])
  [MFA Block] Output after attention weighting shape: torch.Size([32, 7, 120])
[STFTnet] Shape after channel projection conv: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Input shape: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Output shape: torch.Size([32, 64, 120])
[STFTnet] Shape after permuting for transformer: torch.Size([32, 120, 64])
  [Transformer Encoder] Input shape (N, L, C): torch.Size([32, 120, 64])
  [Transformer Encoder] Output shape: torch.Size([32, 120, 64])
[STFTnet] Shape after taking last time step: torch.Size([32, 64])
  [Classifier] Input shape: torch.Size([32, 64])
  [Classifier] Output shape: torch.Size([32, 12])

[STFTnet] Initial input shape: torch.Size([32, 120, 7])
[STFTnet] Shape after permuting for conv: torch.Size([32, 7, 120])
  [MFA Block] Input shape: torch.Size([32, 7, 120])
  [MFA Block] After global average pooling shape: torch.Size([32, 7, 1])
  [MFA Block] Attention weights shape: torch.Size([32, 7, 1])
  [MFA Block] Output after attention weighting shape: torch.Size([32, 7, 120])
[STFTnet] Shape after channel projection conv: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Input shape: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Output shape: torch.Size([32, 64, 120])
[STFTnet] Shape after permuting for transformer: torch.Size([32, 120, 64])
  [Transformer Encoder] Input shape (N, L, C): torch.Size([32, 120, 64])
  [Transformer Encoder] Output shape: torch.Size([32, 120, 64])
[STFTnet] Shape after taking last time step: torch.Size([32, 64])
  [Classifier] Input shape: torch.Size([32, 64])
  [Classifier] Output shape: torch.Size([32, 12])

[STFTnet] Initial input shape: torch.Size([32, 120, 7])
[STFTnet] Shape after permuting for conv: torch.Size([32, 7, 120])
  [MFA Block] Input shape: torch.Size([32, 7, 120])
  [MFA Block] After global average pooling shape: torch.Size([32, 7, 1])
  [MFA Block] Attention weights shape: torch.Size([32, 7, 1])
  [MFA Block] Output after attention weighting shape: torch.Size([32, 7, 120])
[STFTnet] Shape after channel projection conv: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Input shape: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Output shape: torch.Size([32, 64, 120])
[STFTnet] Shape after permuting for transformer: torch.Size([32, 120, 64])
  [Transformer Encoder] Input shape (N, L, C): torch.Size([32, 120, 64])
  [Transformer Encoder] Output shape: torch.Size([32, 120, 64])
[STFTnet] Shape after taking last time step: torch.Size([32, 64])
  [Classifier] Input shape: torch.Size([32, 64])
  [Classifier] Output shape: torch.Size([32, 12])

[STFTnet] Initial input shape: torch.Size([32, 120, 7])
[STFTnet] Shape after permuting for conv: torch.Size([32, 7, 120])
  [MFA Block] Input shape: torch.Size([32, 7, 120])
  [MFA Block] After global average pooling shape: torch.Size([32, 7, 1])
  [MFA Block] Attention weights shape: torch.Size([32, 7, 1])
  [MFA Block] Output after attention weighting shape: torch.Size([32, 7, 120])
[STFTnet] Shape after channel projection conv: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Input shape: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Output shape: torch.Size([32, 64, 120])
[STFTnet] Shape after permuting for transformer: torch.Size([32, 120, 64])
  [Transformer Encoder] Input shape (N, L, C): torch.Size([32, 120, 64])
  [Transformer Encoder] Output shape: torch.Size([32, 120, 64])
[STFTnet] Shape after taking last time step: torch.Size([32, 64])
  [Classifier] Input shape: torch.Size([32, 64])
  [Classifier] Output shape: torch.Size([32, 12])

[STFTnet] Initial input shape: torch.Size([32, 120, 7])
[STFTnet] Shape after permuting for conv: torch.Size([32, 7, 120])
  [MFA Block] Input shape: torch.Size([32, 7, 120])
  [MFA Block] After global average pooling shape: torch.Size([32, 7, 1])
  [MFA Block] Attention weights shape: torch.Size([32, 7, 1])
  [MFA Block] Output after attention weighting shape: torch.Size([32, 7, 120])
[STFTnet] Shape after channel projection conv: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Input shape: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Output shape: torch.Size([32, 64, 120])
[STFTnet] Shape after permuting for transformer: torch.Size([32, 120, 64])
  [Transformer Encoder] Input shape (N, L, C): torch.Size([32, 120, 64])
  [Transformer Encoder] Output shape: torch.Size([32, 120, 64])
[STFTnet] Shape after taking last time step: torch.Size([32, 64])
  [Classifier] Input shape: torch.Size([32, 64])
  [Classifier] Output shape: torch.Size([32, 12])

[STFTnet] Initial input shape: torch.Size([32, 120, 7])
[STFTnet] Shape after permuting for conv: torch.Size([32, 7, 120])
  [MFA Block] Input shape: torch.Size([32, 7, 120])
  [MFA Block] After global average pooling shape: torch.Size([32, 7, 1])
  [MFA Block] Attention weights shape: torch.Size([32, 7, 1])
  [MFA Block] Output after attention weighting shape: torch.Size([32, 7, 120])
[STFTnet] Shape after channel projection conv: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Input shape: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Output shape: torch.Size([32, 64, 120])
[STFTnet] Shape after permuting for transformer: torch.Size([32, 120, 64])
  [Transformer Encoder] Input shape (N, L, C): torch.Size([32, 120, 64])
  [Transformer Encoder] Output shape: torch.Size([32, 120, 64])
[STFTnet] Shape after taking last time step: torch.Size([32, 64])
  [Classifier] Input shape: torch.Size([32, 64])
  [Classifier] Output shape: torch.Size([32, 12])

--- Completed a few batches for validation purposes; ending this epoch early. ---

========================================================
Epoch 1, Loss: 2.4895, Accuracy: 0.0781
========================================================


[STFTnet] Initial input shape: torch.Size([32, 120, 7])
[STFTnet] Shape after permuting for conv: torch.Size([32, 7, 120])
  [MFA Block] Input shape: torch.Size([32, 7, 120])
  [MFA Block] After global average pooling shape: torch.Size([32, 7, 1])
  [MFA Block] Attention weights shape: torch.Size([32, 7, 1])
  [MFA Block] Output after attention weighting shape: torch.Size([32, 7, 120])
[STFTnet] Shape after channel projection conv: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Input shape: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Output shape: torch.Size([32, 64, 120])
[STFTnet] Shape after permuting for transformer: torch.Size([32, 120, 64])
  [Transformer Encoder] Input shape (N, L, C): torch.Size([32, 120, 64])
  [Transformer Encoder] Output shape: torch.Size([32, 120, 64])
[STFTnet] Shape after taking last time step: torch.Size([32, 64])
  [Classifier] Input shape: torch.Size([32, 64])
  [Classifier] Output shape: torch.Size([32, 12])

[STFTnet] Initial input shape: torch.Size([32, 120, 7])
[STFTnet] Shape after permuting for conv: torch.Size([32, 7, 120])
  [MFA Block] Input shape: torch.Size([32, 7, 120])
  [MFA Block] After global average pooling shape: torch.Size([32, 7, 1])
  [MFA Block] Attention weights shape: torch.Size([32, 7, 1])
  [MFA Block] Output after attention weighting shape: torch.Size([32, 7, 120])
[STFTnet] Shape after channel projection conv: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Input shape: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Output shape: torch.Size([32, 64, 120])
[STFTnet] Shape after permuting for transformer: torch.Size([32, 120, 64])
  [Transformer Encoder] Input shape (N, L, C): torch.Size([32, 120, 64])
  [Transformer Encoder] Output shape: torch.Size([32, 120, 64])
[STFTnet] Shape after taking last time step: torch.Size([32, 64])
  [Classifier] Input shape: torch.Size([32, 64])
  [Classifier] Output shape: torch.Size([32, 12])

[STFTnet] Initial input shape: torch.Size([32, 120, 7])
[STFTnet] Shape after permuting for conv: torch.Size([32, 7, 120])
  [MFA Block] Input shape: torch.Size([32, 7, 120])
  [MFA Block] After global average pooling shape: torch.Size([32, 7, 1])
  [MFA Block] Attention weights shape: torch.Size([32, 7, 1])
  [MFA Block] Output after attention weighting shape: torch.Size([32, 7, 120])
[STFTnet] Shape after channel projection conv: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Input shape: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Output shape: torch.Size([32, 64, 120])
[STFTnet] Shape after permuting for transformer: torch.Size([32, 120, 64])
  [Transformer Encoder] Input shape (N, L, C): torch.Size([32, 120, 64])
  [Transformer Encoder] Output shape: torch.Size([32, 120, 64])
[STFTnet] Shape after taking last time step: torch.Size([32, 64])
  [Classifier] Input shape: torch.Size([32, 64])
  [Classifier] Output shape: torch.Size([32, 12])

[STFTnet] Initial input shape: torch.Size([32, 120, 7])
[STFTnet] Shape after permuting for conv: torch.Size([32, 7, 120])
  [MFA Block] Input shape: torch.Size([32, 7, 120])
  [MFA Block] After global average pooling shape: torch.Size([32, 7, 1])
  [MFA Block] Attention weights shape: torch.Size([32, 7, 1])
  [MFA Block] Output after attention weighting shape: torch.Size([32, 7, 120])
[STFTnet] Shape after channel projection conv: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Input shape: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Output shape: torch.Size([32, 64, 120])
[STFTnet] Shape after permuting for transformer: torch.Size([32, 120, 64])
  [Transformer Encoder] Input shape (N, L, C): torch.Size([32, 120, 64])
  [Transformer Encoder] Output shape: torch.Size([32, 120, 64])
[STFTnet] Shape after taking last time step: torch.Size([32, 64])
  [Classifier] Input shape: torch.Size([32, 64])
  [Classifier] Output shape: torch.Size([32, 12])

[STFTnet] Initial input shape: torch.Size([32, 120, 7])
[STFTnet] Shape after permuting for conv: torch.Size([32, 7, 120])
  [MFA Block] Input shape: torch.Size([32, 7, 120])
  [MFA Block] After global average pooling shape: torch.Size([32, 7, 1])
  [MFA Block] Attention weights shape: torch.Size([32, 7, 1])
  [MFA Block] Output after attention weighting shape: torch.Size([32, 7, 120])
[STFTnet] Shape after channel projection conv: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Input shape: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Output shape: torch.Size([32, 64, 120])
[STFTnet] Shape after permuting for transformer: torch.Size([32, 120, 64])
  [Transformer Encoder] Input shape (N, L, C): torch.Size([32, 120, 64])
  [Transformer Encoder] Output shape: torch.Size([32, 120, 64])
[STFTnet] Shape after taking last time step: torch.Size([32, 64])
  [Classifier] Input shape: torch.Size([32, 64])
  [Classifier] Output shape: torch.Size([32, 12])

[STFTnet] Initial input shape: torch.Size([32, 120, 7])
[STFTnet] Shape after permuting for conv: torch.Size([32, 7, 120])
  [MFA Block] Input shape: torch.Size([32, 7, 120])
  [MFA Block] After global average pooling shape: torch.Size([32, 7, 1])
  [MFA Block] Attention weights shape: torch.Size([32, 7, 1])
  [MFA Block] Output after attention weighting shape: torch.Size([32, 7, 120])
[STFTnet] Shape after channel projection conv: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Input shape: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Output shape: torch.Size([32, 64, 120])
[STFTnet] Shape after permuting for transformer: torch.Size([32, 120, 64])
  [Transformer Encoder] Input shape (N, L, C): torch.Size([32, 120, 64])
  [Transformer Encoder] Output shape: torch.Size([32, 120, 64])
[STFTnet] Shape after taking last time step: torch.Size([32, 64])
  [Classifier] Input shape: torch.Size([32, 64])
  [Classifier] Output shape: torch.Size([32, 12])

--- Completed a few batches for validation purposes; ending this epoch early. ---

========================================================
Epoch 2, Loss: 2.4818, Accuracy: 0.0938
========================================================


[STFTnet] Initial input shape: torch.Size([32, 120, 7])
[STFTnet] Shape after permuting for conv: torch.Size([32, 7, 120])
  [MFA Block] Input shape: torch.Size([32, 7, 120])
  [MFA Block] After global average pooling shape: torch.Size([32, 7, 1])
  [MFA Block] Attention weights shape: torch.Size([32, 7, 1])
  [MFA Block] Output after attention weighting shape: torch.Size([32, 7, 120])
[STFTnet] Shape after channel projection conv: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Input shape: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Output shape: torch.Size([32, 64, 120])
[STFTnet] Shape after permuting for transformer: torch.Size([32, 120, 64])
  [Transformer Encoder] Input shape (N, L, C): torch.Size([32, 120, 64])
  [Transformer Encoder] Output shape: torch.Size([32, 120, 64])
[STFTnet] Shape after taking last time step: torch.Size([32, 64])
  [Classifier] Input shape: torch.Size([32, 64])
  [Classifier] Output shape: torch.Size([32, 12])

[STFTnet] Initial input shape: torch.Size([32, 120, 7])
[STFTnet] Shape after permuting for conv: torch.Size([32, 7, 120])
  [MFA Block] Input shape: torch.Size([32, 7, 120])
  [MFA Block] After global average pooling shape: torch.Size([32, 7, 1])
  [MFA Block] Attention weights shape: torch.Size([32, 7, 1])
  [MFA Block] Output after attention weighting shape: torch.Size([32, 7, 120])
[STFTnet] Shape after channel projection conv: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Input shape: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Output shape: torch.Size([32, 64, 120])
[STFTnet] Shape after permuting for transformer: torch.Size([32, 120, 64])
  [Transformer Encoder] Input shape (N, L, C): torch.Size([32, 120, 64])
  [Transformer Encoder] Output shape: torch.Size([32, 120, 64])
[STFTnet] Shape after taking last time step: torch.Size([32, 64])
  [Classifier] Input shape: torch.Size([32, 64])
  [Classifier] Output shape: torch.Size([32, 12])

[STFTnet] Initial input shape: torch.Size([32, 120, 7])
[STFTnet] Shape after permuting for conv: torch.Size([32, 7, 120])
  [MFA Block] Input shape: torch.Size([32, 7, 120])
  [MFA Block] After global average pooling shape: torch.Size([32, 7, 1])
  [MFA Block] Attention weights shape: torch.Size([32, 7, 1])
  [MFA Block] Output after attention weighting shape: torch.Size([32, 7, 120])
[STFTnet] Shape after channel projection conv: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Input shape: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Output shape: torch.Size([32, 64, 120])
[STFTnet] Shape after permuting for transformer: torch.Size([32, 120, 64])
  [Transformer Encoder] Input shape (N, L, C): torch.Size([32, 120, 64])
  [Transformer Encoder] Output shape: torch.Size([32, 120, 64])
[STFTnet] Shape after taking last time step: torch.Size([32, 64])
  [Classifier] Input shape: torch.Size([32, 64])
  [Classifier] Output shape: torch.Size([32, 12])

[STFTnet] Initial input shape: torch.Size([32, 120, 7])
[STFTnet] Shape after permuting for conv: torch.Size([32, 7, 120])
  [MFA Block] Input shape: torch.Size([32, 7, 120])
  [MFA Block] After global average pooling shape: torch.Size([32, 7, 1])
  [MFA Block] Attention weights shape: torch.Size([32, 7, 1])
  [MFA Block] Output after attention weighting shape: torch.Size([32, 7, 120])
[STFTnet] Shape after channel projection conv: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Input shape: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Output shape: torch.Size([32, 64, 120])
[STFTnet] Shape after permuting for transformer: torch.Size([32, 120, 64])
  [Transformer Encoder] Input shape (N, L, C): torch.Size([32, 120, 64])
  [Transformer Encoder] Output shape: torch.Size([32, 120, 64])
[STFTnet] Shape after taking last time step: torch.Size([32, 64])
  [Classifier] Input shape: torch.Size([32, 64])
  [Classifier] Output shape: torch.Size([32, 12])

[STFTnet] Initial input shape: torch.Size([32, 120, 7])
[STFTnet] Shape after permuting for conv: torch.Size([32, 7, 120])
  [MFA Block] Input shape: torch.Size([32, 7, 120])
  [MFA Block] After global average pooling shape: torch.Size([32, 7, 1])
  [MFA Block] Attention weights shape: torch.Size([32, 7, 1])
  [MFA Block] Output after attention weighting shape: torch.Size([32, 7, 120])
[STFTnet] Shape after channel projection conv: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Input shape: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Output shape: torch.Size([32, 64, 120])
[STFTnet] Shape after permuting for transformer: torch.Size([32, 120, 64])
  [Transformer Encoder] Input shape (N, L, C): torch.Size([32, 120, 64])
  [Transformer Encoder] Output shape: torch.Size([32, 120, 64])
[STFTnet] Shape after taking last time step: torch.Size([32, 64])
  [Classifier] Input shape: torch.Size([32, 64])
  [Classifier] Output shape: torch.Size([32, 12])

[STFTnet] Initial input shape: torch.Size([32, 120, 7])
[STFTnet] Shape after permuting for conv: torch.Size([32, 7, 120])
  [MFA Block] Input shape: torch.Size([32, 7, 120])
  [MFA Block] After global average pooling shape: torch.Size([32, 7, 1])
  [MFA Block] Attention weights shape: torch.Size([32, 7, 1])
  [MFA Block] Output after attention weighting shape: torch.Size([32, 7, 120])
[STFTnet] Shape after channel projection conv: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Input shape: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Output shape: torch.Size([32, 64, 120])
[STFTnet] Shape after permuting for transformer: torch.Size([32, 120, 64])
  [Transformer Encoder] Input shape (N, L, C): torch.Size([32, 120, 64])
  [Transformer Encoder] Output shape: torch.Size([32, 120, 64])
[STFTnet] Shape after taking last time step: torch.Size([32, 64])
  [Classifier] Input shape: torch.Size([32, 64])
  [Classifier] Output shape: torch.Size([32, 12])

--- Completed a few batches for validation purposes; ending this epoch early. ---

========================================================
Epoch 3, Loss: 2.4788, Accuracy: 0.1302
========================================================


[STFTnet] Initial input shape: torch.Size([32, 120, 7])
[STFTnet] Shape after permuting for conv: torch.Size([32, 7, 120])
  [MFA Block] Input shape: torch.Size([32, 7, 120])
  [MFA Block] After global average pooling shape: torch.Size([32, 7, 1])
  [MFA Block] Attention weights shape: torch.Size([32, 7, 1])
  [MFA Block] Output after attention weighting shape: torch.Size([32, 7, 120])
[STFTnet] Shape after channel projection conv: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Input shape: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Output shape: torch.Size([32, 64, 120])
[STFTnet] Shape after permuting for transformer: torch.Size([32, 120, 64])
  [Transformer Encoder] Input shape (N, L, C): torch.Size([32, 120, 64])
  [Transformer Encoder] Output shape: torch.Size([32, 120, 64])
[STFTnet] Shape after taking last time step: torch.Size([32, 64])
  [Classifier] Input shape: torch.Size([32, 64])
  [Classifier] Output shape: torch.Size([32, 12])

[STFTnet] Initial input shape: torch.Size([32, 120, 7])
[STFTnet] Shape after permuting for conv: torch.Size([32, 7, 120])
  [MFA Block] Input shape: torch.Size([32, 7, 120])
  [MFA Block] After global average pooling shape: torch.Size([32, 7, 1])
  [MFA Block] Attention weights shape: torch.Size([32, 7, 1])
  [MFA Block] Output after attention weighting shape: torch.Size([32, 7, 120])
[STFTnet] Shape after channel projection conv: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Input shape: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Output shape: torch.Size([32, 64, 120])
[STFTnet] Shape after permuting for transformer: torch.Size([32, 120, 64])
  [Transformer Encoder] Input shape (N, L, C): torch.Size([32, 120, 64])
  [Transformer Encoder] Output shape: torch.Size([32, 120, 64])
[STFTnet] Shape after taking last time step: torch.Size([32, 64])
  [Classifier] Input shape: torch.Size([32, 64])
  [Classifier] Output shape: torch.Size([32, 12])

[STFTnet] Initial input shape: torch.Size([32, 120, 7])
[STFTnet] Shape after permuting for conv: torch.Size([32, 7, 120])
  [MFA Block] Input shape: torch.Size([32, 7, 120])
  [MFA Block] After global average pooling shape: torch.Size([32, 7, 1])
  [MFA Block] Attention weights shape: torch.Size([32, 7, 1])
  [MFA Block] Output after attention weighting shape: torch.Size([32, 7, 120])
[STFTnet] Shape after channel projection conv: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Input shape: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Output shape: torch.Size([32, 64, 120])
[STFTnet] Shape after permuting for transformer: torch.Size([32, 120, 64])
  [Transformer Encoder] Input shape (N, L, C): torch.Size([32, 120, 64])
  [Transformer Encoder] Output shape: torch.Size([32, 120, 64])
[STFTnet] Shape after taking last time step: torch.Size([32, 64])
  [Classifier] Input shape: torch.Size([32, 64])
  [Classifier] Output shape: torch.Size([32, 12])

[STFTnet] Initial input shape: torch.Size([32, 120, 7])
[STFTnet] Shape after permuting for conv: torch.Size([32, 7, 120])
  [MFA Block] Input shape: torch.Size([32, 7, 120])
  [MFA Block] After global average pooling shape: torch.Size([32, 7, 1])
  [MFA Block] Attention weights shape: torch.Size([32, 7, 1])
  [MFA Block] Output after attention weighting shape: torch.Size([32, 7, 120])
[STFTnet] Shape after channel projection conv: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Input shape: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Output shape: torch.Size([32, 64, 120])
[STFTnet] Shape after permuting for transformer: torch.Size([32, 120, 64])
  [Transformer Encoder] Input shape (N, L, C): torch.Size([32, 120, 64])
  [Transformer Encoder] Output shape: torch.Size([32, 120, 64])
[STFTnet] Shape after taking last time step: torch.Size([32, 64])
  [Classifier] Input shape: torch.Size([32, 64])
  [Classifier] Output shape: torch.Size([32, 12])

[STFTnet] Initial input shape: torch.Size([32, 120, 7])
[STFTnet] Shape after permuting for conv: torch.Size([32, 7, 120])
  [MFA Block] Input shape: torch.Size([32, 7, 120])
  [MFA Block] After global average pooling shape: torch.Size([32, 7, 1])
  [MFA Block] Attention weights shape: torch.Size([32, 7, 1])
  [MFA Block] Output after attention weighting shape: torch.Size([32, 7, 120])
[STFTnet] Shape after channel projection conv: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Input shape: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Output shape: torch.Size([32, 64, 120])
[STFTnet] Shape after permuting for transformer: torch.Size([32, 120, 64])
  [Transformer Encoder] Input shape (N, L, C): torch.Size([32, 120, 64])
  [Transformer Encoder] Output shape: torch.Size([32, 120, 64])
[STFTnet] Shape after taking last time step: torch.Size([32, 64])
  [Classifier] Input shape: torch.Size([32, 64])
  [Classifier] Output shape: torch.Size([32, 12])

[STFTnet] Initial input shape: torch.Size([32, 120, 7])
[STFTnet] Shape after permuting for conv: torch.Size([32, 7, 120])
  [MFA Block] Input shape: torch.Size([32, 7, 120])
  [MFA Block] After global average pooling shape: torch.Size([32, 7, 1])
  [MFA Block] Attention weights shape: torch.Size([32, 7, 1])
  [MFA Block] Output after attention weighting shape: torch.Size([32, 7, 120])
[STFTnet] Shape after channel projection conv: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Input shape: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Output shape: torch.Size([32, 64, 120])
[STFTnet] Shape after permuting for transformer: torch.Size([32, 120, 64])
  [Transformer Encoder] Input shape (N, L, C): torch.Size([32, 120, 64])
  [Transformer Encoder] Output shape: torch.Size([32, 120, 64])
[STFTnet] Shape after taking last time step: torch.Size([32, 64])
  [Classifier] Input shape: torch.Size([32, 64])
  [Classifier] Output shape: torch.Size([32, 12])

--- Completed a few batches for validation purposes; ending this epoch early. ---

========================================================
Epoch 4, Loss: 2.4695, Accuracy: 0.1094
========================================================


[STFTnet] Initial input shape: torch.Size([32, 120, 7])
[STFTnet] Shape after permuting for conv: torch.Size([32, 7, 120])
  [MFA Block] Input shape: torch.Size([32, 7, 120])
  [MFA Block] After global average pooling shape: torch.Size([32, 7, 1])
  [MFA Block] Attention weights shape: torch.Size([32, 7, 1])
  [MFA Block] Output after attention weighting shape: torch.Size([32, 7, 120])
[STFTnet] Shape after channel projection conv: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Input shape: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Output shape: torch.Size([32, 64, 120])
[STFTnet] Shape after permuting for transformer: torch.Size([32, 120, 64])
  [Transformer Encoder] Input shape (N, L, C): torch.Size([32, 120, 64])
  [Transformer Encoder] Output shape: torch.Size([32, 120, 64])
[STFTnet] Shape after taking last time step: torch.Size([32, 64])
  [Classifier] Input shape: torch.Size([32, 64])
  [Classifier] Output shape: torch.Size([32, 12])

[STFTnet] Initial input shape: torch.Size([32, 120, 7])
[STFTnet] Shape after permuting for conv: torch.Size([32, 7, 120])
  [MFA Block] Input shape: torch.Size([32, 7, 120])
  [MFA Block] After global average pooling shape: torch.Size([32, 7, 1])
  [MFA Block] Attention weights shape: torch.Size([32, 7, 1])
  [MFA Block] Output after attention weighting shape: torch.Size([32, 7, 120])
[STFTnet] Shape after channel projection conv: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Input shape: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Output shape: torch.Size([32, 64, 120])
[STFTnet] Shape after permuting for transformer: torch.Size([32, 120, 64])
  [Transformer Encoder] Input shape (N, L, C): torch.Size([32, 120, 64])
  [Transformer Encoder] Output shape: torch.Size([32, 120, 64])
[STFTnet] Shape after taking last time step: torch.Size([32, 64])
  [Classifier] Input shape: torch.Size([32, 64])
  [Classifier] Output shape: torch.Size([32, 12])

[STFTnet] Initial input shape: torch.Size([32, 120, 7])
[STFTnet] Shape after permuting for conv: torch.Size([32, 7, 120])
  [MFA Block] Input shape: torch.Size([32, 7, 120])
  [MFA Block] After global average pooling shape: torch.Size([32, 7, 1])
  [MFA Block] Attention weights shape: torch.Size([32, 7, 1])
  [MFA Block] Output after attention weighting shape: torch.Size([32, 7, 120])
[STFTnet] Shape after channel projection conv: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Input shape: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Output shape: torch.Size([32, 64, 120])
[STFTnet] Shape after permuting for transformer: torch.Size([32, 120, 64])
  [Transformer Encoder] Input shape (N, L, C): torch.Size([32, 120, 64])
  [Transformer Encoder] Output shape: torch.Size([32, 120, 64])
[STFTnet] Shape after taking last time step: torch.Size([32, 64])
  [Classifier] Input shape: torch.Size([32, 64])
  [Classifier] Output shape: torch.Size([32, 12])

[STFTnet] Initial input shape: torch.Size([32, 120, 7])
[STFTnet] Shape after permuting for conv: torch.Size([32, 7, 120])
  [MFA Block] Input shape: torch.Size([32, 7, 120])
  [MFA Block] After global average pooling shape: torch.Size([32, 7, 1])
  [MFA Block] Attention weights shape: torch.Size([32, 7, 1])
  [MFA Block] Output after attention weighting shape: torch.Size([32, 7, 120])
[STFTnet] Shape after channel projection conv: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Input shape: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Output shape: torch.Size([32, 64, 120])
[STFTnet] Shape after permuting for transformer: torch.Size([32, 120, 64])
  [Transformer Encoder] Input shape (N, L, C): torch.Size([32, 120, 64])
  [Transformer Encoder] Output shape: torch.Size([32, 120, 64])
[STFTnet] Shape after taking last time step: torch.Size([32, 64])
  [Classifier] Input shape: torch.Size([32, 64])
  [Classifier] Output shape: torch.Size([32, 12])

[STFTnet] Initial input shape: torch.Size([32, 120, 7])
[STFTnet] Shape after permuting for conv: torch.Size([32, 7, 120])
  [MFA Block] Input shape: torch.Size([32, 7, 120])
  [MFA Block] After global average pooling shape: torch.Size([32, 7, 1])
  [MFA Block] Attention weights shape: torch.Size([32, 7, 1])
  [MFA Block] Output after attention weighting shape: torch.Size([32, 7, 120])
[STFTnet] Shape after channel projection conv: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Input shape: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Output shape: torch.Size([32, 64, 120])
[STFTnet] Shape after permuting for transformer: torch.Size([32, 120, 64])
  [Transformer Encoder] Input shape (N, L, C): torch.Size([32, 120, 64])
  [Transformer Encoder] Output shape: torch.Size([32, 120, 64])
[STFTnet] Shape after taking last time step: torch.Size([32, 64])
  [Classifier] Input shape: torch.Size([32, 64])
  [Classifier] Output shape: torch.Size([32, 12])

[STFTnet] Initial input shape: torch.Size([32, 120, 7])
[STFTnet] Shape after permuting for conv: torch.Size([32, 7, 120])
  [MFA Block] Input shape: torch.Size([32, 7, 120])
  [MFA Block] After global average pooling shape: torch.Size([32, 7, 1])
  [MFA Block] Attention weights shape: torch.Size([32, 7, 1])
  [MFA Block] Output after attention weighting shape: torch.Size([32, 7, 120])
[STFTnet] Shape after channel projection conv: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Input shape: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Output shape: torch.Size([32, 64, 120])
[STFTnet] Shape after permuting for transformer: torch.Size([32, 120, 64])
  [Transformer Encoder] Input shape (N, L, C): torch.Size([32, 120, 64])
  [Transformer Encoder] Output shape: torch.Size([32, 120, 64])
[STFTnet] Shape after taking last time step: torch.Size([32, 64])
  [Classifier] Input shape: torch.Size([32, 64])
  [Classifier] Output shape: torch.Size([32, 12])

--- Completed a few batches for validation purposes; ending this epoch early. ---

========================================================
Epoch 5, Loss: 2.4316, Accuracy: 0.2135
========================================================


[STFTnet] Initial input shape: torch.Size([32, 120, 7])
[STFTnet] Shape after permuting for conv: torch.Size([32, 7, 120])
  [MFA Block] Input shape: torch.Size([32, 7, 120])
  [MFA Block] After global average pooling shape: torch.Size([32, 7, 1])
  [MFA Block] Attention weights shape: torch.Size([32, 7, 1])
  [MFA Block] Output after attention weighting shape: torch.Size([32, 7, 120])
[STFTnet] Shape after channel projection conv: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Input shape: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Output shape: torch.Size([32, 64, 120])
[STFTnet] Shape after permuting for transformer: torch.Size([32, 120, 64])
  [Transformer Encoder] Input shape (N, L, C): torch.Size([32, 120, 64])
  [Transformer Encoder] Output shape: torch.Size([32, 120, 64])
[STFTnet] Shape after taking last time step: torch.Size([32, 64])
  [Classifier] Input shape: torch.Size([32, 64])
  [Classifier] Output shape: torch.Size([32, 12])

[STFTnet] Initial input shape: torch.Size([32, 120, 7])
[STFTnet] Shape after permuting for conv: torch.Size([32, 7, 120])
  [MFA Block] Input shape: torch.Size([32, 7, 120])
  [MFA Block] After global average pooling shape: torch.Size([32, 7, 1])
  [MFA Block] Attention weights shape: torch.Size([32, 7, 1])
  [MFA Block] Output after attention weighting shape: torch.Size([32, 7, 120])
[STFTnet] Shape after channel projection conv: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Input shape: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Output shape: torch.Size([32, 64, 120])
[STFTnet] Shape after permuting for transformer: torch.Size([32, 120, 64])
  [Transformer Encoder] Input shape (N, L, C): torch.Size([32, 120, 64])
  [Transformer Encoder] Output shape: torch.Size([32, 120, 64])
[STFTnet] Shape after taking last time step: torch.Size([32, 64])
  [Classifier] Input shape: torch.Size([32, 64])
  [Classifier] Output shape: torch.Size([32, 12])

[STFTnet] Initial input shape: torch.Size([32, 120, 7])
[STFTnet] Shape after permuting for conv: torch.Size([32, 7, 120])
  [MFA Block] Input shape: torch.Size([32, 7, 120])
  [MFA Block] After global average pooling shape: torch.Size([32, 7, 1])
  [MFA Block] Attention weights shape: torch.Size([32, 7, 1])
  [MFA Block] Output after attention weighting shape: torch.Size([32, 7, 120])
[STFTnet] Shape after channel projection conv: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Input shape: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Output shape: torch.Size([32, 64, 120])
[STFTnet] Shape after permuting for transformer: torch.Size([32, 120, 64])
  [Transformer Encoder] Input shape (N, L, C): torch.Size([32, 120, 64])
  [Transformer Encoder] Output shape: torch.Size([32, 120, 64])
[STFTnet] Shape after taking last time step: torch.Size([32, 64])
  [Classifier] Input shape: torch.Size([32, 64])
  [Classifier] Output shape: torch.Size([32, 12])

[STFTnet] Initial input shape: torch.Size([32, 120, 7])
[STFTnet] Shape after permuting for conv: torch.Size([32, 7, 120])
  [MFA Block] Input shape: torch.Size([32, 7, 120])
  [MFA Block] After global average pooling shape: torch.Size([32, 7, 1])
  [MFA Block] Attention weights shape: torch.Size([32, 7, 1])
  [MFA Block] Output after attention weighting shape: torch.Size([32, 7, 120])
[STFTnet] Shape after channel projection conv: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Input shape: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Output shape: torch.Size([32, 64, 120])
[STFTnet] Shape after permuting for transformer: torch.Size([32, 120, 64])
  [Transformer Encoder] Input shape (N, L, C): torch.Size([32, 120, 64])
  [Transformer Encoder] Output shape: torch.Size([32, 120, 64])
[STFTnet] Shape after taking last time step: torch.Size([32, 64])
  [Classifier] Input shape: torch.Size([32, 64])
  [Classifier] Output shape: torch.Size([32, 12])

[STFTnet] Initial input shape: torch.Size([32, 120, 7])
[STFTnet] Shape after permuting for conv: torch.Size([32, 7, 120])
  [MFA Block] Input shape: torch.Size([32, 7, 120])
  [MFA Block] After global average pooling shape: torch.Size([32, 7, 1])
  [MFA Block] Attention weights shape: torch.Size([32, 7, 1])
  [MFA Block] Output after attention weighting shape: torch.Size([32, 7, 120])
[STFTnet] Shape after channel projection conv: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Input shape: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Output shape: torch.Size([32, 64, 120])
[STFTnet] Shape after permuting for transformer: torch.Size([32, 120, 64])
  [Transformer Encoder] Input shape (N, L, C): torch.Size([32, 120, 64])
  [Transformer Encoder] Output shape: torch.Size([32, 120, 64])
[STFTnet] Shape after taking last time step: torch.Size([32, 64])
  [Classifier] Input shape: torch.Size([32, 64])
  [Classifier] Output shape: torch.Size([32, 12])

[STFTnet] Initial input shape: torch.Size([32, 120, 7])
[STFTnet] Shape after permuting for conv: torch.Size([32, 7, 120])
  [MFA Block] Input shape: torch.Size([32, 7, 120])
  [MFA Block] After global average pooling shape: torch.Size([32, 7, 1])
  [MFA Block] Attention weights shape: torch.Size([32, 7, 1])
  [MFA Block] Output after attention weighting shape: torch.Size([32, 7, 120])
[STFTnet] Shape after channel projection conv: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Input shape: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Output shape: torch.Size([32, 64, 120])
[STFTnet] Shape after permuting for transformer: torch.Size([32, 120, 64])
  [Transformer Encoder] Input shape (N, L, C): torch.Size([32, 120, 64])
  [Transformer Encoder] Output shape: torch.Size([32, 120, 64])
[STFTnet] Shape after taking last time step: torch.Size([32, 64])
  [Classifier] Input shape: torch.Size([32, 64])
  [Classifier] Output shape: torch.Size([32, 12])

--- Completed a few batches for validation purposes; ending this epoch early. ---

========================================================
Epoch 6, Loss: 2.3873, Accuracy: 0.2760
========================================================


[STFTnet] Initial input shape: torch.Size([32, 120, 7])
[STFTnet] Shape after permuting for conv: torch.Size([32, 7, 120])
  [MFA Block] Input shape: torch.Size([32, 7, 120])
  [MFA Block] After global average pooling shape: torch.Size([32, 7, 1])
  [MFA Block] Attention weights shape: torch.Size([32, 7, 1])
  [MFA Block] Output after attention weighting shape: torch.Size([32, 7, 120])
[STFTnet] Shape after channel projection conv: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Input shape: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Output shape: torch.Size([32, 64, 120])
[STFTnet] Shape after permuting for transformer: torch.Size([32, 120, 64])
  [Transformer Encoder] Input shape (N, L, C): torch.Size([32, 120, 64])
  [Transformer Encoder] Output shape: torch.Size([32, 120, 64])
[STFTnet] Shape after taking last time step: torch.Size([32, 64])
  [Classifier] Input shape: torch.Size([32, 64])
  [Classifier] Output shape: torch.Size([32, 12])

[STFTnet] Initial input shape: torch.Size([32, 120, 7])
[STFTnet] Shape after permuting for conv: torch.Size([32, 7, 120])
  [MFA Block] Input shape: torch.Size([32, 7, 120])
  [MFA Block] After global average pooling shape: torch.Size([32, 7, 1])
  [MFA Block] Attention weights shape: torch.Size([32, 7, 1])
  [MFA Block] Output after attention weighting shape: torch.Size([32, 7, 120])
[STFTnet] Shape after channel projection conv: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Input shape: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Output shape: torch.Size([32, 64, 120])
[STFTnet] Shape after permuting for transformer: torch.Size([32, 120, 64])
  [Transformer Encoder] Input shape (N, L, C): torch.Size([32, 120, 64])
  [Transformer Encoder] Output shape: torch.Size([32, 120, 64])
[STFTnet] Shape after taking last time step: torch.Size([32, 64])
  [Classifier] Input shape: torch.Size([32, 64])
  [Classifier] Output shape: torch.Size([32, 12])

[STFTnet] Initial input shape: torch.Size([32, 120, 7])
[STFTnet] Shape after permuting for conv: torch.Size([32, 7, 120])
  [MFA Block] Input shape: torch.Size([32, 7, 120])
  [MFA Block] After global average pooling shape: torch.Size([32, 7, 1])
  [MFA Block] Attention weights shape: torch.Size([32, 7, 1])
  [MFA Block] Output after attention weighting shape: torch.Size([32, 7, 120])
[STFTnet] Shape after channel projection conv: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Input shape: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Output shape: torch.Size([32, 64, 120])
[STFTnet] Shape after permuting for transformer: torch.Size([32, 120, 64])
  [Transformer Encoder] Input shape (N, L, C): torch.Size([32, 120, 64])
  [Transformer Encoder] Output shape: torch.Size([32, 120, 64])
[STFTnet] Shape after taking last time step: torch.Size([32, 64])
  [Classifier] Input shape: torch.Size([32, 64])
  [Classifier] Output shape: torch.Size([32, 12])

[STFTnet] Initial input shape: torch.Size([32, 120, 7])
[STFTnet] Shape after permuting for conv: torch.Size([32, 7, 120])
  [MFA Block] Input shape: torch.Size([32, 7, 120])
  [MFA Block] After global average pooling shape: torch.Size([32, 7, 1])
  [MFA Block] Attention weights shape: torch.Size([32, 7, 1])
  [MFA Block] Output after attention weighting shape: torch.Size([32, 7, 120])
[STFTnet] Shape after channel projection conv: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Input shape: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Output shape: torch.Size([32, 64, 120])
[STFTnet] Shape after permuting for transformer: torch.Size([32, 120, 64])
  [Transformer Encoder] Input shape (N, L, C): torch.Size([32, 120, 64])
  [Transformer Encoder] Output shape: torch.Size([32, 120, 64])
[STFTnet] Shape after taking last time step: torch.Size([32, 64])
  [Classifier] Input shape: torch.Size([32, 64])
  [Classifier] Output shape: torch.Size([32, 12])

[STFTnet] Initial input shape: torch.Size([32, 120, 7])
[STFTnet] Shape after permuting for conv: torch.Size([32, 7, 120])
  [MFA Block] Input shape: torch.Size([32, 7, 120])
  [MFA Block] After global average pooling shape: torch.Size([32, 7, 1])
  [MFA Block] Attention weights shape: torch.Size([32, 7, 1])
  [MFA Block] Output after attention weighting shape: torch.Size([32, 7, 120])
[STFTnet] Shape after channel projection conv: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Input shape: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Output shape: torch.Size([32, 64, 120])
[STFTnet] Shape after permuting for transformer: torch.Size([32, 120, 64])
  [Transformer Encoder] Input shape (N, L, C): torch.Size([32, 120, 64])
  [Transformer Encoder] Output shape: torch.Size([32, 120, 64])
[STFTnet] Shape after taking last time step: torch.Size([32, 64])
  [Classifier] Input shape: torch.Size([32, 64])
  [Classifier] Output shape: torch.Size([32, 12])

[STFTnet] Initial input shape: torch.Size([32, 120, 7])
[STFTnet] Shape after permuting for conv: torch.Size([32, 7, 120])
  [MFA Block] Input shape: torch.Size([32, 7, 120])
  [MFA Block] After global average pooling shape: torch.Size([32, 7, 1])
  [MFA Block] Attention weights shape: torch.Size([32, 7, 1])
  [MFA Block] Output after attention weighting shape: torch.Size([32, 7, 120])
[STFTnet] Shape after channel projection conv: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Input shape: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Output shape: torch.Size([32, 64, 120])
[STFTnet] Shape after permuting for transformer: torch.Size([32, 120, 64])
  [Transformer Encoder] Input shape (N, L, C): torch.Size([32, 120, 64])
  [Transformer Encoder] Output shape: torch.Size([32, 120, 64])
[STFTnet] Shape after taking last time step: torch.Size([32, 64])
  [Classifier] Input shape: torch.Size([32, 64])
  [Classifier] Output shape: torch.Size([32, 12])

--- Completed a few batches for validation purposes; ending this epoch early. ---

========================================================
Epoch 7, Loss: 2.2959, Accuracy: 0.4271
========================================================


[STFTnet] Initial input shape: torch.Size([32, 120, 7])
[STFTnet] Shape after permuting for conv: torch.Size([32, 7, 120])
  [MFA Block] Input shape: torch.Size([32, 7, 120])
  [MFA Block] After global average pooling shape: torch.Size([32, 7, 1])
  [MFA Block] Attention weights shape: torch.Size([32, 7, 1])
  [MFA Block] Output after attention weighting shape: torch.Size([32, 7, 120])
[STFTnet] Shape after channel projection conv: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Input shape: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Output shape: torch.Size([32, 64, 120])
[STFTnet] Shape after permuting for transformer: torch.Size([32, 120, 64])
  [Transformer Encoder] Input shape (N, L, C): torch.Size([32, 120, 64])
  [Transformer Encoder] Output shape: torch.Size([32, 120, 64])
[STFTnet] Shape after taking last time step: torch.Size([32, 64])
  [Classifier] Input shape: torch.Size([32, 64])
  [Classifier] Output shape: torch.Size([32, 12])

[STFTnet] Initial input shape: torch.Size([32, 120, 7])
[STFTnet] Shape after permuting for conv: torch.Size([32, 7, 120])
  [MFA Block] Input shape: torch.Size([32, 7, 120])
  [MFA Block] After global average pooling shape: torch.Size([32, 7, 1])
  [MFA Block] Attention weights shape: torch.Size([32, 7, 1])
  [MFA Block] Output after attention weighting shape: torch.Size([32, 7, 120])
[STFTnet] Shape after channel projection conv: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Input shape: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Output shape: torch.Size([32, 64, 120])
[STFTnet] Shape after permuting for transformer: torch.Size([32, 120, 64])
  [Transformer Encoder] Input shape (N, L, C): torch.Size([32, 120, 64])
  [Transformer Encoder] Output shape: torch.Size([32, 120, 64])
[STFTnet] Shape after taking last time step: torch.Size([32, 64])
  [Classifier] Input shape: torch.Size([32, 64])
  [Classifier] Output shape: torch.Size([32, 12])

[STFTnet] Initial input shape: torch.Size([32, 120, 7])
[STFTnet] Shape after permuting for conv: torch.Size([32, 7, 120])
  [MFA Block] Input shape: torch.Size([32, 7, 120])
  [MFA Block] After global average pooling shape: torch.Size([32, 7, 1])
  [MFA Block] Attention weights shape: torch.Size([32, 7, 1])
  [MFA Block] Output after attention weighting shape: torch.Size([32, 7, 120])
[STFTnet] Shape after channel projection conv: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Input shape: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Output shape: torch.Size([32, 64, 120])
[STFTnet] Shape after permuting for transformer: torch.Size([32, 120, 64])
  [Transformer Encoder] Input shape (N, L, C): torch.Size([32, 120, 64])
  [Transformer Encoder] Output shape: torch.Size([32, 120, 64])
[STFTnet] Shape after taking last time step: torch.Size([32, 64])
  [Classifier] Input shape: torch.Size([32, 64])
  [Classifier] Output shape: torch.Size([32, 12])

[STFTnet] Initial input shape: torch.Size([32, 120, 7])
[STFTnet] Shape after permuting for conv: torch.Size([32, 7, 120])
  [MFA Block] Input shape: torch.Size([32, 7, 120])
  [MFA Block] After global average pooling shape: torch.Size([32, 7, 1])
  [MFA Block] Attention weights shape: torch.Size([32, 7, 1])
  [MFA Block] Output after attention weighting shape: torch.Size([32, 7, 120])
[STFTnet] Shape after channel projection conv: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Input shape: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Output shape: torch.Size([32, 64, 120])
[STFTnet] Shape after permuting for transformer: torch.Size([32, 120, 64])
  [Transformer Encoder] Input shape (N, L, C): torch.Size([32, 120, 64])
  [Transformer Encoder] Output shape: torch.Size([32, 120, 64])
[STFTnet] Shape after taking last time step: torch.Size([32, 64])
  [Classifier] Input shape: torch.Size([32, 64])
  [Classifier] Output shape: torch.Size([32, 12])

[STFTnet] Initial input shape: torch.Size([32, 120, 7])
[STFTnet] Shape after permuting for conv: torch.Size([32, 7, 120])
  [MFA Block] Input shape: torch.Size([32, 7, 120])
  [MFA Block] After global average pooling shape: torch.Size([32, 7, 1])
  [MFA Block] Attention weights shape: torch.Size([32, 7, 1])
  [MFA Block] Output after attention weighting shape: torch.Size([32, 7, 120])
[STFTnet] Shape after channel projection conv: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Input shape: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Output shape: torch.Size([32, 64, 120])
[STFTnet] Shape after permuting for transformer: torch.Size([32, 120, 64])
  [Transformer Encoder] Input shape (N, L, C): torch.Size([32, 120, 64])
  [Transformer Encoder] Output shape: torch.Size([32, 120, 64])
[STFTnet] Shape after taking last time step: torch.Size([32, 64])
  [Classifier] Input shape: torch.Size([32, 64])
  [Classifier] Output shape: torch.Size([32, 12])

[STFTnet] Initial input shape: torch.Size([32, 120, 7])
[STFTnet] Shape after permuting for conv: torch.Size([32, 7, 120])
  [MFA Block] Input shape: torch.Size([32, 7, 120])
  [MFA Block] After global average pooling shape: torch.Size([32, 7, 1])
  [MFA Block] Attention weights shape: torch.Size([32, 7, 1])
  [MFA Block] Output after attention weighting shape: torch.Size([32, 7, 120])
[STFTnet] Shape after channel projection conv: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Input shape: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Output shape: torch.Size([32, 64, 120])
[STFTnet] Shape after permuting for transformer: torch.Size([32, 120, 64])
  [Transformer Encoder] Input shape (N, L, C): torch.Size([32, 120, 64])
  [Transformer Encoder] Output shape: torch.Size([32, 120, 64])
[STFTnet] Shape after taking last time step: torch.Size([32, 64])
  [Classifier] Input shape: torch.Size([32, 64])
  [Classifier] Output shape: torch.Size([32, 12])

--- Completed a few batches for validation purposes; ending this epoch early. ---

========================================================
Epoch 8, Loss: 2.2414, Accuracy: 0.4375
========================================================


[STFTnet] Initial input shape: torch.Size([32, 120, 7])
[STFTnet] Shape after permuting for conv: torch.Size([32, 7, 120])
  [MFA Block] Input shape: torch.Size([32, 7, 120])
  [MFA Block] After global average pooling shape: torch.Size([32, 7, 1])
  [MFA Block] Attention weights shape: torch.Size([32, 7, 1])
  [MFA Block] Output after attention weighting shape: torch.Size([32, 7, 120])
[STFTnet] Shape after channel projection conv: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Input shape: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Output shape: torch.Size([32, 64, 120])
[STFTnet] Shape after permuting for transformer: torch.Size([32, 120, 64])
  [Transformer Encoder] Input shape (N, L, C): torch.Size([32, 120, 64])
  [Transformer Encoder] Output shape: torch.Size([32, 120, 64])
[STFTnet] Shape after taking last time step: torch.Size([32, 64])
  [Classifier] Input shape: torch.Size([32, 64])
  [Classifier] Output shape: torch.Size([32, 12])

[STFTnet] Initial input shape: torch.Size([32, 120, 7])
[STFTnet] Shape after permuting for conv: torch.Size([32, 7, 120])
  [MFA Block] Input shape: torch.Size([32, 7, 120])
  [MFA Block] After global average pooling shape: torch.Size([32, 7, 1])
  [MFA Block] Attention weights shape: torch.Size([32, 7, 1])
  [MFA Block] Output after attention weighting shape: torch.Size([32, 7, 120])
[STFTnet] Shape after channel projection conv: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Input shape: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Output shape: torch.Size([32, 64, 120])
[STFTnet] Shape after permuting for transformer: torch.Size([32, 120, 64])
  [Transformer Encoder] Input shape (N, L, C): torch.Size([32, 120, 64])
  [Transformer Encoder] Output shape: torch.Size([32, 120, 64])
[STFTnet] Shape after taking last time step: torch.Size([32, 64])
  [Classifier] Input shape: torch.Size([32, 64])
  [Classifier] Output shape: torch.Size([32, 12])

[STFTnet] Initial input shape: torch.Size([32, 120, 7])
[STFTnet] Shape after permuting for conv: torch.Size([32, 7, 120])
  [MFA Block] Input shape: torch.Size([32, 7, 120])
  [MFA Block] After global average pooling shape: torch.Size([32, 7, 1])
  [MFA Block] Attention weights shape: torch.Size([32, 7, 1])
  [MFA Block] Output after attention weighting shape: torch.Size([32, 7, 120])
[STFTnet] Shape after channel projection conv: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Input shape: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Output shape: torch.Size([32, 64, 120])
[STFTnet] Shape after permuting for transformer: torch.Size([32, 120, 64])
  [Transformer Encoder] Input shape (N, L, C): torch.Size([32, 120, 64])
  [Transformer Encoder] Output shape: torch.Size([32, 120, 64])
[STFTnet] Shape after taking last time step: torch.Size([32, 64])
  [Classifier] Input shape: torch.Size([32, 64])
  [Classifier] Output shape: torch.Size([32, 12])

[STFTnet] Initial input shape: torch.Size([32, 120, 7])
[STFTnet] Shape after permuting for conv: torch.Size([32, 7, 120])
  [MFA Block] Input shape: torch.Size([32, 7, 120])
  [MFA Block] After global average pooling shape: torch.Size([32, 7, 1])
  [MFA Block] Attention weights shape: torch.Size([32, 7, 1])
  [MFA Block] Output after attention weighting shape: torch.Size([32, 7, 120])
[STFTnet] Shape after channel projection conv: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Input shape: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Output shape: torch.Size([32, 64, 120])
[STFTnet] Shape after permuting for transformer: torch.Size([32, 120, 64])
  [Transformer Encoder] Input shape (N, L, C): torch.Size([32, 120, 64])
  [Transformer Encoder] Output shape: torch.Size([32, 120, 64])
[STFTnet] Shape after taking last time step: torch.Size([32, 64])
  [Classifier] Input shape: torch.Size([32, 64])
  [Classifier] Output shape: torch.Size([32, 12])

[STFTnet] Initial input shape: torch.Size([32, 120, 7])
[STFTnet] Shape after permuting for conv: torch.Size([32, 7, 120])
  [MFA Block] Input shape: torch.Size([32, 7, 120])
  [MFA Block] After global average pooling shape: torch.Size([32, 7, 1])
  [MFA Block] Attention weights shape: torch.Size([32, 7, 1])
  [MFA Block] Output after attention weighting shape: torch.Size([32, 7, 120])
[STFTnet] Shape after channel projection conv: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Input shape: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Output shape: torch.Size([32, 64, 120])
[STFTnet] Shape after permuting for transformer: torch.Size([32, 120, 64])
  [Transformer Encoder] Input shape (N, L, C): torch.Size([32, 120, 64])
  [Transformer Encoder] Output shape: torch.Size([32, 120, 64])
[STFTnet] Shape after taking last time step: torch.Size([32, 64])
  [Classifier] Input shape: torch.Size([32, 64])
  [Classifier] Output shape: torch.Size([32, 12])

[STFTnet] Initial input shape: torch.Size([32, 120, 7])
[STFTnet] Shape after permuting for conv: torch.Size([32, 7, 120])
  [MFA Block] Input shape: torch.Size([32, 7, 120])
  [MFA Block] After global average pooling shape: torch.Size([32, 7, 1])
  [MFA Block] Attention weights shape: torch.Size([32, 7, 1])
  [MFA Block] Output after attention weighting shape: torch.Size([32, 7, 120])
[STFTnet] Shape after channel projection conv: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Input shape: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Output shape: torch.Size([32, 64, 120])
[STFTnet] Shape after permuting for transformer: torch.Size([32, 120, 64])
  [Transformer Encoder] Input shape (N, L, C): torch.Size([32, 120, 64])
  [Transformer Encoder] Output shape: torch.Size([32, 120, 64])
[STFTnet] Shape after taking last time step: torch.Size([32, 64])
  [Classifier] Input shape: torch.Size([32, 64])
  [Classifier] Output shape: torch.Size([32, 12])

--- Completed a few batches for validation purposes; ending this epoch early. ---

========================================================
Epoch 9, Loss: 2.1853, Accuracy: 0.5312
========================================================


[STFTnet] Initial input shape: torch.Size([32, 120, 7])
[STFTnet] Shape after permuting for conv: torch.Size([32, 7, 120])
  [MFA Block] Input shape: torch.Size([32, 7, 120])
  [MFA Block] After global average pooling shape: torch.Size([32, 7, 1])
  [MFA Block] Attention weights shape: torch.Size([32, 7, 1])
  [MFA Block] Output after attention weighting shape: torch.Size([32, 7, 120])
[STFTnet] Shape after channel projection conv: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Input shape: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Output shape: torch.Size([32, 64, 120])
[STFTnet] Shape after permuting for transformer: torch.Size([32, 120, 64])
  [Transformer Encoder] Input shape (N, L, C): torch.Size([32, 120, 64])
  [Transformer Encoder] Output shape: torch.Size([32, 120, 64])
[STFTnet] Shape after taking last time step: torch.Size([32, 64])
  [Classifier] Input shape: torch.Size([32, 64])
  [Classifier] Output shape: torch.Size([32, 12])

[STFTnet] Initial input shape: torch.Size([32, 120, 7])
[STFTnet] Shape after permuting for conv: torch.Size([32, 7, 120])
  [MFA Block] Input shape: torch.Size([32, 7, 120])
  [MFA Block] After global average pooling shape: torch.Size([32, 7, 1])
  [MFA Block] Attention weights shape: torch.Size([32, 7, 1])
  [MFA Block] Output after attention weighting shape: torch.Size([32, 7, 120])
[STFTnet] Shape after channel projection conv: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Input shape: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Output shape: torch.Size([32, 64, 120])
[STFTnet] Shape after permuting for transformer: torch.Size([32, 120, 64])
  [Transformer Encoder] Input shape (N, L, C): torch.Size([32, 120, 64])
  [Transformer Encoder] Output shape: torch.Size([32, 120, 64])
[STFTnet] Shape after taking last time step: torch.Size([32, 64])
  [Classifier] Input shape: torch.Size([32, 64])
  [Classifier] Output shape: torch.Size([32, 12])

[STFTnet] Initial input shape: torch.Size([32, 120, 7])
[STFTnet] Shape after permuting for conv: torch.Size([32, 7, 120])
  [MFA Block] Input shape: torch.Size([32, 7, 120])
  [MFA Block] After global average pooling shape: torch.Size([32, 7, 1])
  [MFA Block] Attention weights shape: torch.Size([32, 7, 1])
  [MFA Block] Output after attention weighting shape: torch.Size([32, 7, 120])
[STFTnet] Shape after channel projection conv: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Input shape: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Output shape: torch.Size([32, 64, 120])
[STFTnet] Shape after permuting for transformer: torch.Size([32, 120, 64])
  [Transformer Encoder] Input shape (N, L, C): torch.Size([32, 120, 64])
  [Transformer Encoder] Output shape: torch.Size([32, 120, 64])
[STFTnet] Shape after taking last time step: torch.Size([32, 64])
  [Classifier] Input shape: torch.Size([32, 64])
  [Classifier] Output shape: torch.Size([32, 12])

[STFTnet] Initial input shape: torch.Size([32, 120, 7])
[STFTnet] Shape after permuting for conv: torch.Size([32, 7, 120])
  [MFA Block] Input shape: torch.Size([32, 7, 120])
  [MFA Block] After global average pooling shape: torch.Size([32, 7, 1])
  [MFA Block] Attention weights shape: torch.Size([32, 7, 1])
  [MFA Block] Output after attention weighting shape: torch.Size([32, 7, 120])
[STFTnet] Shape after channel projection conv: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Input shape: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Output shape: torch.Size([32, 64, 120])
[STFTnet] Shape after permuting for transformer: torch.Size([32, 120, 64])
  [Transformer Encoder] Input shape (N, L, C): torch.Size([32, 120, 64])
  [Transformer Encoder] Output shape: torch.Size([32, 120, 64])
[STFTnet] Shape after taking last time step: torch.Size([32, 64])
  [Classifier] Input shape: torch.Size([32, 64])
  [Classifier] Output shape: torch.Size([32, 12])

[STFTnet] Initial input shape: torch.Size([32, 120, 7])
[STFTnet] Shape after permuting for conv: torch.Size([32, 7, 120])
  [MFA Block] Input shape: torch.Size([32, 7, 120])
  [MFA Block] After global average pooling shape: torch.Size([32, 7, 1])
  [MFA Block] Attention weights shape: torch.Size([32, 7, 1])
  [MFA Block] Output after attention weighting shape: torch.Size([32, 7, 120])
[STFTnet] Shape after channel projection conv: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Input shape: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Output shape: torch.Size([32, 64, 120])
[STFTnet] Shape after permuting for transformer: torch.Size([32, 120, 64])
  [Transformer Encoder] Input shape (N, L, C): torch.Size([32, 120, 64])
  [Transformer Encoder] Output shape: torch.Size([32, 120, 64])
[STFTnet] Shape after taking last time step: torch.Size([32, 64])
  [Classifier] Input shape: torch.Size([32, 64])
  [Classifier] Output shape: torch.Size([32, 12])

[STFTnet] Initial input shape: torch.Size([32, 120, 7])
[STFTnet] Shape after permuting for conv: torch.Size([32, 7, 120])
  [MFA Block] Input shape: torch.Size([32, 7, 120])
  [MFA Block] After global average pooling shape: torch.Size([32, 7, 1])
  [MFA Block] Attention weights shape: torch.Size([32, 7, 1])
  [MFA Block] Output after attention weighting shape: torch.Size([32, 7, 120])
[STFTnet] Shape after channel projection conv: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Input shape: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Output shape: torch.Size([32, 64, 120])
[STFTnet] Shape after permuting for transformer: torch.Size([32, 120, 64])
  [Transformer Encoder] Input shape (N, L, C): torch.Size([32, 120, 64])
  [Transformer Encoder] Output shape: torch.Size([32, 120, 64])
[STFTnet] Shape after taking last time step: torch.Size([32, 64])
  [Classifier] Input shape: torch.Size([32, 64])
  [Classifier] Output shape: torch.Size([32, 12])

--- Completed a few batches for validation purposes; ending this epoch early. ---

========================================================
Epoch 10, Loss: 2.1462, Accuracy: 0.5365
========================================================


[STFTnet] Initial input shape: torch.Size([32, 120, 7])
[STFTnet] Shape after permuting for conv: torch.Size([32, 7, 120])
  [MFA Block] Input shape: torch.Size([32, 7, 120])
  [MFA Block] After global average pooling shape: torch.Size([32, 7, 1])
  [MFA Block] Attention weights shape: torch.Size([32, 7, 1])
  [MFA Block] Output after attention weighting shape: torch.Size([32, 7, 120])
[STFTnet] Shape after channel projection conv: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Input shape: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Output shape: torch.Size([32, 64, 120])
[STFTnet] Shape after permuting for transformer: torch.Size([32, 120, 64])
  [Transformer Encoder] Input shape (N, L, C): torch.Size([32, 120, 64])
  [Transformer Encoder] Output shape: torch.Size([32, 120, 64])
[STFTnet] Shape after taking last time step: torch.Size([32, 64])
  [Classifier] Input shape: torch.Size([32, 64])
  [Classifier] Output shape: torch.Size([32, 12])

[STFTnet] Initial input shape: torch.Size([32, 120, 7])
[STFTnet] Shape after permuting for conv: torch.Size([32, 7, 120])
  [MFA Block] Input shape: torch.Size([32, 7, 120])
  [MFA Block] After global average pooling shape: torch.Size([32, 7, 1])
  [MFA Block] Attention weights shape: torch.Size([32, 7, 1])
  [MFA Block] Output after attention weighting shape: torch.Size([32, 7, 120])
[STFTnet] Shape after channel projection conv: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Input shape: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Output shape: torch.Size([32, 64, 120])
[STFTnet] Shape after permuting for transformer: torch.Size([32, 120, 64])
  [Transformer Encoder] Input shape (N, L, C): torch.Size([32, 120, 64])
  [Transformer Encoder] Output shape: torch.Size([32, 120, 64])
[STFTnet] Shape after taking last time step: torch.Size([32, 64])
  [Classifier] Input shape: torch.Size([32, 64])
  [Classifier] Output shape: torch.Size([32, 12])

[STFTnet] Initial input shape: torch.Size([32, 120, 7])
[STFTnet] Shape after permuting for conv: torch.Size([32, 7, 120])
  [MFA Block] Input shape: torch.Size([32, 7, 120])
  [MFA Block] After global average pooling shape: torch.Size([32, 7, 1])
  [MFA Block] Attention weights shape: torch.Size([32, 7, 1])
  [MFA Block] Output after attention weighting shape: torch.Size([32, 7, 120])
[STFTnet] Shape after channel projection conv: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Input shape: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Output shape: torch.Size([32, 64, 120])
[STFTnet] Shape after permuting for transformer: torch.Size([32, 120, 64])
  [Transformer Encoder] Input shape (N, L, C): torch.Size([32, 120, 64])
  [Transformer Encoder] Output shape: torch.Size([32, 120, 64])
[STFTnet] Shape after taking last time step: torch.Size([32, 64])
  [Classifier] Input shape: torch.Size([32, 64])
  [Classifier] Output shape: torch.Size([32, 12])

[STFTnet] Initial input shape: torch.Size([32, 120, 7])
[STFTnet] Shape after permuting for conv: torch.Size([32, 7, 120])
  [MFA Block] Input shape: torch.Size([32, 7, 120])
  [MFA Block] After global average pooling shape: torch.Size([32, 7, 1])
  [MFA Block] Attention weights shape: torch.Size([32, 7, 1])
  [MFA Block] Output after attention weighting shape: torch.Size([32, 7, 120])
[STFTnet] Shape after channel projection conv: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Input shape: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Output shape: torch.Size([32, 64, 120])
[STFTnet] Shape after permuting for transformer: torch.Size([32, 120, 64])
  [Transformer Encoder] Input shape (N, L, C): torch.Size([32, 120, 64])
  [Transformer Encoder] Output shape: torch.Size([32, 120, 64])
[STFTnet] Shape after taking last time step: torch.Size([32, 64])
  [Classifier] Input shape: torch.Size([32, 64])
  [Classifier] Output shape: torch.Size([32, 12])

[STFTnet] Initial input shape: torch.Size([32, 120, 7])
[STFTnet] Shape after permuting for conv: torch.Size([32, 7, 120])
  [MFA Block] Input shape: torch.Size([32, 7, 120])
  [MFA Block] After global average pooling shape: torch.Size([32, 7, 1])
  [MFA Block] Attention weights shape: torch.Size([32, 7, 1])
  [MFA Block] Output after attention weighting shape: torch.Size([32, 7, 120])
[STFTnet] Shape after channel projection conv: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Input shape: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Output shape: torch.Size([32, 64, 120])
[STFTnet] Shape after permuting for transformer: torch.Size([32, 120, 64])
  [Transformer Encoder] Input shape (N, L, C): torch.Size([32, 120, 64])
  [Transformer Encoder] Output shape: torch.Size([32, 120, 64])
[STFTnet] Shape after taking last time step: torch.Size([32, 64])
  [Classifier] Input shape: torch.Size([32, 64])
  [Classifier] Output shape: torch.Size([32, 12])

[STFTnet] Initial input shape: torch.Size([32, 120, 7])
[STFTnet] Shape after permuting for conv: torch.Size([32, 7, 120])
  [MFA Block] Input shape: torch.Size([32, 7, 120])
  [MFA Block] After global average pooling shape: torch.Size([32, 7, 1])
  [MFA Block] Attention weights shape: torch.Size([32, 7, 1])
  [MFA Block] Output after attention weighting shape: torch.Size([32, 7, 120])
[STFTnet] Shape after channel projection conv: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Input shape: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Output shape: torch.Size([32, 64, 120])
[STFTnet] Shape after permuting for transformer: torch.Size([32, 120, 64])
  [Transformer Encoder] Input shape (N, L, C): torch.Size([32, 120, 64])
  [Transformer Encoder] Output shape: torch.Size([32, 120, 64])
[STFTnet] Shape after taking last time step: torch.Size([32, 64])
  [Classifier] Input shape: torch.Size([32, 64])
  [Classifier] Output shape: torch.Size([32, 12])

--- Completed a few batches for validation purposes; ending this epoch early. ---

========================================================
Epoch 11, Loss: 2.0809, Accuracy: 0.5990
========================================================


[STFTnet] Initial input shape: torch.Size([32, 120, 7])
[STFTnet] Shape after permuting for conv: torch.Size([32, 7, 120])
  [MFA Block] Input shape: torch.Size([32, 7, 120])
  [MFA Block] After global average pooling shape: torch.Size([32, 7, 1])
  [MFA Block] Attention weights shape: torch.Size([32, 7, 1])
  [MFA Block] Output after attention weighting shape: torch.Size([32, 7, 120])
[STFTnet] Shape after channel projection conv: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Input shape: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Output shape: torch.Size([32, 64, 120])
[STFTnet] Shape after permuting for transformer: torch.Size([32, 120, 64])
  [Transformer Encoder] Input shape (N, L, C): torch.Size([32, 120, 64])
  [Transformer Encoder] Output shape: torch.Size([32, 120, 64])
[STFTnet] Shape after taking last time step: torch.Size([32, 64])
  [Classifier] Input shape: torch.Size([32, 64])
  [Classifier] Output shape: torch.Size([32, 12])

[STFTnet] Initial input shape: torch.Size([32, 120, 7])
[STFTnet] Shape after permuting for conv: torch.Size([32, 7, 120])
  [MFA Block] Input shape: torch.Size([32, 7, 120])
  [MFA Block] After global average pooling shape: torch.Size([32, 7, 1])
  [MFA Block] Attention weights shape: torch.Size([32, 7, 1])
  [MFA Block] Output after attention weighting shape: torch.Size([32, 7, 120])
[STFTnet] Shape after channel projection conv: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Input shape: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Output shape: torch.Size([32, 64, 120])
[STFTnet] Shape after permuting for transformer: torch.Size([32, 120, 64])
  [Transformer Encoder] Input shape (N, L, C): torch.Size([32, 120, 64])
  [Transformer Encoder] Output shape: torch.Size([32, 120, 64])
[STFTnet] Shape after taking last time step: torch.Size([32, 64])
  [Classifier] Input shape: torch.Size([32, 64])
  [Classifier] Output shape: torch.Size([32, 12])

[STFTnet] Initial input shape: torch.Size([32, 120, 7])
[STFTnet] Shape after permuting for conv: torch.Size([32, 7, 120])
  [MFA Block] Input shape: torch.Size([32, 7, 120])
  [MFA Block] After global average pooling shape: torch.Size([32, 7, 1])
  [MFA Block] Attention weights shape: torch.Size([32, 7, 1])
  [MFA Block] Output after attention weighting shape: torch.Size([32, 7, 120])
[STFTnet] Shape after channel projection conv: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Input shape: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Output shape: torch.Size([32, 64, 120])
[STFTnet] Shape after permuting for transformer: torch.Size([32, 120, 64])
  [Transformer Encoder] Input shape (N, L, C): torch.Size([32, 120, 64])
  [Transformer Encoder] Output shape: torch.Size([32, 120, 64])
[STFTnet] Shape after taking last time step: torch.Size([32, 64])
  [Classifier] Input shape: torch.Size([32, 64])
  [Classifier] Output shape: torch.Size([32, 12])

[STFTnet] Initial input shape: torch.Size([32, 120, 7])
[STFTnet] Shape after permuting for conv: torch.Size([32, 7, 120])
  [MFA Block] Input shape: torch.Size([32, 7, 120])
  [MFA Block] After global average pooling shape: torch.Size([32, 7, 1])
  [MFA Block] Attention weights shape: torch.Size([32, 7, 1])
  [MFA Block] Output after attention weighting shape: torch.Size([32, 7, 120])
[STFTnet] Shape after channel projection conv: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Input shape: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Output shape: torch.Size([32, 64, 120])
[STFTnet] Shape after permuting for transformer: torch.Size([32, 120, 64])
  [Transformer Encoder] Input shape (N, L, C): torch.Size([32, 120, 64])
  [Transformer Encoder] Output shape: torch.Size([32, 120, 64])
[STFTnet] Shape after taking last time step: torch.Size([32, 64])
  [Classifier] Input shape: torch.Size([32, 64])
  [Classifier] Output shape: torch.Size([32, 12])

[STFTnet] Initial input shape: torch.Size([32, 120, 7])
[STFTnet] Shape after permuting for conv: torch.Size([32, 7, 120])
  [MFA Block] Input shape: torch.Size([32, 7, 120])
  [MFA Block] After global average pooling shape: torch.Size([32, 7, 1])
  [MFA Block] Attention weights shape: torch.Size([32, 7, 1])
  [MFA Block] Output after attention weighting shape: torch.Size([32, 7, 120])
[STFTnet] Shape after channel projection conv: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Input shape: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Output shape: torch.Size([32, 64, 120])
[STFTnet] Shape after permuting for transformer: torch.Size([32, 120, 64])
  [Transformer Encoder] Input shape (N, L, C): torch.Size([32, 120, 64])
  [Transformer Encoder] Output shape: torch.Size([32, 120, 64])
[STFTnet] Shape after taking last time step: torch.Size([32, 64])
  [Classifier] Input shape: torch.Size([32, 64])
  [Classifier] Output shape: torch.Size([32, 12])

[STFTnet] Initial input shape: torch.Size([32, 120, 7])
[STFTnet] Shape after permuting for conv: torch.Size([32, 7, 120])
  [MFA Block] Input shape: torch.Size([32, 7, 120])
  [MFA Block] After global average pooling shape: torch.Size([32, 7, 1])
  [MFA Block] Attention weights shape: torch.Size([32, 7, 1])
  [MFA Block] Output after attention weighting shape: torch.Size([32, 7, 120])
[STFTnet] Shape after channel projection conv: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Input shape: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Output shape: torch.Size([32, 64, 120])
[STFTnet] Shape after permuting for transformer: torch.Size([32, 120, 64])
  [Transformer Encoder] Input shape (N, L, C): torch.Size([32, 120, 64])
  [Transformer Encoder] Output shape: torch.Size([32, 120, 64])
[STFTnet] Shape after taking last time step: torch.Size([32, 64])
  [Classifier] Input shape: torch.Size([32, 64])
  [Classifier] Output shape: torch.Size([32, 12])

--- Completed a few batches for validation purposes; ending this epoch early. ---

========================================================
Epoch 12, Loss: 2.0204, Accuracy: 0.6302
========================================================


[STFTnet] Initial input shape: torch.Size([32, 120, 7])
[STFTnet] Shape after permuting for conv: torch.Size([32, 7, 120])
  [MFA Block] Input shape: torch.Size([32, 7, 120])
  [MFA Block] After global average pooling shape: torch.Size([32, 7, 1])
  [MFA Block] Attention weights shape: torch.Size([32, 7, 1])
  [MFA Block] Output after attention weighting shape: torch.Size([32, 7, 120])
[STFTnet] Shape after channel projection conv: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Input shape: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Output shape: torch.Size([32, 64, 120])
[STFTnet] Shape after permuting for transformer: torch.Size([32, 120, 64])
  [Transformer Encoder] Input shape (N, L, C): torch.Size([32, 120, 64])
  [Transformer Encoder] Output shape: torch.Size([32, 120, 64])
[STFTnet] Shape after taking last time step: torch.Size([32, 64])
  [Classifier] Input shape: torch.Size([32, 64])
  [Classifier] Output shape: torch.Size([32, 12])

[STFTnet] Initial input shape: torch.Size([32, 120, 7])
[STFTnet] Shape after permuting for conv: torch.Size([32, 7, 120])
  [MFA Block] Input shape: torch.Size([32, 7, 120])
  [MFA Block] After global average pooling shape: torch.Size([32, 7, 1])
  [MFA Block] Attention weights shape: torch.Size([32, 7, 1])
  [MFA Block] Output after attention weighting shape: torch.Size([32, 7, 120])
[STFTnet] Shape after channel projection conv: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Input shape: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Output shape: torch.Size([32, 64, 120])
[STFTnet] Shape after permuting for transformer: torch.Size([32, 120, 64])
  [Transformer Encoder] Input shape (N, L, C): torch.Size([32, 120, 64])
  [Transformer Encoder] Output shape: torch.Size([32, 120, 64])
[STFTnet] Shape after taking last time step: torch.Size([32, 64])
  [Classifier] Input shape: torch.Size([32, 64])
  [Classifier] Output shape: torch.Size([32, 12])

[STFTnet] Initial input shape: torch.Size([32, 120, 7])
[STFTnet] Shape after permuting for conv: torch.Size([32, 7, 120])
  [MFA Block] Input shape: torch.Size([32, 7, 120])
  [MFA Block] After global average pooling shape: torch.Size([32, 7, 1])
  [MFA Block] Attention weights shape: torch.Size([32, 7, 1])
  [MFA Block] Output after attention weighting shape: torch.Size([32, 7, 120])
[STFTnet] Shape after channel projection conv: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Input shape: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Output shape: torch.Size([32, 64, 120])
[STFTnet] Shape after permuting for transformer: torch.Size([32, 120, 64])
  [Transformer Encoder] Input shape (N, L, C): torch.Size([32, 120, 64])
  [Transformer Encoder] Output shape: torch.Size([32, 120, 64])
[STFTnet] Shape after taking last time step: torch.Size([32, 64])
  [Classifier] Input shape: torch.Size([32, 64])
  [Classifier] Output shape: torch.Size([32, 12])

[STFTnet] Initial input shape: torch.Size([32, 120, 7])
[STFTnet] Shape after permuting for conv: torch.Size([32, 7, 120])
  [MFA Block] Input shape: torch.Size([32, 7, 120])
  [MFA Block] After global average pooling shape: torch.Size([32, 7, 1])
  [MFA Block] Attention weights shape: torch.Size([32, 7, 1])
  [MFA Block] Output after attention weighting shape: torch.Size([32, 7, 120])
[STFTnet] Shape after channel projection conv: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Input shape: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Output shape: torch.Size([32, 64, 120])
[STFTnet] Shape after permuting for transformer: torch.Size([32, 120, 64])
  [Transformer Encoder] Input shape (N, L, C): torch.Size([32, 120, 64])
  [Transformer Encoder] Output shape: torch.Size([32, 120, 64])
[STFTnet] Shape after taking last time step: torch.Size([32, 64])
  [Classifier] Input shape: torch.Size([32, 64])
  [Classifier] Output shape: torch.Size([32, 12])

[STFTnet] Initial input shape: torch.Size([32, 120, 7])
[STFTnet] Shape after permuting for conv: torch.Size([32, 7, 120])
  [MFA Block] Input shape: torch.Size([32, 7, 120])
  [MFA Block] After global average pooling shape: torch.Size([32, 7, 1])
  [MFA Block] Attention weights shape: torch.Size([32, 7, 1])
  [MFA Block] Output after attention weighting shape: torch.Size([32, 7, 120])
[STFTnet] Shape after channel projection conv: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Input shape: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Output shape: torch.Size([32, 64, 120])
[STFTnet] Shape after permuting for transformer: torch.Size([32, 120, 64])
  [Transformer Encoder] Input shape (N, L, C): torch.Size([32, 120, 64])
  [Transformer Encoder] Output shape: torch.Size([32, 120, 64])
[STFTnet] Shape after taking last time step: torch.Size([32, 64])
  [Classifier] Input shape: torch.Size([32, 64])
  [Classifier] Output shape: torch.Size([32, 12])

[STFTnet] Initial input shape: torch.Size([32, 120, 7])
[STFTnet] Shape after permuting for conv: torch.Size([32, 7, 120])
  [MFA Block] Input shape: torch.Size([32, 7, 120])
  [MFA Block] After global average pooling shape: torch.Size([32, 7, 1])
  [MFA Block] Attention weights shape: torch.Size([32, 7, 1])
  [MFA Block] Output after attention weighting shape: torch.Size([32, 7, 120])
[STFTnet] Shape after channel projection conv: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Input shape: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Output shape: torch.Size([32, 64, 120])
[STFTnet] Shape after permuting for transformer: torch.Size([32, 120, 64])
  [Transformer Encoder] Input shape (N, L, C): torch.Size([32, 120, 64])
  [Transformer Encoder] Output shape: torch.Size([32, 120, 64])
[STFTnet] Shape after taking last time step: torch.Size([32, 64])
  [Classifier] Input shape: torch.Size([32, 64])
  [Classifier] Output shape: torch.Size([32, 12])

--- Completed a few batches for validation purposes; ending this epoch early. ---

========================================================
Epoch 13, Loss: 2.0000, Accuracy: 0.6302
========================================================


[STFTnet] Initial input shape: torch.Size([32, 120, 7])
[STFTnet] Shape after permuting for conv: torch.Size([32, 7, 120])
  [MFA Block] Input shape: torch.Size([32, 7, 120])
  [MFA Block] After global average pooling shape: torch.Size([32, 7, 1])
  [MFA Block] Attention weights shape: torch.Size([32, 7, 1])
  [MFA Block] Output after attention weighting shape: torch.Size([32, 7, 120])
[STFTnet] Shape after channel projection conv: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Input shape: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Output shape: torch.Size([32, 64, 120])
[STFTnet] Shape after permuting for transformer: torch.Size([32, 120, 64])
  [Transformer Encoder] Input shape (N, L, C): torch.Size([32, 120, 64])
  [Transformer Encoder] Output shape: torch.Size([32, 120, 64])
[STFTnet] Shape after taking last time step: torch.Size([32, 64])
  [Classifier] Input shape: torch.Size([32, 64])
  [Classifier] Output shape: torch.Size([32, 12])

[STFTnet] Initial input shape: torch.Size([32, 120, 7])
[STFTnet] Shape after permuting for conv: torch.Size([32, 7, 120])
  [MFA Block] Input shape: torch.Size([32, 7, 120])
  [MFA Block] After global average pooling shape: torch.Size([32, 7, 1])
  [MFA Block] Attention weights shape: torch.Size([32, 7, 1])
  [MFA Block] Output after attention weighting shape: torch.Size([32, 7, 120])
[STFTnet] Shape after channel projection conv: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Input shape: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Output shape: torch.Size([32, 64, 120])
[STFTnet] Shape after permuting for transformer: torch.Size([32, 120, 64])
  [Transformer Encoder] Input shape (N, L, C): torch.Size([32, 120, 64])
  [Transformer Encoder] Output shape: torch.Size([32, 120, 64])
[STFTnet] Shape after taking last time step: torch.Size([32, 64])
  [Classifier] Input shape: torch.Size([32, 64])
  [Classifier] Output shape: torch.Size([32, 12])

[STFTnet] Initial input shape: torch.Size([32, 120, 7])
[STFTnet] Shape after permuting for conv: torch.Size([32, 7, 120])
  [MFA Block] Input shape: torch.Size([32, 7, 120])
  [MFA Block] After global average pooling shape: torch.Size([32, 7, 1])
  [MFA Block] Attention weights shape: torch.Size([32, 7, 1])
  [MFA Block] Output after attention weighting shape: torch.Size([32, 7, 120])
[STFTnet] Shape after channel projection conv: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Input shape: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Output shape: torch.Size([32, 64, 120])
[STFTnet] Shape after permuting for transformer: torch.Size([32, 120, 64])
  [Transformer Encoder] Input shape (N, L, C): torch.Size([32, 120, 64])
  [Transformer Encoder] Output shape: torch.Size([32, 120, 64])
[STFTnet] Shape after taking last time step: torch.Size([32, 64])
  [Classifier] Input shape: torch.Size([32, 64])
  [Classifier] Output shape: torch.Size([32, 12])

[STFTnet] Initial input shape: torch.Size([32, 120, 7])
[STFTnet] Shape after permuting for conv: torch.Size([32, 7, 120])
  [MFA Block] Input shape: torch.Size([32, 7, 120])
  [MFA Block] After global average pooling shape: torch.Size([32, 7, 1])
  [MFA Block] Attention weights shape: torch.Size([32, 7, 1])
  [MFA Block] Output after attention weighting shape: torch.Size([32, 7, 120])
[STFTnet] Shape after channel projection conv: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Input shape: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Output shape: torch.Size([32, 64, 120])
[STFTnet] Shape after permuting for transformer: torch.Size([32, 120, 64])
  [Transformer Encoder] Input shape (N, L, C): torch.Size([32, 120, 64])
  [Transformer Encoder] Output shape: torch.Size([32, 120, 64])
[STFTnet] Shape after taking last time step: torch.Size([32, 64])
  [Classifier] Input shape: torch.Size([32, 64])
  [Classifier] Output shape: torch.Size([32, 12])

[STFTnet] Initial input shape: torch.Size([32, 120, 7])
[STFTnet] Shape after permuting for conv: torch.Size([32, 7, 120])
  [MFA Block] Input shape: torch.Size([32, 7, 120])
  [MFA Block] After global average pooling shape: torch.Size([32, 7, 1])
  [MFA Block] Attention weights shape: torch.Size([32, 7, 1])
  [MFA Block] Output after attention weighting shape: torch.Size([32, 7, 120])
[STFTnet] Shape after channel projection conv: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Input shape: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Output shape: torch.Size([32, 64, 120])
[STFTnet] Shape after permuting for transformer: torch.Size([32, 120, 64])
  [Transformer Encoder] Input shape (N, L, C): torch.Size([32, 120, 64])
  [Transformer Encoder] Output shape: torch.Size([32, 120, 64])
[STFTnet] Shape after taking last time step: torch.Size([32, 64])
  [Classifier] Input shape: torch.Size([32, 64])
  [Classifier] Output shape: torch.Size([32, 12])

[STFTnet] Initial input shape: torch.Size([32, 120, 7])
[STFTnet] Shape after permuting for conv: torch.Size([32, 7, 120])
  [MFA Block] Input shape: torch.Size([32, 7, 120])
  [MFA Block] After global average pooling shape: torch.Size([32, 7, 1])
  [MFA Block] Attention weights shape: torch.Size([32, 7, 1])
  [MFA Block] Output after attention weighting shape: torch.Size([32, 7, 120])
[STFTnet] Shape after channel projection conv: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Input shape: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Output shape: torch.Size([32, 64, 120])
[STFTnet] Shape after permuting for transformer: torch.Size([32, 120, 64])
  [Transformer Encoder] Input shape (N, L, C): torch.Size([32, 120, 64])
  [Transformer Encoder] Output shape: torch.Size([32, 120, 64])
[STFTnet] Shape after taking last time step: torch.Size([32, 64])
  [Classifier] Input shape: torch.Size([32, 64])
  [Classifier] Output shape: torch.Size([32, 12])

--- Completed a few batches for validation purposes; ending this epoch early. ---

========================================================
Epoch 14, Loss: 1.9875, Accuracy: 0.6719
========================================================


[STFTnet] Initial input shape: torch.Size([32, 120, 7])
[STFTnet] Shape after permuting for conv: torch.Size([32, 7, 120])
  [MFA Block] Input shape: torch.Size([32, 7, 120])
  [MFA Block] After global average pooling shape: torch.Size([32, 7, 1])
  [MFA Block] Attention weights shape: torch.Size([32, 7, 1])
  [MFA Block] Output after attention weighting shape: torch.Size([32, 7, 120])
[STFTnet] Shape after channel projection conv: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Input shape: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Output shape: torch.Size([32, 64, 120])
[STFTnet] Shape after permuting for transformer: torch.Size([32, 120, 64])
  [Transformer Encoder] Input shape (N, L, C): torch.Size([32, 120, 64])
  [Transformer Encoder] Output shape: torch.Size([32, 120, 64])
[STFTnet] Shape after taking last time step: torch.Size([32, 64])
  [Classifier] Input shape: torch.Size([32, 64])
  [Classifier] Output shape: torch.Size([32, 12])

[STFTnet] Initial input shape: torch.Size([32, 120, 7])
[STFTnet] Shape after permuting for conv: torch.Size([32, 7, 120])
  [MFA Block] Input shape: torch.Size([32, 7, 120])
  [MFA Block] After global average pooling shape: torch.Size([32, 7, 1])
  [MFA Block] Attention weights shape: torch.Size([32, 7, 1])
  [MFA Block] Output after attention weighting shape: torch.Size([32, 7, 120])
[STFTnet] Shape after channel projection conv: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Input shape: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Output shape: torch.Size([32, 64, 120])
[STFTnet] Shape after permuting for transformer: torch.Size([32, 120, 64])
  [Transformer Encoder] Input shape (N, L, C): torch.Size([32, 120, 64])
  [Transformer Encoder] Output shape: torch.Size([32, 120, 64])
[STFTnet] Shape after taking last time step: torch.Size([32, 64])
  [Classifier] Input shape: torch.Size([32, 64])
  [Classifier] Output shape: torch.Size([32, 12])

[STFTnet] Initial input shape: torch.Size([32, 120, 7])
[STFTnet] Shape after permuting for conv: torch.Size([32, 7, 120])
  [MFA Block] Input shape: torch.Size([32, 7, 120])
  [MFA Block] After global average pooling shape: torch.Size([32, 7, 1])
  [MFA Block] Attention weights shape: torch.Size([32, 7, 1])
  [MFA Block] Output after attention weighting shape: torch.Size([32, 7, 120])
[STFTnet] Shape after channel projection conv: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Input shape: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Output shape: torch.Size([32, 64, 120])
[STFTnet] Shape after permuting for transformer: torch.Size([32, 120, 64])
  [Transformer Encoder] Input shape (N, L, C): torch.Size([32, 120, 64])
  [Transformer Encoder] Output shape: torch.Size([32, 120, 64])
[STFTnet] Shape after taking last time step: torch.Size([32, 64])
  [Classifier] Input shape: torch.Size([32, 64])
  [Classifier] Output shape: torch.Size([32, 12])

[STFTnet] Initial input shape: torch.Size([32, 120, 7])
[STFTnet] Shape after permuting for conv: torch.Size([32, 7, 120])
  [MFA Block] Input shape: torch.Size([32, 7, 120])
  [MFA Block] After global average pooling shape: torch.Size([32, 7, 1])
  [MFA Block] Attention weights shape: torch.Size([32, 7, 1])
  [MFA Block] Output after attention weighting shape: torch.Size([32, 7, 120])
[STFTnet] Shape after channel projection conv: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Input shape: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Output shape: torch.Size([32, 64, 120])
[STFTnet] Shape after permuting for transformer: torch.Size([32, 120, 64])
  [Transformer Encoder] Input shape (N, L, C): torch.Size([32, 120, 64])
  [Transformer Encoder] Output shape: torch.Size([32, 120, 64])
[STFTnet] Shape after taking last time step: torch.Size([32, 64])
  [Classifier] Input shape: torch.Size([32, 64])
  [Classifier] Output shape: torch.Size([32, 12])

[STFTnet] Initial input shape: torch.Size([32, 120, 7])
[STFTnet] Shape after permuting for conv: torch.Size([32, 7, 120])
  [MFA Block] Input shape: torch.Size([32, 7, 120])
  [MFA Block] After global average pooling shape: torch.Size([32, 7, 1])
  [MFA Block] Attention weights shape: torch.Size([32, 7, 1])
  [MFA Block] Output after attention weighting shape: torch.Size([32, 7, 120])
[STFTnet] Shape after channel projection conv: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Input shape: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Output shape: torch.Size([32, 64, 120])
[STFTnet] Shape after permuting for transformer: torch.Size([32, 120, 64])
  [Transformer Encoder] Input shape (N, L, C): torch.Size([32, 120, 64])
  [Transformer Encoder] Output shape: torch.Size([32, 120, 64])
[STFTnet] Shape after taking last time step: torch.Size([32, 64])
  [Classifier] Input shape: torch.Size([32, 64])
  [Classifier] Output shape: torch.Size([32, 12])

[STFTnet] Initial input shape: torch.Size([32, 120, 7])
[STFTnet] Shape after permuting for conv: torch.Size([32, 7, 120])
  [MFA Block] Input shape: torch.Size([32, 7, 120])
  [MFA Block] After global average pooling shape: torch.Size([32, 7, 1])
  [MFA Block] Attention weights shape: torch.Size([32, 7, 1])
  [MFA Block] Output after attention weighting shape: torch.Size([32, 7, 120])
[STFTnet] Shape after channel projection conv: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Input shape: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Output shape: torch.Size([32, 64, 120])
[STFTnet] Shape after permuting for transformer: torch.Size([32, 120, 64])
  [Transformer Encoder] Input shape (N, L, C): torch.Size([32, 120, 64])
  [Transformer Encoder] Output shape: torch.Size([32, 120, 64])
[STFTnet] Shape after taking last time step: torch.Size([32, 64])
  [Classifier] Input shape: torch.Size([32, 64])
  [Classifier] Output shape: torch.Size([32, 12])

--- Completed a few batches for validation purposes; ending this epoch early. ---

========================================================
Epoch 15, Loss: 1.9628, Accuracy: 0.6823
========================================================


[STFTnet] Initial input shape: torch.Size([32, 120, 7])
[STFTnet] Shape after permuting for conv: torch.Size([32, 7, 120])
  [MFA Block] Input shape: torch.Size([32, 7, 120])
  [MFA Block] After global average pooling shape: torch.Size([32, 7, 1])
  [MFA Block] Attention weights shape: torch.Size([32, 7, 1])
  [MFA Block] Output after attention weighting shape: torch.Size([32, 7, 120])
[STFTnet] Shape after channel projection conv: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Input shape: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Output shape: torch.Size([32, 64, 120])
[STFTnet] Shape after permuting for transformer: torch.Size([32, 120, 64])
  [Transformer Encoder] Input shape (N, L, C): torch.Size([32, 120, 64])
  [Transformer Encoder] Output shape: torch.Size([32, 120, 64])
[STFTnet] Shape after taking last time step: torch.Size([32, 64])
  [Classifier] Input shape: torch.Size([32, 64])
  [Classifier] Output shape: torch.Size([32, 12])

[STFTnet] Initial input shape: torch.Size([32, 120, 7])
[STFTnet] Shape after permuting for conv: torch.Size([32, 7, 120])
  [MFA Block] Input shape: torch.Size([32, 7, 120])
  [MFA Block] After global average pooling shape: torch.Size([32, 7, 1])
  [MFA Block] Attention weights shape: torch.Size([32, 7, 1])
  [MFA Block] Output after attention weighting shape: torch.Size([32, 7, 120])
[STFTnet] Shape after channel projection conv: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Input shape: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Output shape: torch.Size([32, 64, 120])
[STFTnet] Shape after permuting for transformer: torch.Size([32, 120, 64])
  [Transformer Encoder] Input shape (N, L, C): torch.Size([32, 120, 64])
  [Transformer Encoder] Output shape: torch.Size([32, 120, 64])
[STFTnet] Shape after taking last time step: torch.Size([32, 64])
  [Classifier] Input shape: torch.Size([32, 64])
  [Classifier] Output shape: torch.Size([32, 12])

[STFTnet] Initial input shape: torch.Size([32, 120, 7])
[STFTnet] Shape after permuting for conv: torch.Size([32, 7, 120])
  [MFA Block] Input shape: torch.Size([32, 7, 120])
  [MFA Block] After global average pooling shape: torch.Size([32, 7, 1])
  [MFA Block] Attention weights shape: torch.Size([32, 7, 1])
  [MFA Block] Output after attention weighting shape: torch.Size([32, 7, 120])
[STFTnet] Shape after channel projection conv: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Input shape: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Output shape: torch.Size([32, 64, 120])
[STFTnet] Shape after permuting for transformer: torch.Size([32, 120, 64])
  [Transformer Encoder] Input shape (N, L, C): torch.Size([32, 120, 64])
  [Transformer Encoder] Output shape: torch.Size([32, 120, 64])
[STFTnet] Shape after taking last time step: torch.Size([32, 64])
  [Classifier] Input shape: torch.Size([32, 64])
  [Classifier] Output shape: torch.Size([32, 12])

[STFTnet] Initial input shape: torch.Size([32, 120, 7])
[STFTnet] Shape after permuting for conv: torch.Size([32, 7, 120])
  [MFA Block] Input shape: torch.Size([32, 7, 120])
  [MFA Block] After global average pooling shape: torch.Size([32, 7, 1])
  [MFA Block] Attention weights shape: torch.Size([32, 7, 1])
  [MFA Block] Output after attention weighting shape: torch.Size([32, 7, 120])
[STFTnet] Shape after channel projection conv: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Input shape: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Output shape: torch.Size([32, 64, 120])
[STFTnet] Shape after permuting for transformer: torch.Size([32, 120, 64])
  [Transformer Encoder] Input shape (N, L, C): torch.Size([32, 120, 64])
  [Transformer Encoder] Output shape: torch.Size([32, 120, 64])
[STFTnet] Shape after taking last time step: torch.Size([32, 64])
  [Classifier] Input shape: torch.Size([32, 64])
  [Classifier] Output shape: torch.Size([32, 12])

[STFTnet] Initial input shape: torch.Size([32, 120, 7])
[STFTnet] Shape after permuting for conv: torch.Size([32, 7, 120])
  [MFA Block] Input shape: torch.Size([32, 7, 120])
  [MFA Block] After global average pooling shape: torch.Size([32, 7, 1])
  [MFA Block] Attention weights shape: torch.Size([32, 7, 1])
  [MFA Block] Output after attention weighting shape: torch.Size([32, 7, 120])
[STFTnet] Shape after channel projection conv: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Input shape: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Output shape: torch.Size([32, 64, 120])
[STFTnet] Shape after permuting for transformer: torch.Size([32, 120, 64])
  [Transformer Encoder] Input shape (N, L, C): torch.Size([32, 120, 64])
  [Transformer Encoder] Output shape: torch.Size([32, 120, 64])
[STFTnet] Shape after taking last time step: torch.Size([32, 64])
  [Classifier] Input shape: torch.Size([32, 64])
  [Classifier] Output shape: torch.Size([32, 12])

[STFTnet] Initial input shape: torch.Size([32, 120, 7])
[STFTnet] Shape after permuting for conv: torch.Size([32, 7, 120])
  [MFA Block] Input shape: torch.Size([32, 7, 120])
  [MFA Block] After global average pooling shape: torch.Size([32, 7, 1])
  [MFA Block] Attention weights shape: torch.Size([32, 7, 1])
  [MFA Block] Output after attention weighting shape: torch.Size([32, 7, 120])
[STFTnet] Shape after channel projection conv: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Input shape: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Output shape: torch.Size([32, 64, 120])
[STFTnet] Shape after permuting for transformer: torch.Size([32, 120, 64])
  [Transformer Encoder] Input shape (N, L, C): torch.Size([32, 120, 64])
  [Transformer Encoder] Output shape: torch.Size([32, 120, 64])
[STFTnet] Shape after taking last time step: torch.Size([32, 64])
  [Classifier] Input shape: torch.Size([32, 64])
  [Classifier] Output shape: torch.Size([32, 12])

--- Completed a few batches for validation purposes; ending this epoch early. ---

========================================================
Epoch 16, Loss: 1.8984, Accuracy: 0.7604
========================================================


[STFTnet] Initial input shape: torch.Size([32, 120, 7])
[STFTnet] Shape after permuting for conv: torch.Size([32, 7, 120])
  [MFA Block] Input shape: torch.Size([32, 7, 120])
  [MFA Block] After global average pooling shape: torch.Size([32, 7, 1])
  [MFA Block] Attention weights shape: torch.Size([32, 7, 1])
  [MFA Block] Output after attention weighting shape: torch.Size([32, 7, 120])
[STFTnet] Shape after channel projection conv: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Input shape: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Output shape: torch.Size([32, 64, 120])
[STFTnet] Shape after permuting for transformer: torch.Size([32, 120, 64])
  [Transformer Encoder] Input shape (N, L, C): torch.Size([32, 120, 64])
  [Transformer Encoder] Output shape: torch.Size([32, 120, 64])
[STFTnet] Shape after taking last time step: torch.Size([32, 64])
  [Classifier] Input shape: torch.Size([32, 64])
  [Classifier] Output shape: torch.Size([32, 12])

[STFTnet] Initial input shape: torch.Size([32, 120, 7])
[STFTnet] Shape after permuting for conv: torch.Size([32, 7, 120])
  [MFA Block] Input shape: torch.Size([32, 7, 120])
  [MFA Block] After global average pooling shape: torch.Size([32, 7, 1])
  [MFA Block] Attention weights shape: torch.Size([32, 7, 1])
  [MFA Block] Output after attention weighting shape: torch.Size([32, 7, 120])
[STFTnet] Shape after channel projection conv: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Input shape: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Output shape: torch.Size([32, 64, 120])
[STFTnet] Shape after permuting for transformer: torch.Size([32, 120, 64])
  [Transformer Encoder] Input shape (N, L, C): torch.Size([32, 120, 64])
  [Transformer Encoder] Output shape: torch.Size([32, 120, 64])
[STFTnet] Shape after taking last time step: torch.Size([32, 64])
  [Classifier] Input shape: torch.Size([32, 64])
  [Classifier] Output shape: torch.Size([32, 12])

[STFTnet] Initial input shape: torch.Size([32, 120, 7])
[STFTnet] Shape after permuting for conv: torch.Size([32, 7, 120])
  [MFA Block] Input shape: torch.Size([32, 7, 120])
  [MFA Block] After global average pooling shape: torch.Size([32, 7, 1])
  [MFA Block] Attention weights shape: torch.Size([32, 7, 1])
  [MFA Block] Output after attention weighting shape: torch.Size([32, 7, 120])
[STFTnet] Shape after channel projection conv: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Input shape: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Output shape: torch.Size([32, 64, 120])
[STFTnet] Shape after permuting for transformer: torch.Size([32, 120, 64])
  [Transformer Encoder] Input shape (N, L, C): torch.Size([32, 120, 64])
  [Transformer Encoder] Output shape: torch.Size([32, 120, 64])
[STFTnet] Shape after taking last time step: torch.Size([32, 64])
  [Classifier] Input shape: torch.Size([32, 64])
  [Classifier] Output shape: torch.Size([32, 12])

[STFTnet] Initial input shape: torch.Size([32, 120, 7])
[STFTnet] Shape after permuting for conv: torch.Size([32, 7, 120])
  [MFA Block] Input shape: torch.Size([32, 7, 120])
  [MFA Block] After global average pooling shape: torch.Size([32, 7, 1])
  [MFA Block] Attention weights shape: torch.Size([32, 7, 1])
  [MFA Block] Output after attention weighting shape: torch.Size([32, 7, 120])
[STFTnet] Shape after channel projection conv: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Input shape: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Output shape: torch.Size([32, 64, 120])
[STFTnet] Shape after permuting for transformer: torch.Size([32, 120, 64])
  [Transformer Encoder] Input shape (N, L, C): torch.Size([32, 120, 64])
  [Transformer Encoder] Output shape: torch.Size([32, 120, 64])
[STFTnet] Shape after taking last time step: torch.Size([32, 64])
  [Classifier] Input shape: torch.Size([32, 64])
  [Classifier] Output shape: torch.Size([32, 12])

[STFTnet] Initial input shape: torch.Size([32, 120, 7])
[STFTnet] Shape after permuting for conv: torch.Size([32, 7, 120])
  [MFA Block] Input shape: torch.Size([32, 7, 120])
  [MFA Block] After global average pooling shape: torch.Size([32, 7, 1])
  [MFA Block] Attention weights shape: torch.Size([32, 7, 1])
  [MFA Block] Output after attention weighting shape: torch.Size([32, 7, 120])
[STFTnet] Shape after channel projection conv: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Input shape: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Output shape: torch.Size([32, 64, 120])
[STFTnet] Shape after permuting for transformer: torch.Size([32, 120, 64])
  [Transformer Encoder] Input shape (N, L, C): torch.Size([32, 120, 64])
  [Transformer Encoder] Output shape: torch.Size([32, 120, 64])
[STFTnet] Shape after taking last time step: torch.Size([32, 64])
  [Classifier] Input shape: torch.Size([32, 64])
  [Classifier] Output shape: torch.Size([32, 12])

[STFTnet] Initial input shape: torch.Size([32, 120, 7])
[STFTnet] Shape after permuting for conv: torch.Size([32, 7, 120])
  [MFA Block] Input shape: torch.Size([32, 7, 120])
  [MFA Block] After global average pooling shape: torch.Size([32, 7, 1])
  [MFA Block] Attention weights shape: torch.Size([32, 7, 1])
  [MFA Block] Output after attention weighting shape: torch.Size([32, 7, 120])
[STFTnet] Shape after channel projection conv: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Input shape: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Output shape: torch.Size([32, 64, 120])
[STFTnet] Shape after permuting for transformer: torch.Size([32, 120, 64])
  [Transformer Encoder] Input shape (N, L, C): torch.Size([32, 120, 64])
  [Transformer Encoder] Output shape: torch.Size([32, 120, 64])
[STFTnet] Shape after taking last time step: torch.Size([32, 64])
  [Classifier] Input shape: torch.Size([32, 64])
  [Classifier] Output shape: torch.Size([32, 12])

--- Completed a few batches for validation purposes; ending this epoch early. ---

========================================================
Epoch 17, Loss: 1.9310, Accuracy: 0.7188
========================================================


[STFTnet] Initial input shape: torch.Size([32, 120, 7])
[STFTnet] Shape after permuting for conv: torch.Size([32, 7, 120])
  [MFA Block] Input shape: torch.Size([32, 7, 120])
  [MFA Block] After global average pooling shape: torch.Size([32, 7, 1])
  [MFA Block] Attention weights shape: torch.Size([32, 7, 1])
  [MFA Block] Output after attention weighting shape: torch.Size([32, 7, 120])
[STFTnet] Shape after channel projection conv: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Input shape: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Output shape: torch.Size([32, 64, 120])
[STFTnet] Shape after permuting for transformer: torch.Size([32, 120, 64])
  [Transformer Encoder] Input shape (N, L, C): torch.Size([32, 120, 64])
  [Transformer Encoder] Output shape: torch.Size([32, 120, 64])
[STFTnet] Shape after taking last time step: torch.Size([32, 64])
  [Classifier] Input shape: torch.Size([32, 64])
  [Classifier] Output shape: torch.Size([32, 12])

[STFTnet] Initial input shape: torch.Size([32, 120, 7])
[STFTnet] Shape after permuting for conv: torch.Size([32, 7, 120])
  [MFA Block] Input shape: torch.Size([32, 7, 120])
  [MFA Block] After global average pooling shape: torch.Size([32, 7, 1])
  [MFA Block] Attention weights shape: torch.Size([32, 7, 1])
  [MFA Block] Output after attention weighting shape: torch.Size([32, 7, 120])
[STFTnet] Shape after channel projection conv: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Input shape: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Output shape: torch.Size([32, 64, 120])
[STFTnet] Shape after permuting for transformer: torch.Size([32, 120, 64])
  [Transformer Encoder] Input shape (N, L, C): torch.Size([32, 120, 64])
  [Transformer Encoder] Output shape: torch.Size([32, 120, 64])
[STFTnet] Shape after taking last time step: torch.Size([32, 64])
  [Classifier] Input shape: torch.Size([32, 64])
  [Classifier] Output shape: torch.Size([32, 12])

[STFTnet] Initial input shape: torch.Size([32, 120, 7])
[STFTnet] Shape after permuting for conv: torch.Size([32, 7, 120])
  [MFA Block] Input shape: torch.Size([32, 7, 120])
  [MFA Block] After global average pooling shape: torch.Size([32, 7, 1])
  [MFA Block] Attention weights shape: torch.Size([32, 7, 1])
  [MFA Block] Output after attention weighting shape: torch.Size([32, 7, 120])
[STFTnet] Shape after channel projection conv: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Input shape: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Output shape: torch.Size([32, 64, 120])
[STFTnet] Shape after permuting for transformer: torch.Size([32, 120, 64])
  [Transformer Encoder] Input shape (N, L, C): torch.Size([32, 120, 64])
  [Transformer Encoder] Output shape: torch.Size([32, 120, 64])
[STFTnet] Shape after taking last time step: torch.Size([32, 64])
  [Classifier] Input shape: torch.Size([32, 64])
  [Classifier] Output shape: torch.Size([32, 12])

[STFTnet] Initial input shape: torch.Size([32, 120, 7])
[STFTnet] Shape after permuting for conv: torch.Size([32, 7, 120])
  [MFA Block] Input shape: torch.Size([32, 7, 120])
  [MFA Block] After global average pooling shape: torch.Size([32, 7, 1])
  [MFA Block] Attention weights shape: torch.Size([32, 7, 1])
  [MFA Block] Output after attention weighting shape: torch.Size([32, 7, 120])
[STFTnet] Shape after channel projection conv: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Input shape: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Output shape: torch.Size([32, 64, 120])
[STFTnet] Shape after permuting for transformer: torch.Size([32, 120, 64])
  [Transformer Encoder] Input shape (N, L, C): torch.Size([32, 120, 64])
  [Transformer Encoder] Output shape: torch.Size([32, 120, 64])
[STFTnet] Shape after taking last time step: torch.Size([32, 64])
  [Classifier] Input shape: torch.Size([32, 64])
  [Classifier] Output shape: torch.Size([32, 12])

[STFTnet] Initial input shape: torch.Size([32, 120, 7])
[STFTnet] Shape after permuting for conv: torch.Size([32, 7, 120])
  [MFA Block] Input shape: torch.Size([32, 7, 120])
  [MFA Block] After global average pooling shape: torch.Size([32, 7, 1])
  [MFA Block] Attention weights shape: torch.Size([32, 7, 1])
  [MFA Block] Output after attention weighting shape: torch.Size([32, 7, 120])
[STFTnet] Shape after channel projection conv: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Input shape: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Output shape: torch.Size([32, 64, 120])
[STFTnet] Shape after permuting for transformer: torch.Size([32, 120, 64])
  [Transformer Encoder] Input shape (N, L, C): torch.Size([32, 120, 64])
  [Transformer Encoder] Output shape: torch.Size([32, 120, 64])
[STFTnet] Shape after taking last time step: torch.Size([32, 64])
  [Classifier] Input shape: torch.Size([32, 64])
  [Classifier] Output shape: torch.Size([32, 12])

[STFTnet] Initial input shape: torch.Size([32, 120, 7])
[STFTnet] Shape after permuting for conv: torch.Size([32, 7, 120])
  [MFA Block] Input shape: torch.Size([32, 7, 120])
  [MFA Block] After global average pooling shape: torch.Size([32, 7, 1])
  [MFA Block] Attention weights shape: torch.Size([32, 7, 1])
  [MFA Block] Output after attention weighting shape: torch.Size([32, 7, 120])
[STFTnet] Shape after channel projection conv: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Input shape: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Output shape: torch.Size([32, 64, 120])
[STFTnet] Shape after permuting for transformer: torch.Size([32, 120, 64])
  [Transformer Encoder] Input shape (N, L, C): torch.Size([32, 120, 64])
  [Transformer Encoder] Output shape: torch.Size([32, 120, 64])
[STFTnet] Shape after taking last time step: torch.Size([32, 64])
  [Classifier] Input shape: torch.Size([32, 64])
  [Classifier] Output shape: torch.Size([32, 12])

--- Completed a few batches for validation purposes; ending this epoch early. ---

========================================================
Epoch 18, Loss: 1.8386, Accuracy: 0.8333
========================================================


[STFTnet] Initial input shape: torch.Size([32, 120, 7])
[STFTnet] Shape after permuting for conv: torch.Size([32, 7, 120])
  [MFA Block] Input shape: torch.Size([32, 7, 120])
  [MFA Block] After global average pooling shape: torch.Size([32, 7, 1])
  [MFA Block] Attention weights shape: torch.Size([32, 7, 1])
  [MFA Block] Output after attention weighting shape: torch.Size([32, 7, 120])
[STFTnet] Shape after channel projection conv: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Input shape: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Output shape: torch.Size([32, 64, 120])
[STFTnet] Shape after permuting for transformer: torch.Size([32, 120, 64])
  [Transformer Encoder] Input shape (N, L, C): torch.Size([32, 120, 64])
  [Transformer Encoder] Output shape: torch.Size([32, 120, 64])
[STFTnet] Shape after taking last time step: torch.Size([32, 64])
  [Classifier] Input shape: torch.Size([32, 64])
  [Classifier] Output shape: torch.Size([32, 12])

[STFTnet] Initial input shape: torch.Size([32, 120, 7])
[STFTnet] Shape after permuting for conv: torch.Size([32, 7, 120])
  [MFA Block] Input shape: torch.Size([32, 7, 120])
  [MFA Block] After global average pooling shape: torch.Size([32, 7, 1])
  [MFA Block] Attention weights shape: torch.Size([32, 7, 1])
  [MFA Block] Output after attention weighting shape: torch.Size([32, 7, 120])
[STFTnet] Shape after channel projection conv: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Input shape: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Output shape: torch.Size([32, 64, 120])
[STFTnet] Shape after permuting for transformer: torch.Size([32, 120, 64])
  [Transformer Encoder] Input shape (N, L, C): torch.Size([32, 120, 64])
  [Transformer Encoder] Output shape: torch.Size([32, 120, 64])
[STFTnet] Shape after taking last time step: torch.Size([32, 64])
  [Classifier] Input shape: torch.Size([32, 64])
  [Classifier] Output shape: torch.Size([32, 12])

[STFTnet] Initial input shape: torch.Size([32, 120, 7])
[STFTnet] Shape after permuting for conv: torch.Size([32, 7, 120])
  [MFA Block] Input shape: torch.Size([32, 7, 120])
  [MFA Block] After global average pooling shape: torch.Size([32, 7, 1])
  [MFA Block] Attention weights shape: torch.Size([32, 7, 1])
  [MFA Block] Output after attention weighting shape: torch.Size([32, 7, 120])
[STFTnet] Shape after channel projection conv: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Input shape: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Output shape: torch.Size([32, 64, 120])
[STFTnet] Shape after permuting for transformer: torch.Size([32, 120, 64])
  [Transformer Encoder] Input shape (N, L, C): torch.Size([32, 120, 64])
  [Transformer Encoder] Output shape: torch.Size([32, 120, 64])
[STFTnet] Shape after taking last time step: torch.Size([32, 64])
  [Classifier] Input shape: torch.Size([32, 64])
  [Classifier] Output shape: torch.Size([32, 12])

[STFTnet] Initial input shape: torch.Size([32, 120, 7])
[STFTnet] Shape after permuting for conv: torch.Size([32, 7, 120])
  [MFA Block] Input shape: torch.Size([32, 7, 120])
  [MFA Block] After global average pooling shape: torch.Size([32, 7, 1])
  [MFA Block] Attention weights shape: torch.Size([32, 7, 1])
  [MFA Block] Output after attention weighting shape: torch.Size([32, 7, 120])
[STFTnet] Shape after channel projection conv: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Input shape: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Output shape: torch.Size([32, 64, 120])
[STFTnet] Shape after permuting for transformer: torch.Size([32, 120, 64])
  [Transformer Encoder] Input shape (N, L, C): torch.Size([32, 120, 64])
  [Transformer Encoder] Output shape: torch.Size([32, 120, 64])
[STFTnet] Shape after taking last time step: torch.Size([32, 64])
  [Classifier] Input shape: torch.Size([32, 64])
  [Classifier] Output shape: torch.Size([32, 12])

[STFTnet] Initial input shape: torch.Size([32, 120, 7])
[STFTnet] Shape after permuting for conv: torch.Size([32, 7, 120])
  [MFA Block] Input shape: torch.Size([32, 7, 120])
  [MFA Block] After global average pooling shape: torch.Size([32, 7, 1])
  [MFA Block] Attention weights shape: torch.Size([32, 7, 1])
  [MFA Block] Output after attention weighting shape: torch.Size([32, 7, 120])
[STFTnet] Shape after channel projection conv: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Input shape: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Output shape: torch.Size([32, 64, 120])
[STFTnet] Shape after permuting for transformer: torch.Size([32, 120, 64])
  [Transformer Encoder] Input shape (N, L, C): torch.Size([32, 120, 64])
  [Transformer Encoder] Output shape: torch.Size([32, 120, 64])
[STFTnet] Shape after taking last time step: torch.Size([32, 64])
  [Classifier] Input shape: torch.Size([32, 64])
  [Classifier] Output shape: torch.Size([32, 12])

[STFTnet] Initial input shape: torch.Size([32, 120, 7])
[STFTnet] Shape after permuting for conv: torch.Size([32, 7, 120])
  [MFA Block] Input shape: torch.Size([32, 7, 120])
  [MFA Block] After global average pooling shape: torch.Size([32, 7, 1])
  [MFA Block] Attention weights shape: torch.Size([32, 7, 1])
  [MFA Block] Output after attention weighting shape: torch.Size([32, 7, 120])
[STFTnet] Shape after channel projection conv: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Input shape: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Output shape: torch.Size([32, 64, 120])
[STFTnet] Shape after permuting for transformer: torch.Size([32, 120, 64])
  [Transformer Encoder] Input shape (N, L, C): torch.Size([32, 120, 64])
  [Transformer Encoder] Output shape: torch.Size([32, 120, 64])
[STFTnet] Shape after taking last time step: torch.Size([32, 64])
  [Classifier] Input shape: torch.Size([32, 64])
  [Classifier] Output shape: torch.Size([32, 12])

--- Completed a few batches for validation purposes; ending this epoch early. ---

========================================================
Epoch 19, Loss: 1.8267, Accuracy: 0.8385
========================================================


[STFTnet] Initial input shape: torch.Size([32, 120, 7])
[STFTnet] Shape after permuting for conv: torch.Size([32, 7, 120])
  [MFA Block] Input shape: torch.Size([32, 7, 120])
  [MFA Block] After global average pooling shape: torch.Size([32, 7, 1])
  [MFA Block] Attention weights shape: torch.Size([32, 7, 1])
  [MFA Block] Output after attention weighting shape: torch.Size([32, 7, 120])
[STFTnet] Shape after channel projection conv: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Input shape: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Output shape: torch.Size([32, 64, 120])
[STFTnet] Shape after permuting for transformer: torch.Size([32, 120, 64])
  [Transformer Encoder] Input shape (N, L, C): torch.Size([32, 120, 64])
  [Transformer Encoder] Output shape: torch.Size([32, 120, 64])
[STFTnet] Shape after taking last time step: torch.Size([32, 64])
  [Classifier] Input shape: torch.Size([32, 64])
  [Classifier] Output shape: torch.Size([32, 12])

[STFTnet] Initial input shape: torch.Size([32, 120, 7])
[STFTnet] Shape after permuting for conv: torch.Size([32, 7, 120])
  [MFA Block] Input shape: torch.Size([32, 7, 120])
  [MFA Block] After global average pooling shape: torch.Size([32, 7, 1])
  [MFA Block] Attention weights shape: torch.Size([32, 7, 1])
  [MFA Block] Output after attention weighting shape: torch.Size([32, 7, 120])
[STFTnet] Shape after channel projection conv: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Input shape: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Output shape: torch.Size([32, 64, 120])
[STFTnet] Shape after permuting for transformer: torch.Size([32, 120, 64])
  [Transformer Encoder] Input shape (N, L, C): torch.Size([32, 120, 64])
  [Transformer Encoder] Output shape: torch.Size([32, 120, 64])
[STFTnet] Shape after taking last time step: torch.Size([32, 64])
  [Classifier] Input shape: torch.Size([32, 64])
  [Classifier] Output shape: torch.Size([32, 12])

[STFTnet] Initial input shape: torch.Size([32, 120, 7])
[STFTnet] Shape after permuting for conv: torch.Size([32, 7, 120])
  [MFA Block] Input shape: torch.Size([32, 7, 120])
  [MFA Block] After global average pooling shape: torch.Size([32, 7, 1])
  [MFA Block] Attention weights shape: torch.Size([32, 7, 1])
  [MFA Block] Output after attention weighting shape: torch.Size([32, 7, 120])
[STFTnet] Shape after channel projection conv: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Input shape: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Output shape: torch.Size([32, 64, 120])
[STFTnet] Shape after permuting for transformer: torch.Size([32, 120, 64])
  [Transformer Encoder] Input shape (N, L, C): torch.Size([32, 120, 64])
  [Transformer Encoder] Output shape: torch.Size([32, 120, 64])
[STFTnet] Shape after taking last time step: torch.Size([32, 64])
  [Classifier] Input shape: torch.Size([32, 64])
  [Classifier] Output shape: torch.Size([32, 12])

[STFTnet] Initial input shape: torch.Size([32, 120, 7])
[STFTnet] Shape after permuting for conv: torch.Size([32, 7, 120])
  [MFA Block] Input shape: torch.Size([32, 7, 120])
  [MFA Block] After global average pooling shape: torch.Size([32, 7, 1])
  [MFA Block] Attention weights shape: torch.Size([32, 7, 1])
  [MFA Block] Output after attention weighting shape: torch.Size([32, 7, 120])
[STFTnet] Shape after channel projection conv: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Input shape: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Output shape: torch.Size([32, 64, 120])
[STFTnet] Shape after permuting for transformer: torch.Size([32, 120, 64])
  [Transformer Encoder] Input shape (N, L, C): torch.Size([32, 120, 64])
  [Transformer Encoder] Output shape: torch.Size([32, 120, 64])
[STFTnet] Shape after taking last time step: torch.Size([32, 64])
  [Classifier] Input shape: torch.Size([32, 64])
  [Classifier] Output shape: torch.Size([32, 12])

[STFTnet] Initial input shape: torch.Size([32, 120, 7])
[STFTnet] Shape after permuting for conv: torch.Size([32, 7, 120])
  [MFA Block] Input shape: torch.Size([32, 7, 120])
  [MFA Block] After global average pooling shape: torch.Size([32, 7, 1])
  [MFA Block] Attention weights shape: torch.Size([32, 7, 1])
  [MFA Block] Output after attention weighting shape: torch.Size([32, 7, 120])
[STFTnet] Shape after channel projection conv: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Input shape: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Output shape: torch.Size([32, 64, 120])
[STFTnet] Shape after permuting for transformer: torch.Size([32, 120, 64])
  [Transformer Encoder] Input shape (N, L, C): torch.Size([32, 120, 64])
  [Transformer Encoder] Output shape: torch.Size([32, 120, 64])
[STFTnet] Shape after taking last time step: torch.Size([32, 64])
  [Classifier] Input shape: torch.Size([32, 64])
  [Classifier] Output shape: torch.Size([32, 12])

[STFTnet] Initial input shape: torch.Size([32, 120, 7])
[STFTnet] Shape after permuting for conv: torch.Size([32, 7, 120])
  [MFA Block] Input shape: torch.Size([32, 7, 120])
  [MFA Block] After global average pooling shape: torch.Size([32, 7, 1])
  [MFA Block] Attention weights shape: torch.Size([32, 7, 1])
  [MFA Block] Output after attention weighting shape: torch.Size([32, 7, 120])
[STFTnet] Shape after channel projection conv: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Input shape: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Output shape: torch.Size([32, 64, 120])
[STFTnet] Shape after permuting for transformer: torch.Size([32, 120, 64])
  [Transformer Encoder] Input shape (N, L, C): torch.Size([32, 120, 64])
  [Transformer Encoder] Output shape: torch.Size([32, 120, 64])
[STFTnet] Shape after taking last time step: torch.Size([32, 64])
  [Classifier] Input shape: torch.Size([32, 64])
  [Classifier] Output shape: torch.Size([32, 12])

--- Completed a few batches for validation purposes; ending this epoch early. ---

========================================================
Epoch 20, Loss: 1.8202, Accuracy: 0.8490
========================================================


[STFTnet] Initial input shape: torch.Size([32, 120, 7])
[STFTnet] Shape after permuting for conv: torch.Size([32, 7, 120])
  [MFA Block] Input shape: torch.Size([32, 7, 120])
  [MFA Block] After global average pooling shape: torch.Size([32, 7, 1])
  [MFA Block] Attention weights shape: torch.Size([32, 7, 1])
  [MFA Block] Output after attention weighting shape: torch.Size([32, 7, 120])
[STFTnet] Shape after channel projection conv: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Input shape: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Output shape: torch.Size([32, 64, 120])
[STFTnet] Shape after permuting for transformer: torch.Size([32, 120, 64])
  [Transformer Encoder] Input shape (N, L, C): torch.Size([32, 120, 64])
  [Transformer Encoder] Output shape: torch.Size([32, 120, 64])
[STFTnet] Shape after taking last time step: torch.Size([32, 64])
  [Classifier] Input shape: torch.Size([32, 64])
  [Classifier] Output shape: torch.Size([32, 12])

[STFTnet] Initial input shape: torch.Size([32, 120, 7])
[STFTnet] Shape after permuting for conv: torch.Size([32, 7, 120])
  [MFA Block] Input shape: torch.Size([32, 7, 120])
  [MFA Block] After global average pooling shape: torch.Size([32, 7, 1])
  [MFA Block] Attention weights shape: torch.Size([32, 7, 1])
  [MFA Block] Output after attention weighting shape: torch.Size([32, 7, 120])
[STFTnet] Shape after channel projection conv: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Input shape: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Output shape: torch.Size([32, 64, 120])
[STFTnet] Shape after permuting for transformer: torch.Size([32, 120, 64])
  [Transformer Encoder] Input shape (N, L, C): torch.Size([32, 120, 64])
  [Transformer Encoder] Output shape: torch.Size([32, 120, 64])
[STFTnet] Shape after taking last time step: torch.Size([32, 64])
  [Classifier] Input shape: torch.Size([32, 64])
  [Classifier] Output shape: torch.Size([32, 12])

[STFTnet] Initial input shape: torch.Size([32, 120, 7])
[STFTnet] Shape after permuting for conv: torch.Size([32, 7, 120])
  [MFA Block] Input shape: torch.Size([32, 7, 120])
  [MFA Block] After global average pooling shape: torch.Size([32, 7, 1])
  [MFA Block] Attention weights shape: torch.Size([32, 7, 1])
  [MFA Block] Output after attention weighting shape: torch.Size([32, 7, 120])
[STFTnet] Shape after channel projection conv: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Input shape: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Output shape: torch.Size([32, 64, 120])
[STFTnet] Shape after permuting for transformer: torch.Size([32, 120, 64])
  [Transformer Encoder] Input shape (N, L, C): torch.Size([32, 120, 64])
  [Transformer Encoder] Output shape: torch.Size([32, 120, 64])
[STFTnet] Shape after taking last time step: torch.Size([32, 64])
  [Classifier] Input shape: torch.Size([32, 64])
  [Classifier] Output shape: torch.Size([32, 12])

[STFTnet] Initial input shape: torch.Size([32, 120, 7])
[STFTnet] Shape after permuting for conv: torch.Size([32, 7, 120])
  [MFA Block] Input shape: torch.Size([32, 7, 120])
  [MFA Block] After global average pooling shape: torch.Size([32, 7, 1])
  [MFA Block] Attention weights shape: torch.Size([32, 7, 1])
  [MFA Block] Output after attention weighting shape: torch.Size([32, 7, 120])
[STFTnet] Shape after channel projection conv: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Input shape: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Output shape: torch.Size([32, 64, 120])
[STFTnet] Shape after permuting for transformer: torch.Size([32, 120, 64])
  [Transformer Encoder] Input shape (N, L, C): torch.Size([32, 120, 64])
  [Transformer Encoder] Output shape: torch.Size([32, 120, 64])
[STFTnet] Shape after taking last time step: torch.Size([32, 64])
  [Classifier] Input shape: torch.Size([32, 64])
  [Classifier] Output shape: torch.Size([32, 12])

[STFTnet] Initial input shape: torch.Size([32, 120, 7])
[STFTnet] Shape after permuting for conv: torch.Size([32, 7, 120])
  [MFA Block] Input shape: torch.Size([32, 7, 120])
  [MFA Block] After global average pooling shape: torch.Size([32, 7, 1])
  [MFA Block] Attention weights shape: torch.Size([32, 7, 1])
  [MFA Block] Output after attention weighting shape: torch.Size([32, 7, 120])
[STFTnet] Shape after channel projection conv: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Input shape: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Output shape: torch.Size([32, 64, 120])
[STFTnet] Shape after permuting for transformer: torch.Size([32, 120, 64])
  [Transformer Encoder] Input shape (N, L, C): torch.Size([32, 120, 64])
  [Transformer Encoder] Output shape: torch.Size([32, 120, 64])
[STFTnet] Shape after taking last time step: torch.Size([32, 64])
  [Classifier] Input shape: torch.Size([32, 64])
  [Classifier] Output shape: torch.Size([32, 12])

[STFTnet] Initial input shape: torch.Size([32, 120, 7])
[STFTnet] Shape after permuting for conv: torch.Size([32, 7, 120])
  [MFA Block] Input shape: torch.Size([32, 7, 120])
  [MFA Block] After global average pooling shape: torch.Size([32, 7, 1])
  [MFA Block] Attention weights shape: torch.Size([32, 7, 1])
  [MFA Block] Output after attention weighting shape: torch.Size([32, 7, 120])
[STFTnet] Shape after channel projection conv: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Input shape: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Output shape: torch.Size([32, 64, 120])
[STFTnet] Shape after permuting for transformer: torch.Size([32, 120, 64])
  [Transformer Encoder] Input shape (N, L, C): torch.Size([32, 120, 64])
  [Transformer Encoder] Output shape: torch.Size([32, 120, 64])
[STFTnet] Shape after taking last time step: torch.Size([32, 64])
  [Classifier] Input shape: torch.Size([32, 64])
  [Classifier] Output shape: torch.Size([32, 12])

--- Completed a few batches for validation purposes; ending this epoch early. ---

========================================================
Epoch 21, Loss: 1.7899, Accuracy: 0.8698
========================================================


[STFTnet] Initial input shape: torch.Size([32, 120, 7])
[STFTnet] Shape after permuting for conv: torch.Size([32, 7, 120])
  [MFA Block] Input shape: torch.Size([32, 7, 120])
  [MFA Block] After global average pooling shape: torch.Size([32, 7, 1])
  [MFA Block] Attention weights shape: torch.Size([32, 7, 1])
  [MFA Block] Output after attention weighting shape: torch.Size([32, 7, 120])
[STFTnet] Shape after channel projection conv: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Input shape: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Output shape: torch.Size([32, 64, 120])
[STFTnet] Shape after permuting for transformer: torch.Size([32, 120, 64])
  [Transformer Encoder] Input shape (N, L, C): torch.Size([32, 120, 64])
  [Transformer Encoder] Output shape: torch.Size([32, 120, 64])
[STFTnet] Shape after taking last time step: torch.Size([32, 64])
  [Classifier] Input shape: torch.Size([32, 64])
  [Classifier] Output shape: torch.Size([32, 12])

[STFTnet] Initial input shape: torch.Size([32, 120, 7])
[STFTnet] Shape after permuting for conv: torch.Size([32, 7, 120])
  [MFA Block] Input shape: torch.Size([32, 7, 120])
  [MFA Block] After global average pooling shape: torch.Size([32, 7, 1])
  [MFA Block] Attention weights shape: torch.Size([32, 7, 1])
  [MFA Block] Output after attention weighting shape: torch.Size([32, 7, 120])
[STFTnet] Shape after channel projection conv: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Input shape: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Output shape: torch.Size([32, 64, 120])
[STFTnet] Shape after permuting for transformer: torch.Size([32, 120, 64])
  [Transformer Encoder] Input shape (N, L, C): torch.Size([32, 120, 64])
  [Transformer Encoder] Output shape: torch.Size([32, 120, 64])
[STFTnet] Shape after taking last time step: torch.Size([32, 64])
  [Classifier] Input shape: torch.Size([32, 64])
  [Classifier] Output shape: torch.Size([32, 12])

[STFTnet] Initial input shape: torch.Size([32, 120, 7])
[STFTnet] Shape after permuting for conv: torch.Size([32, 7, 120])
  [MFA Block] Input shape: torch.Size([32, 7, 120])
  [MFA Block] After global average pooling shape: torch.Size([32, 7, 1])
  [MFA Block] Attention weights shape: torch.Size([32, 7, 1])
  [MFA Block] Output after attention weighting shape: torch.Size([32, 7, 120])
[STFTnet] Shape after channel projection conv: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Input shape: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Output shape: torch.Size([32, 64, 120])
[STFTnet] Shape after permuting for transformer: torch.Size([32, 120, 64])
  [Transformer Encoder] Input shape (N, L, C): torch.Size([32, 120, 64])
  [Transformer Encoder] Output shape: torch.Size([32, 120, 64])
[STFTnet] Shape after taking last time step: torch.Size([32, 64])
  [Classifier] Input shape: torch.Size([32, 64])
  [Classifier] Output shape: torch.Size([32, 12])

[STFTnet] Initial input shape: torch.Size([32, 120, 7])
[STFTnet] Shape after permuting for conv: torch.Size([32, 7, 120])
  [MFA Block] Input shape: torch.Size([32, 7, 120])
  [MFA Block] After global average pooling shape: torch.Size([32, 7, 1])
  [MFA Block] Attention weights shape: torch.Size([32, 7, 1])
  [MFA Block] Output after attention weighting shape: torch.Size([32, 7, 120])
[STFTnet] Shape after channel projection conv: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Input shape: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Output shape: torch.Size([32, 64, 120])
[STFTnet] Shape after permuting for transformer: torch.Size([32, 120, 64])
  [Transformer Encoder] Input shape (N, L, C): torch.Size([32, 120, 64])
  [Transformer Encoder] Output shape: torch.Size([32, 120, 64])
[STFTnet] Shape after taking last time step: torch.Size([32, 64])
  [Classifier] Input shape: torch.Size([32, 64])
  [Classifier] Output shape: torch.Size([32, 12])

[STFTnet] Initial input shape: torch.Size([32, 120, 7])
[STFTnet] Shape after permuting for conv: torch.Size([32, 7, 120])
  [MFA Block] Input shape: torch.Size([32, 7, 120])
  [MFA Block] After global average pooling shape: torch.Size([32, 7, 1])
  [MFA Block] Attention weights shape: torch.Size([32, 7, 1])
  [MFA Block] Output after attention weighting shape: torch.Size([32, 7, 120])
[STFTnet] Shape after channel projection conv: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Input shape: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Output shape: torch.Size([32, 64, 120])
[STFTnet] Shape after permuting for transformer: torch.Size([32, 120, 64])
  [Transformer Encoder] Input shape (N, L, C): torch.Size([32, 120, 64])
  [Transformer Encoder] Output shape: torch.Size([32, 120, 64])
[STFTnet] Shape after taking last time step: torch.Size([32, 64])
  [Classifier] Input shape: torch.Size([32, 64])
  [Classifier] Output shape: torch.Size([32, 12])

[STFTnet] Initial input shape: torch.Size([32, 120, 7])
[STFTnet] Shape after permuting for conv: torch.Size([32, 7, 120])
  [MFA Block] Input shape: torch.Size([32, 7, 120])
  [MFA Block] After global average pooling shape: torch.Size([32, 7, 1])
  [MFA Block] Attention weights shape: torch.Size([32, 7, 1])
  [MFA Block] Output after attention weighting shape: torch.Size([32, 7, 120])
[STFTnet] Shape after channel projection conv: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Input shape: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Output shape: torch.Size([32, 64, 120])
[STFTnet] Shape after permuting for transformer: torch.Size([32, 120, 64])
  [Transformer Encoder] Input shape (N, L, C): torch.Size([32, 120, 64])
  [Transformer Encoder] Output shape: torch.Size([32, 120, 64])
[STFTnet] Shape after taking last time step: torch.Size([32, 64])
  [Classifier] Input shape: torch.Size([32, 64])
  [Classifier] Output shape: torch.Size([32, 12])

--- Completed a few batches for validation purposes; ending this epoch early. ---

========================================================
Epoch 22, Loss: 1.7803, Accuracy: 0.8906
========================================================


[STFTnet] Initial input shape: torch.Size([32, 120, 7])
[STFTnet] Shape after permuting for conv: torch.Size([32, 7, 120])
  [MFA Block] Input shape: torch.Size([32, 7, 120])
  [MFA Block] After global average pooling shape: torch.Size([32, 7, 1])
  [MFA Block] Attention weights shape: torch.Size([32, 7, 1])
  [MFA Block] Output after attention weighting shape: torch.Size([32, 7, 120])
[STFTnet] Shape after channel projection conv: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Input shape: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Output shape: torch.Size([32, 64, 120])
[STFTnet] Shape after permuting for transformer: torch.Size([32, 120, 64])
  [Transformer Encoder] Input shape (N, L, C): torch.Size([32, 120, 64])
  [Transformer Encoder] Output shape: torch.Size([32, 120, 64])
[STFTnet] Shape after taking last time step: torch.Size([32, 64])
  [Classifier] Input shape: torch.Size([32, 64])
  [Classifier] Output shape: torch.Size([32, 12])

[STFTnet] Initial input shape: torch.Size([32, 120, 7])
[STFTnet] Shape after permuting for conv: torch.Size([32, 7, 120])
  [MFA Block] Input shape: torch.Size([32, 7, 120])
  [MFA Block] After global average pooling shape: torch.Size([32, 7, 1])
  [MFA Block] Attention weights shape: torch.Size([32, 7, 1])
  [MFA Block] Output after attention weighting shape: torch.Size([32, 7, 120])
[STFTnet] Shape after channel projection conv: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Input shape: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Output shape: torch.Size([32, 64, 120])
[STFTnet] Shape after permuting for transformer: torch.Size([32, 120, 64])
  [Transformer Encoder] Input shape (N, L, C): torch.Size([32, 120, 64])
  [Transformer Encoder] Output shape: torch.Size([32, 120, 64])
[STFTnet] Shape after taking last time step: torch.Size([32, 64])
  [Classifier] Input shape: torch.Size([32, 64])
  [Classifier] Output shape: torch.Size([32, 12])

[STFTnet] Initial input shape: torch.Size([32, 120, 7])
[STFTnet] Shape after permuting for conv: torch.Size([32, 7, 120])
  [MFA Block] Input shape: torch.Size([32, 7, 120])
  [MFA Block] After global average pooling shape: torch.Size([32, 7, 1])
  [MFA Block] Attention weights shape: torch.Size([32, 7, 1])
  [MFA Block] Output after attention weighting shape: torch.Size([32, 7, 120])
[STFTnet] Shape after channel projection conv: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Input shape: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Output shape: torch.Size([32, 64, 120])
[STFTnet] Shape after permuting for transformer: torch.Size([32, 120, 64])
  [Transformer Encoder] Input shape (N, L, C): torch.Size([32, 120, 64])
  [Transformer Encoder] Output shape: torch.Size([32, 120, 64])
[STFTnet] Shape after taking last time step: torch.Size([32, 64])
  [Classifier] Input shape: torch.Size([32, 64])
  [Classifier] Output shape: torch.Size([32, 12])

[STFTnet] Initial input shape: torch.Size([32, 120, 7])
[STFTnet] Shape after permuting for conv: torch.Size([32, 7, 120])
  [MFA Block] Input shape: torch.Size([32, 7, 120])
  [MFA Block] After global average pooling shape: torch.Size([32, 7, 1])
  [MFA Block] Attention weights shape: torch.Size([32, 7, 1])
  [MFA Block] Output after attention weighting shape: torch.Size([32, 7, 120])
[STFTnet] Shape after channel projection conv: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Input shape: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Output shape: torch.Size([32, 64, 120])
[STFTnet] Shape after permuting for transformer: torch.Size([32, 120, 64])
  [Transformer Encoder] Input shape (N, L, C): torch.Size([32, 120, 64])
  [Transformer Encoder] Output shape: torch.Size([32, 120, 64])
[STFTnet] Shape after taking last time step: torch.Size([32, 64])
  [Classifier] Input shape: torch.Size([32, 64])
  [Classifier] Output shape: torch.Size([32, 12])

[STFTnet] Initial input shape: torch.Size([32, 120, 7])
[STFTnet] Shape after permuting for conv: torch.Size([32, 7, 120])
  [MFA Block] Input shape: torch.Size([32, 7, 120])
  [MFA Block] After global average pooling shape: torch.Size([32, 7, 1])
  [MFA Block] Attention weights shape: torch.Size([32, 7, 1])
  [MFA Block] Output after attention weighting shape: torch.Size([32, 7, 120])
[STFTnet] Shape after channel projection conv: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Input shape: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Output shape: torch.Size([32, 64, 120])
[STFTnet] Shape after permuting for transformer: torch.Size([32, 120, 64])
  [Transformer Encoder] Input shape (N, L, C): torch.Size([32, 120, 64])
  [Transformer Encoder] Output shape: torch.Size([32, 120, 64])
[STFTnet] Shape after taking last time step: torch.Size([32, 64])
  [Classifier] Input shape: torch.Size([32, 64])
  [Classifier] Output shape: torch.Size([32, 12])

[STFTnet] Initial input shape: torch.Size([32, 120, 7])
[STFTnet] Shape after permuting for conv: torch.Size([32, 7, 120])
  [MFA Block] Input shape: torch.Size([32, 7, 120])
  [MFA Block] After global average pooling shape: torch.Size([32, 7, 1])
  [MFA Block] Attention weights shape: torch.Size([32, 7, 1])
  [MFA Block] Output after attention weighting shape: torch.Size([32, 7, 120])
[STFTnet] Shape after channel projection conv: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Input shape: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Output shape: torch.Size([32, 64, 120])
[STFTnet] Shape after permuting for transformer: torch.Size([32, 120, 64])
  [Transformer Encoder] Input shape (N, L, C): torch.Size([32, 120, 64])
  [Transformer Encoder] Output shape: torch.Size([32, 120, 64])
[STFTnet] Shape after taking last time step: torch.Size([32, 64])
  [Classifier] Input shape: torch.Size([32, 64])
  [Classifier] Output shape: torch.Size([32, 12])

--- Completed a few batches for validation purposes; ending this epoch early. ---

========================================================
Epoch 23, Loss: 1.7281, Accuracy: 0.9323
========================================================


[STFTnet] Initial input shape: torch.Size([32, 120, 7])
[STFTnet] Shape after permuting for conv: torch.Size([32, 7, 120])
  [MFA Block] Input shape: torch.Size([32, 7, 120])
  [MFA Block] After global average pooling shape: torch.Size([32, 7, 1])
  [MFA Block] Attention weights shape: torch.Size([32, 7, 1])
  [MFA Block] Output after attention weighting shape: torch.Size([32, 7, 120])
[STFTnet] Shape after channel projection conv: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Input shape: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Output shape: torch.Size([32, 64, 120])
[STFTnet] Shape after permuting for transformer: torch.Size([32, 120, 64])
  [Transformer Encoder] Input shape (N, L, C): torch.Size([32, 120, 64])
  [Transformer Encoder] Output shape: torch.Size([32, 120, 64])
[STFTnet] Shape after taking last time step: torch.Size([32, 64])
  [Classifier] Input shape: torch.Size([32, 64])
  [Classifier] Output shape: torch.Size([32, 12])

[STFTnet] Initial input shape: torch.Size([32, 120, 7])
[STFTnet] Shape after permuting for conv: torch.Size([32, 7, 120])
  [MFA Block] Input shape: torch.Size([32, 7, 120])
  [MFA Block] After global average pooling shape: torch.Size([32, 7, 1])
  [MFA Block] Attention weights shape: torch.Size([32, 7, 1])
  [MFA Block] Output after attention weighting shape: torch.Size([32, 7, 120])
[STFTnet] Shape after channel projection conv: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Input shape: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Output shape: torch.Size([32, 64, 120])
[STFTnet] Shape after permuting for transformer: torch.Size([32, 120, 64])
  [Transformer Encoder] Input shape (N, L, C): torch.Size([32, 120, 64])
  [Transformer Encoder] Output shape: torch.Size([32, 120, 64])
[STFTnet] Shape after taking last time step: torch.Size([32, 64])
  [Classifier] Input shape: torch.Size([32, 64])
  [Classifier] Output shape: torch.Size([32, 12])

[STFTnet] Initial input shape: torch.Size([32, 120, 7])
[STFTnet] Shape after permuting for conv: torch.Size([32, 7, 120])
  [MFA Block] Input shape: torch.Size([32, 7, 120])
  [MFA Block] After global average pooling shape: torch.Size([32, 7, 1])
  [MFA Block] Attention weights shape: torch.Size([32, 7, 1])
  [MFA Block] Output after attention weighting shape: torch.Size([32, 7, 120])
[STFTnet] Shape after channel projection conv: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Input shape: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Output shape: torch.Size([32, 64, 120])
[STFTnet] Shape after permuting for transformer: torch.Size([32, 120, 64])
  [Transformer Encoder] Input shape (N, L, C): torch.Size([32, 120, 64])
  [Transformer Encoder] Output shape: torch.Size([32, 120, 64])
[STFTnet] Shape after taking last time step: torch.Size([32, 64])
  [Classifier] Input shape: torch.Size([32, 64])
  [Classifier] Output shape: torch.Size([32, 12])

[STFTnet] Initial input shape: torch.Size([32, 120, 7])
[STFTnet] Shape after permuting for conv: torch.Size([32, 7, 120])
  [MFA Block] Input shape: torch.Size([32, 7, 120])
  [MFA Block] After global average pooling shape: torch.Size([32, 7, 1])
  [MFA Block] Attention weights shape: torch.Size([32, 7, 1])
  [MFA Block] Output after attention weighting shape: torch.Size([32, 7, 120])
[STFTnet] Shape after channel projection conv: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Input shape: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Output shape: torch.Size([32, 64, 120])
[STFTnet] Shape after permuting for transformer: torch.Size([32, 120, 64])
  [Transformer Encoder] Input shape (N, L, C): torch.Size([32, 120, 64])
  [Transformer Encoder] Output shape: torch.Size([32, 120, 64])
[STFTnet] Shape after taking last time step: torch.Size([32, 64])
  [Classifier] Input shape: torch.Size([32, 64])
  [Classifier] Output shape: torch.Size([32, 12])

[STFTnet] Initial input shape: torch.Size([32, 120, 7])
[STFTnet] Shape after permuting for conv: torch.Size([32, 7, 120])
  [MFA Block] Input shape: torch.Size([32, 7, 120])
  [MFA Block] After global average pooling shape: torch.Size([32, 7, 1])
  [MFA Block] Attention weights shape: torch.Size([32, 7, 1])
  [MFA Block] Output after attention weighting shape: torch.Size([32, 7, 120])
[STFTnet] Shape after channel projection conv: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Input shape: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Output shape: torch.Size([32, 64, 120])
[STFTnet] Shape after permuting for transformer: torch.Size([32, 120, 64])
  [Transformer Encoder] Input shape (N, L, C): torch.Size([32, 120, 64])
  [Transformer Encoder] Output shape: torch.Size([32, 120, 64])
[STFTnet] Shape after taking last time step: torch.Size([32, 64])
  [Classifier] Input shape: torch.Size([32, 64])
  [Classifier] Output shape: torch.Size([32, 12])

[STFTnet] Initial input shape: torch.Size([32, 120, 7])
[STFTnet] Shape after permuting for conv: torch.Size([32, 7, 120])
  [MFA Block] Input shape: torch.Size([32, 7, 120])
  [MFA Block] After global average pooling shape: torch.Size([32, 7, 1])
  [MFA Block] Attention weights shape: torch.Size([32, 7, 1])
  [MFA Block] Output after attention weighting shape: torch.Size([32, 7, 120])
[STFTnet] Shape after channel projection conv: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Input shape: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Output shape: torch.Size([32, 64, 120])
[STFTnet] Shape after permuting for transformer: torch.Size([32, 120, 64])
  [Transformer Encoder] Input shape (N, L, C): torch.Size([32, 120, 64])
  [Transformer Encoder] Output shape: torch.Size([32, 120, 64])
[STFTnet] Shape after taking last time step: torch.Size([32, 64])
  [Classifier] Input shape: torch.Size([32, 64])
  [Classifier] Output shape: torch.Size([32, 12])

--- Completed a few batches for validation purposes; ending this epoch early. ---

========================================================
Epoch 24, Loss: 1.7464, Accuracy: 0.9062
========================================================


[STFTnet] Initial input shape: torch.Size([32, 120, 7])
[STFTnet] Shape after permuting for conv: torch.Size([32, 7, 120])
  [MFA Block] Input shape: torch.Size([32, 7, 120])
  [MFA Block] After global average pooling shape: torch.Size([32, 7, 1])
  [MFA Block] Attention weights shape: torch.Size([32, 7, 1])
  [MFA Block] Output after attention weighting shape: torch.Size([32, 7, 120])
[STFTnet] Shape after channel projection conv: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Input shape: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Output shape: torch.Size([32, 64, 120])
[STFTnet] Shape after permuting for transformer: torch.Size([32, 120, 64])
  [Transformer Encoder] Input shape (N, L, C): torch.Size([32, 120, 64])
  [Transformer Encoder] Output shape: torch.Size([32, 120, 64])
[STFTnet] Shape after taking last time step: torch.Size([32, 64])
  [Classifier] Input shape: torch.Size([32, 64])
  [Classifier] Output shape: torch.Size([32, 12])

[STFTnet] Initial input shape: torch.Size([32, 120, 7])
[STFTnet] Shape after permuting for conv: torch.Size([32, 7, 120])
  [MFA Block] Input shape: torch.Size([32, 7, 120])
  [MFA Block] After global average pooling shape: torch.Size([32, 7, 1])
  [MFA Block] Attention weights shape: torch.Size([32, 7, 1])
  [MFA Block] Output after attention weighting shape: torch.Size([32, 7, 120])
[STFTnet] Shape after channel projection conv: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Input shape: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Output shape: torch.Size([32, 64, 120])
[STFTnet] Shape after permuting for transformer: torch.Size([32, 120, 64])
  [Transformer Encoder] Input shape (N, L, C): torch.Size([32, 120, 64])
  [Transformer Encoder] Output shape: torch.Size([32, 120, 64])
[STFTnet] Shape after taking last time step: torch.Size([32, 64])
  [Classifier] Input shape: torch.Size([32, 64])
  [Classifier] Output shape: torch.Size([32, 12])

[STFTnet] Initial input shape: torch.Size([32, 120, 7])
[STFTnet] Shape after permuting for conv: torch.Size([32, 7, 120])
  [MFA Block] Input shape: torch.Size([32, 7, 120])
  [MFA Block] After global average pooling shape: torch.Size([32, 7, 1])
  [MFA Block] Attention weights shape: torch.Size([32, 7, 1])
  [MFA Block] Output after attention weighting shape: torch.Size([32, 7, 120])
[STFTnet] Shape after channel projection conv: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Input shape: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Output shape: torch.Size([32, 64, 120])
[STFTnet] Shape after permuting for transformer: torch.Size([32, 120, 64])
  [Transformer Encoder] Input shape (N, L, C): torch.Size([32, 120, 64])
  [Transformer Encoder] Output shape: torch.Size([32, 120, 64])
[STFTnet] Shape after taking last time step: torch.Size([32, 64])
  [Classifier] Input shape: torch.Size([32, 64])
  [Classifier] Output shape: torch.Size([32, 12])

[STFTnet] Initial input shape: torch.Size([32, 120, 7])
[STFTnet] Shape after permuting for conv: torch.Size([32, 7, 120])
  [MFA Block] Input shape: torch.Size([32, 7, 120])
  [MFA Block] After global average pooling shape: torch.Size([32, 7, 1])
  [MFA Block] Attention weights shape: torch.Size([32, 7, 1])
  [MFA Block] Output after attention weighting shape: torch.Size([32, 7, 120])
[STFTnet] Shape after channel projection conv: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Input shape: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Output shape: torch.Size([32, 64, 120])
[STFTnet] Shape after permuting for transformer: torch.Size([32, 120, 64])
  [Transformer Encoder] Input shape (N, L, C): torch.Size([32, 120, 64])
  [Transformer Encoder] Output shape: torch.Size([32, 120, 64])
[STFTnet] Shape after taking last time step: torch.Size([32, 64])
  [Classifier] Input shape: torch.Size([32, 64])
  [Classifier] Output shape: torch.Size([32, 12])

[STFTnet] Initial input shape: torch.Size([32, 120, 7])
[STFTnet] Shape after permuting for conv: torch.Size([32, 7, 120])
  [MFA Block] Input shape: torch.Size([32, 7, 120])
  [MFA Block] After global average pooling shape: torch.Size([32, 7, 1])
  [MFA Block] Attention weights shape: torch.Size([32, 7, 1])
  [MFA Block] Output after attention weighting shape: torch.Size([32, 7, 120])
[STFTnet] Shape after channel projection conv: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Input shape: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Output shape: torch.Size([32, 64, 120])
[STFTnet] Shape after permuting for transformer: torch.Size([32, 120, 64])
  [Transformer Encoder] Input shape (N, L, C): torch.Size([32, 120, 64])
  [Transformer Encoder] Output shape: torch.Size([32, 120, 64])
[STFTnet] Shape after taking last time step: torch.Size([32, 64])
  [Classifier] Input shape: torch.Size([32, 64])
  [Classifier] Output shape: torch.Size([32, 12])

[STFTnet] Initial input shape: torch.Size([32, 120, 7])
[STFTnet] Shape after permuting for conv: torch.Size([32, 7, 120])
  [MFA Block] Input shape: torch.Size([32, 7, 120])
  [MFA Block] After global average pooling shape: torch.Size([32, 7, 1])
  [MFA Block] Attention weights shape: torch.Size([32, 7, 1])
  [MFA Block] Output after attention weighting shape: torch.Size([32, 7, 120])
[STFTnet] Shape after channel projection conv: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Input shape: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Output shape: torch.Size([32, 64, 120])
[STFTnet] Shape after permuting for transformer: torch.Size([32, 120, 64])
  [Transformer Encoder] Input shape (N, L, C): torch.Size([32, 120, 64])
  [Transformer Encoder] Output shape: torch.Size([32, 120, 64])
[STFTnet] Shape after taking last time step: torch.Size([32, 64])
  [Classifier] Input shape: torch.Size([32, 64])
  [Classifier] Output shape: torch.Size([32, 12])

--- Completed a few batches for validation purposes; ending this epoch early. ---

========================================================
Epoch 25, Loss: 1.6956, Accuracy: 0.9479
========================================================


[STFTnet] Initial input shape: torch.Size([32, 120, 7])
[STFTnet] Shape after permuting for conv: torch.Size([32, 7, 120])
  [MFA Block] Input shape: torch.Size([32, 7, 120])
  [MFA Block] After global average pooling shape: torch.Size([32, 7, 1])
  [MFA Block] Attention weights shape: torch.Size([32, 7, 1])
  [MFA Block] Output after attention weighting shape: torch.Size([32, 7, 120])
[STFTnet] Shape after channel projection conv: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Input shape: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Output shape: torch.Size([32, 64, 120])
[STFTnet] Shape after permuting for transformer: torch.Size([32, 120, 64])
  [Transformer Encoder] Input shape (N, L, C): torch.Size([32, 120, 64])
  [Transformer Encoder] Output shape: torch.Size([32, 120, 64])
[STFTnet] Shape after taking last time step: torch.Size([32, 64])
  [Classifier] Input shape: torch.Size([32, 64])
  [Classifier] Output shape: torch.Size([32, 12])

[STFTnet] Initial input shape: torch.Size([32, 120, 7])
[STFTnet] Shape after permuting for conv: torch.Size([32, 7, 120])
  [MFA Block] Input shape: torch.Size([32, 7, 120])
  [MFA Block] After global average pooling shape: torch.Size([32, 7, 1])
  [MFA Block] Attention weights shape: torch.Size([32, 7, 1])
  [MFA Block] Output after attention weighting shape: torch.Size([32, 7, 120])
[STFTnet] Shape after channel projection conv: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Input shape: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Output shape: torch.Size([32, 64, 120])
[STFTnet] Shape after permuting for transformer: torch.Size([32, 120, 64])
  [Transformer Encoder] Input shape (N, L, C): torch.Size([32, 120, 64])
  [Transformer Encoder] Output shape: torch.Size([32, 120, 64])
[STFTnet] Shape after taking last time step: torch.Size([32, 64])
  [Classifier] Input shape: torch.Size([32, 64])
  [Classifier] Output shape: torch.Size([32, 12])

[STFTnet] Initial input shape: torch.Size([32, 120, 7])
[STFTnet] Shape after permuting for conv: torch.Size([32, 7, 120])
  [MFA Block] Input shape: torch.Size([32, 7, 120])
  [MFA Block] After global average pooling shape: torch.Size([32, 7, 1])
  [MFA Block] Attention weights shape: torch.Size([32, 7, 1])
  [MFA Block] Output after attention weighting shape: torch.Size([32, 7, 120])
[STFTnet] Shape after channel projection conv: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Input shape: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Output shape: torch.Size([32, 64, 120])
[STFTnet] Shape after permuting for transformer: torch.Size([32, 120, 64])
  [Transformer Encoder] Input shape (N, L, C): torch.Size([32, 120, 64])
  [Transformer Encoder] Output shape: torch.Size([32, 120, 64])
[STFTnet] Shape after taking last time step: torch.Size([32, 64])
  [Classifier] Input shape: torch.Size([32, 64])
  [Classifier] Output shape: torch.Size([32, 12])

[STFTnet] Initial input shape: torch.Size([32, 120, 7])
[STFTnet] Shape after permuting for conv: torch.Size([32, 7, 120])
  [MFA Block] Input shape: torch.Size([32, 7, 120])
  [MFA Block] After global average pooling shape: torch.Size([32, 7, 1])
  [MFA Block] Attention weights shape: torch.Size([32, 7, 1])
  [MFA Block] Output after attention weighting shape: torch.Size([32, 7, 120])
[STFTnet] Shape after channel projection conv: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Input shape: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Output shape: torch.Size([32, 64, 120])
[STFTnet] Shape after permuting for transformer: torch.Size([32, 120, 64])
  [Transformer Encoder] Input shape (N, L, C): torch.Size([32, 120, 64])
  [Transformer Encoder] Output shape: torch.Size([32, 120, 64])
[STFTnet] Shape after taking last time step: torch.Size([32, 64])
  [Classifier] Input shape: torch.Size([32, 64])
  [Classifier] Output shape: torch.Size([32, 12])

[STFTnet] Initial input shape: torch.Size([32, 120, 7])
[STFTnet] Shape after permuting for conv: torch.Size([32, 7, 120])
  [MFA Block] Input shape: torch.Size([32, 7, 120])
  [MFA Block] After global average pooling shape: torch.Size([32, 7, 1])
  [MFA Block] Attention weights shape: torch.Size([32, 7, 1])
  [MFA Block] Output after attention weighting shape: torch.Size([32, 7, 120])
[STFTnet] Shape after channel projection conv: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Input shape: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Output shape: torch.Size([32, 64, 120])
[STFTnet] Shape after permuting for transformer: torch.Size([32, 120, 64])
  [Transformer Encoder] Input shape (N, L, C): torch.Size([32, 120, 64])
  [Transformer Encoder] Output shape: torch.Size([32, 120, 64])
[STFTnet] Shape after taking last time step: torch.Size([32, 64])
  [Classifier] Input shape: torch.Size([32, 64])
  [Classifier] Output shape: torch.Size([32, 12])

[STFTnet] Initial input shape: torch.Size([32, 120, 7])
[STFTnet] Shape after permuting for conv: torch.Size([32, 7, 120])
  [MFA Block] Input shape: torch.Size([32, 7, 120])
  [MFA Block] After global average pooling shape: torch.Size([32, 7, 1])
  [MFA Block] Attention weights shape: torch.Size([32, 7, 1])
  [MFA Block] Output after attention weighting shape: torch.Size([32, 7, 120])
[STFTnet] Shape after channel projection conv: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Input shape: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Output shape: torch.Size([32, 64, 120])
[STFTnet] Shape after permuting for transformer: torch.Size([32, 120, 64])
  [Transformer Encoder] Input shape (N, L, C): torch.Size([32, 120, 64])
  [Transformer Encoder] Output shape: torch.Size([32, 120, 64])
[STFTnet] Shape after taking last time step: torch.Size([32, 64])
  [Classifier] Input shape: torch.Size([32, 64])
  [Classifier] Output shape: torch.Size([32, 12])

--- Completed a few batches for validation purposes; ending this epoch early. ---

========================================================
Epoch 26, Loss: 1.7231, Accuracy: 0.9115
========================================================


[STFTnet] Initial input shape: torch.Size([32, 120, 7])
[STFTnet] Shape after permuting for conv: torch.Size([32, 7, 120])
  [MFA Block] Input shape: torch.Size([32, 7, 120])
  [MFA Block] After global average pooling shape: torch.Size([32, 7, 1])
  [MFA Block] Attention weights shape: torch.Size([32, 7, 1])
  [MFA Block] Output after attention weighting shape: torch.Size([32, 7, 120])
[STFTnet] Shape after channel projection conv: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Input shape: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Output shape: torch.Size([32, 64, 120])
[STFTnet] Shape after permuting for transformer: torch.Size([32, 120, 64])
  [Transformer Encoder] Input shape (N, L, C): torch.Size([32, 120, 64])
  [Transformer Encoder] Output shape: torch.Size([32, 120, 64])
[STFTnet] Shape after taking last time step: torch.Size([32, 64])
  [Classifier] Input shape: torch.Size([32, 64])
  [Classifier] Output shape: torch.Size([32, 12])

[STFTnet] Initial input shape: torch.Size([32, 120, 7])
[STFTnet] Shape after permuting for conv: torch.Size([32, 7, 120])
  [MFA Block] Input shape: torch.Size([32, 7, 120])
  [MFA Block] After global average pooling shape: torch.Size([32, 7, 1])
  [MFA Block] Attention weights shape: torch.Size([32, 7, 1])
  [MFA Block] Output after attention weighting shape: torch.Size([32, 7, 120])
[STFTnet] Shape after channel projection conv: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Input shape: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Output shape: torch.Size([32, 64, 120])
[STFTnet] Shape after permuting for transformer: torch.Size([32, 120, 64])
  [Transformer Encoder] Input shape (N, L, C): torch.Size([32, 120, 64])
  [Transformer Encoder] Output shape: torch.Size([32, 120, 64])
[STFTnet] Shape after taking last time step: torch.Size([32, 64])
  [Classifier] Input shape: torch.Size([32, 64])
  [Classifier] Output shape: torch.Size([32, 12])

[STFTnet] Initial input shape: torch.Size([32, 120, 7])
[STFTnet] Shape after permuting for conv: torch.Size([32, 7, 120])
  [MFA Block] Input shape: torch.Size([32, 7, 120])
  [MFA Block] After global average pooling shape: torch.Size([32, 7, 1])
  [MFA Block] Attention weights shape: torch.Size([32, 7, 1])
  [MFA Block] Output after attention weighting shape: torch.Size([32, 7, 120])
[STFTnet] Shape after channel projection conv: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Input shape: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Output shape: torch.Size([32, 64, 120])
[STFTnet] Shape after permuting for transformer: torch.Size([32, 120, 64])
  [Transformer Encoder] Input shape (N, L, C): torch.Size([32, 120, 64])
  [Transformer Encoder] Output shape: torch.Size([32, 120, 64])
[STFTnet] Shape after taking last time step: torch.Size([32, 64])
  [Classifier] Input shape: torch.Size([32, 64])
  [Classifier] Output shape: torch.Size([32, 12])

[STFTnet] Initial input shape: torch.Size([32, 120, 7])
[STFTnet] Shape after permuting for conv: torch.Size([32, 7, 120])
  [MFA Block] Input shape: torch.Size([32, 7, 120])
  [MFA Block] After global average pooling shape: torch.Size([32, 7, 1])
  [MFA Block] Attention weights shape: torch.Size([32, 7, 1])
  [MFA Block] Output after attention weighting shape: torch.Size([32, 7, 120])
[STFTnet] Shape after channel projection conv: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Input shape: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Output shape: torch.Size([32, 64, 120])
[STFTnet] Shape after permuting for transformer: torch.Size([32, 120, 64])
  [Transformer Encoder] Input shape (N, L, C): torch.Size([32, 120, 64])
  [Transformer Encoder] Output shape: torch.Size([32, 120, 64])
[STFTnet] Shape after taking last time step: torch.Size([32, 64])
  [Classifier] Input shape: torch.Size([32, 64])
  [Classifier] Output shape: torch.Size([32, 12])

[STFTnet] Initial input shape: torch.Size([32, 120, 7])
[STFTnet] Shape after permuting for conv: torch.Size([32, 7, 120])
  [MFA Block] Input shape: torch.Size([32, 7, 120])
  [MFA Block] After global average pooling shape: torch.Size([32, 7, 1])
  [MFA Block] Attention weights shape: torch.Size([32, 7, 1])
  [MFA Block] Output after attention weighting shape: torch.Size([32, 7, 120])
[STFTnet] Shape after channel projection conv: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Input shape: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Output shape: torch.Size([32, 64, 120])
[STFTnet] Shape after permuting for transformer: torch.Size([32, 120, 64])
  [Transformer Encoder] Input shape (N, L, C): torch.Size([32, 120, 64])
  [Transformer Encoder] Output shape: torch.Size([32, 120, 64])
[STFTnet] Shape after taking last time step: torch.Size([32, 64])
  [Classifier] Input shape: torch.Size([32, 64])
  [Classifier] Output shape: torch.Size([32, 12])

[STFTnet] Initial input shape: torch.Size([32, 120, 7])
[STFTnet] Shape after permuting for conv: torch.Size([32, 7, 120])
  [MFA Block] Input shape: torch.Size([32, 7, 120])
  [MFA Block] After global average pooling shape: torch.Size([32, 7, 1])
  [MFA Block] Attention weights shape: torch.Size([32, 7, 1])
  [MFA Block] Output after attention weighting shape: torch.Size([32, 7, 120])
[STFTnet] Shape after channel projection conv: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Input shape: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Output shape: torch.Size([32, 64, 120])
[STFTnet] Shape after permuting for transformer: torch.Size([32, 120, 64])
  [Transformer Encoder] Input shape (N, L, C): torch.Size([32, 120, 64])
  [Transformer Encoder] Output shape: torch.Size([32, 120, 64])
[STFTnet] Shape after taking last time step: torch.Size([32, 64])
  [Classifier] Input shape: torch.Size([32, 64])
  [Classifier] Output shape: torch.Size([32, 12])

--- Completed a few batches for validation purposes; ending this epoch early. ---

========================================================
Epoch 27, Loss: 1.7011, Accuracy: 0.9323
========================================================


[STFTnet] Initial input shape: torch.Size([32, 120, 7])
[STFTnet] Shape after permuting for conv: torch.Size([32, 7, 120])
  [MFA Block] Input shape: torch.Size([32, 7, 120])
  [MFA Block] After global average pooling shape: torch.Size([32, 7, 1])
  [MFA Block] Attention weights shape: torch.Size([32, 7, 1])
  [MFA Block] Output after attention weighting shape: torch.Size([32, 7, 120])
[STFTnet] Shape after channel projection conv: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Input shape: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Output shape: torch.Size([32, 64, 120])
[STFTnet] Shape after permuting for transformer: torch.Size([32, 120, 64])
  [Transformer Encoder] Input shape (N, L, C): torch.Size([32, 120, 64])
  [Transformer Encoder] Output shape: torch.Size([32, 120, 64])
[STFTnet] Shape after taking last time step: torch.Size([32, 64])
  [Classifier] Input shape: torch.Size([32, 64])
  [Classifier] Output shape: torch.Size([32, 12])

[STFTnet] Initial input shape: torch.Size([32, 120, 7])
[STFTnet] Shape after permuting for conv: torch.Size([32, 7, 120])
  [MFA Block] Input shape: torch.Size([32, 7, 120])
  [MFA Block] After global average pooling shape: torch.Size([32, 7, 1])
  [MFA Block] Attention weights shape: torch.Size([32, 7, 1])
  [MFA Block] Output after attention weighting shape: torch.Size([32, 7, 120])
[STFTnet] Shape after channel projection conv: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Input shape: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Output shape: torch.Size([32, 64, 120])
[STFTnet] Shape after permuting for transformer: torch.Size([32, 120, 64])
  [Transformer Encoder] Input shape (N, L, C): torch.Size([32, 120, 64])
  [Transformer Encoder] Output shape: torch.Size([32, 120, 64])
[STFTnet] Shape after taking last time step: torch.Size([32, 64])
  [Classifier] Input shape: torch.Size([32, 64])
  [Classifier] Output shape: torch.Size([32, 12])

[STFTnet] Initial input shape: torch.Size([32, 120, 7])
[STFTnet] Shape after permuting for conv: torch.Size([32, 7, 120])
  [MFA Block] Input shape: torch.Size([32, 7, 120])
  [MFA Block] After global average pooling shape: torch.Size([32, 7, 1])
  [MFA Block] Attention weights shape: torch.Size([32, 7, 1])
  [MFA Block] Output after attention weighting shape: torch.Size([32, 7, 120])
[STFTnet] Shape after channel projection conv: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Input shape: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Output shape: torch.Size([32, 64, 120])
[STFTnet] Shape after permuting for transformer: torch.Size([32, 120, 64])
  [Transformer Encoder] Input shape (N, L, C): torch.Size([32, 120, 64])
  [Transformer Encoder] Output shape: torch.Size([32, 120, 64])
[STFTnet] Shape after taking last time step: torch.Size([32, 64])
  [Classifier] Input shape: torch.Size([32, 64])
  [Classifier] Output shape: torch.Size([32, 12])

[STFTnet] Initial input shape: torch.Size([32, 120, 7])
[STFTnet] Shape after permuting for conv: torch.Size([32, 7, 120])
  [MFA Block] Input shape: torch.Size([32, 7, 120])
  [MFA Block] After global average pooling shape: torch.Size([32, 7, 1])
  [MFA Block] Attention weights shape: torch.Size([32, 7, 1])
  [MFA Block] Output after attention weighting shape: torch.Size([32, 7, 120])
[STFTnet] Shape after channel projection conv: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Input shape: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Output shape: torch.Size([32, 64, 120])
[STFTnet] Shape after permuting for transformer: torch.Size([32, 120, 64])
  [Transformer Encoder] Input shape (N, L, C): torch.Size([32, 120, 64])
  [Transformer Encoder] Output shape: torch.Size([32, 120, 64])
[STFTnet] Shape after taking last time step: torch.Size([32, 64])
  [Classifier] Input shape: torch.Size([32, 64])
  [Classifier] Output shape: torch.Size([32, 12])

[STFTnet] Initial input shape: torch.Size([32, 120, 7])
[STFTnet] Shape after permuting for conv: torch.Size([32, 7, 120])
  [MFA Block] Input shape: torch.Size([32, 7, 120])
  [MFA Block] After global average pooling shape: torch.Size([32, 7, 1])
  [MFA Block] Attention weights shape: torch.Size([32, 7, 1])
  [MFA Block] Output after attention weighting shape: torch.Size([32, 7, 120])
[STFTnet] Shape after channel projection conv: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Input shape: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Output shape: torch.Size([32, 64, 120])
[STFTnet] Shape after permuting for transformer: torch.Size([32, 120, 64])
  [Transformer Encoder] Input shape (N, L, C): torch.Size([32, 120, 64])
  [Transformer Encoder] Output shape: torch.Size([32, 120, 64])
[STFTnet] Shape after taking last time step: torch.Size([32, 64])
  [Classifier] Input shape: torch.Size([32, 64])
  [Classifier] Output shape: torch.Size([32, 12])

[STFTnet] Initial input shape: torch.Size([32, 120, 7])
[STFTnet] Shape after permuting for conv: torch.Size([32, 7, 120])
  [MFA Block] Input shape: torch.Size([32, 7, 120])
  [MFA Block] After global average pooling shape: torch.Size([32, 7, 1])
  [MFA Block] Attention weights shape: torch.Size([32, 7, 1])
  [MFA Block] Output after attention weighting shape: torch.Size([32, 7, 120])
[STFTnet] Shape after channel projection conv: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Input shape: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Output shape: torch.Size([32, 64, 120])
[STFTnet] Shape after permuting for transformer: torch.Size([32, 120, 64])
  [Transformer Encoder] Input shape (N, L, C): torch.Size([32, 120, 64])
  [Transformer Encoder] Output shape: torch.Size([32, 120, 64])
[STFTnet] Shape after taking last time step: torch.Size([32, 64])
  [Classifier] Input shape: torch.Size([32, 64])
  [Classifier] Output shape: torch.Size([32, 12])

--- Completed a few batches for validation purposes; ending this epoch early. ---

========================================================
Epoch 28, Loss: 1.6957, Accuracy: 0.9427
========================================================


[STFTnet] Initial input shape: torch.Size([32, 120, 7])
[STFTnet] Shape after permuting for conv: torch.Size([32, 7, 120])
  [MFA Block] Input shape: torch.Size([32, 7, 120])
  [MFA Block] After global average pooling shape: torch.Size([32, 7, 1])
  [MFA Block] Attention weights shape: torch.Size([32, 7, 1])
  [MFA Block] Output after attention weighting shape: torch.Size([32, 7, 120])
[STFTnet] Shape after channel projection conv: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Input shape: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Output shape: torch.Size([32, 64, 120])
[STFTnet] Shape after permuting for transformer: torch.Size([32, 120, 64])
  [Transformer Encoder] Input shape (N, L, C): torch.Size([32, 120, 64])
  [Transformer Encoder] Output shape: torch.Size([32, 120, 64])
[STFTnet] Shape after taking last time step: torch.Size([32, 64])
  [Classifier] Input shape: torch.Size([32, 64])
  [Classifier] Output shape: torch.Size([32, 12])

[STFTnet] Initial input shape: torch.Size([32, 120, 7])
[STFTnet] Shape after permuting for conv: torch.Size([32, 7, 120])
  [MFA Block] Input shape: torch.Size([32, 7, 120])
  [MFA Block] After global average pooling shape: torch.Size([32, 7, 1])
  [MFA Block] Attention weights shape: torch.Size([32, 7, 1])
  [MFA Block] Output after attention weighting shape: torch.Size([32, 7, 120])
[STFTnet] Shape after channel projection conv: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Input shape: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Output shape: torch.Size([32, 64, 120])
[STFTnet] Shape after permuting for transformer: torch.Size([32, 120, 64])
  [Transformer Encoder] Input shape (N, L, C): torch.Size([32, 120, 64])
  [Transformer Encoder] Output shape: torch.Size([32, 120, 64])
[STFTnet] Shape after taking last time step: torch.Size([32, 64])
  [Classifier] Input shape: torch.Size([32, 64])
  [Classifier] Output shape: torch.Size([32, 12])

[STFTnet] Initial input shape: torch.Size([32, 120, 7])
[STFTnet] Shape after permuting for conv: torch.Size([32, 7, 120])
  [MFA Block] Input shape: torch.Size([32, 7, 120])
  [MFA Block] After global average pooling shape: torch.Size([32, 7, 1])
  [MFA Block] Attention weights shape: torch.Size([32, 7, 1])
  [MFA Block] Output after attention weighting shape: torch.Size([32, 7, 120])
[STFTnet] Shape after channel projection conv: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Input shape: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Output shape: torch.Size([32, 64, 120])
[STFTnet] Shape after permuting for transformer: torch.Size([32, 120, 64])
  [Transformer Encoder] Input shape (N, L, C): torch.Size([32, 120, 64])
  [Transformer Encoder] Output shape: torch.Size([32, 120, 64])
[STFTnet] Shape after taking last time step: torch.Size([32, 64])
  [Classifier] Input shape: torch.Size([32, 64])
  [Classifier] Output shape: torch.Size([32, 12])

[STFTnet] Initial input shape: torch.Size([32, 120, 7])
[STFTnet] Shape after permuting for conv: torch.Size([32, 7, 120])
  [MFA Block] Input shape: torch.Size([32, 7, 120])
  [MFA Block] After global average pooling shape: torch.Size([32, 7, 1])
  [MFA Block] Attention weights shape: torch.Size([32, 7, 1])
  [MFA Block] Output after attention weighting shape: torch.Size([32, 7, 120])
[STFTnet] Shape after channel projection conv: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Input shape: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Output shape: torch.Size([32, 64, 120])
[STFTnet] Shape after permuting for transformer: torch.Size([32, 120, 64])
  [Transformer Encoder] Input shape (N, L, C): torch.Size([32, 120, 64])
  [Transformer Encoder] Output shape: torch.Size([32, 120, 64])
[STFTnet] Shape after taking last time step: torch.Size([32, 64])
  [Classifier] Input shape: torch.Size([32, 64])
  [Classifier] Output shape: torch.Size([32, 12])

[STFTnet] Initial input shape: torch.Size([32, 120, 7])
[STFTnet] Shape after permuting for conv: torch.Size([32, 7, 120])
  [MFA Block] Input shape: torch.Size([32, 7, 120])
  [MFA Block] After global average pooling shape: torch.Size([32, 7, 1])
  [MFA Block] Attention weights shape: torch.Size([32, 7, 1])
  [MFA Block] Output after attention weighting shape: torch.Size([32, 7, 120])
[STFTnet] Shape after channel projection conv: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Input shape: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Output shape: torch.Size([32, 64, 120])
[STFTnet] Shape after permuting for transformer: torch.Size([32, 120, 64])
  [Transformer Encoder] Input shape (N, L, C): torch.Size([32, 120, 64])
  [Transformer Encoder] Output shape: torch.Size([32, 120, 64])
[STFTnet] Shape after taking last time step: torch.Size([32, 64])
  [Classifier] Input shape: torch.Size([32, 64])
  [Classifier] Output shape: torch.Size([32, 12])

[STFTnet] Initial input shape: torch.Size([32, 120, 7])
[STFTnet] Shape after permuting for conv: torch.Size([32, 7, 120])
  [MFA Block] Input shape: torch.Size([32, 7, 120])
  [MFA Block] After global average pooling shape: torch.Size([32, 7, 1])
  [MFA Block] Attention weights shape: torch.Size([32, 7, 1])
  [MFA Block] Output after attention weighting shape: torch.Size([32, 7, 120])
[STFTnet] Shape after channel projection conv: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Input shape: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Output shape: torch.Size([32, 64, 120])
[STFTnet] Shape after permuting for transformer: torch.Size([32, 120, 64])
  [Transformer Encoder] Input shape (N, L, C): torch.Size([32, 120, 64])
  [Transformer Encoder] Output shape: torch.Size([32, 120, 64])
[STFTnet] Shape after taking last time step: torch.Size([32, 64])
  [Classifier] Input shape: torch.Size([32, 64])
  [Classifier] Output shape: torch.Size([32, 12])

--- Completed a few batches for validation purposes; ending this epoch early. ---

========================================================
Epoch 29, Loss: 1.6874, Accuracy: 0.9531
========================================================


[STFTnet] Initial input shape: torch.Size([32, 120, 7])
[STFTnet] Shape after permuting for conv: torch.Size([32, 7, 120])
  [MFA Block] Input shape: torch.Size([32, 7, 120])
  [MFA Block] After global average pooling shape: torch.Size([32, 7, 1])
  [MFA Block] Attention weights shape: torch.Size([32, 7, 1])
  [MFA Block] Output after attention weighting shape: torch.Size([32, 7, 120])
[STFTnet] Shape after channel projection conv: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Input shape: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Output shape: torch.Size([32, 64, 120])
[STFTnet] Shape after permuting for transformer: torch.Size([32, 120, 64])
  [Transformer Encoder] Input shape (N, L, C): torch.Size([32, 120, 64])
  [Transformer Encoder] Output shape: torch.Size([32, 120, 64])
[STFTnet] Shape after taking last time step: torch.Size([32, 64])
  [Classifier] Input shape: torch.Size([32, 64])
  [Classifier] Output shape: torch.Size([32, 12])

[STFTnet] Initial input shape: torch.Size([32, 120, 7])
[STFTnet] Shape after permuting for conv: torch.Size([32, 7, 120])
  [MFA Block] Input shape: torch.Size([32, 7, 120])
  [MFA Block] After global average pooling shape: torch.Size([32, 7, 1])
  [MFA Block] Attention weights shape: torch.Size([32, 7, 1])
  [MFA Block] Output after attention weighting shape: torch.Size([32, 7, 120])
[STFTnet] Shape after channel projection conv: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Input shape: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Output shape: torch.Size([32, 64, 120])
[STFTnet] Shape after permuting for transformer: torch.Size([32, 120, 64])
  [Transformer Encoder] Input shape (N, L, C): torch.Size([32, 120, 64])
  [Transformer Encoder] Output shape: torch.Size([32, 120, 64])
[STFTnet] Shape after taking last time step: torch.Size([32, 64])
  [Classifier] Input shape: torch.Size([32, 64])
  [Classifier] Output shape: torch.Size([32, 12])

[STFTnet] Initial input shape: torch.Size([32, 120, 7])
[STFTnet] Shape after permuting for conv: torch.Size([32, 7, 120])
  [MFA Block] Input shape: torch.Size([32, 7, 120])
  [MFA Block] After global average pooling shape: torch.Size([32, 7, 1])
  [MFA Block] Attention weights shape: torch.Size([32, 7, 1])
  [MFA Block] Output after attention weighting shape: torch.Size([32, 7, 120])
[STFTnet] Shape after channel projection conv: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Input shape: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Output shape: torch.Size([32, 64, 120])
[STFTnet] Shape after permuting for transformer: torch.Size([32, 120, 64])
  [Transformer Encoder] Input shape (N, L, C): torch.Size([32, 120, 64])
  [Transformer Encoder] Output shape: torch.Size([32, 120, 64])
[STFTnet] Shape after taking last time step: torch.Size([32, 64])
  [Classifier] Input shape: torch.Size([32, 64])
  [Classifier] Output shape: torch.Size([32, 12])

[STFTnet] Initial input shape: torch.Size([32, 120, 7])
[STFTnet] Shape after permuting for conv: torch.Size([32, 7, 120])
  [MFA Block] Input shape: torch.Size([32, 7, 120])
  [MFA Block] After global average pooling shape: torch.Size([32, 7, 1])
  [MFA Block] Attention weights shape: torch.Size([32, 7, 1])
  [MFA Block] Output after attention weighting shape: torch.Size([32, 7, 120])
[STFTnet] Shape after channel projection conv: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Input shape: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Output shape: torch.Size([32, 64, 120])
[STFTnet] Shape after permuting for transformer: torch.Size([32, 120, 64])
  [Transformer Encoder] Input shape (N, L, C): torch.Size([32, 120, 64])
  [Transformer Encoder] Output shape: torch.Size([32, 120, 64])
[STFTnet] Shape after taking last time step: torch.Size([32, 64])
  [Classifier] Input shape: torch.Size([32, 64])
  [Classifier] Output shape: torch.Size([32, 12])

[STFTnet] Initial input shape: torch.Size([32, 120, 7])
[STFTnet] Shape after permuting for conv: torch.Size([32, 7, 120])
  [MFA Block] Input shape: torch.Size([32, 7, 120])
  [MFA Block] After global average pooling shape: torch.Size([32, 7, 1])
  [MFA Block] Attention weights shape: torch.Size([32, 7, 1])
  [MFA Block] Output after attention weighting shape: torch.Size([32, 7, 120])
[STFTnet] Shape after channel projection conv: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Input shape: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Output shape: torch.Size([32, 64, 120])
[STFTnet] Shape after permuting for transformer: torch.Size([32, 120, 64])
  [Transformer Encoder] Input shape (N, L, C): torch.Size([32, 120, 64])
  [Transformer Encoder] Output shape: torch.Size([32, 120, 64])
[STFTnet] Shape after taking last time step: torch.Size([32, 64])
  [Classifier] Input shape: torch.Size([32, 64])
  [Classifier] Output shape: torch.Size([32, 12])

[STFTnet] Initial input shape: torch.Size([32, 120, 7])
[STFTnet] Shape after permuting for conv: torch.Size([32, 7, 120])
  [MFA Block] Input shape: torch.Size([32, 7, 120])
  [MFA Block] After global average pooling shape: torch.Size([32, 7, 1])
  [MFA Block] Attention weights shape: torch.Size([32, 7, 1])
  [MFA Block] Output after attention weighting shape: torch.Size([32, 7, 120])
[STFTnet] Shape after channel projection conv: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Input shape: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Output shape: torch.Size([32, 64, 120])
[STFTnet] Shape after permuting for transformer: torch.Size([32, 120, 64])
  [Transformer Encoder] Input shape (N, L, C): torch.Size([32, 120, 64])
  [Transformer Encoder] Output shape: torch.Size([32, 120, 64])
[STFTnet] Shape after taking last time step: torch.Size([32, 64])
  [Classifier] Input shape: torch.Size([32, 64])
  [Classifier] Output shape: torch.Size([32, 12])

--- Completed a few batches for validation purposes; ending this epoch early. ---

========================================================
Epoch 30, Loss: 1.6761, Accuracy: 0.9688
========================================================


[STFTnet] Initial input shape: torch.Size([32, 120, 7])
[STFTnet] Shape after permuting for conv: torch.Size([32, 7, 120])
  [MFA Block] Input shape: torch.Size([32, 7, 120])
  [MFA Block] After global average pooling shape: torch.Size([32, 7, 1])
  [MFA Block] Attention weights shape: torch.Size([32, 7, 1])
  [MFA Block] Output after attention weighting shape: torch.Size([32, 7, 120])
[STFTnet] Shape after channel projection conv: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Input shape: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Output shape: torch.Size([32, 64, 120])
[STFTnet] Shape after permuting for transformer: torch.Size([32, 120, 64])
  [Transformer Encoder] Input shape (N, L, C): torch.Size([32, 120, 64])
  [Transformer Encoder] Output shape: torch.Size([32, 120, 64])
[STFTnet] Shape after taking last time step: torch.Size([32, 64])
  [Classifier] Input shape: torch.Size([32, 64])
  [Classifier] Output shape: torch.Size([32, 12])

[STFTnet] Initial input shape: torch.Size([32, 120, 7])
[STFTnet] Shape after permuting for conv: torch.Size([32, 7, 120])
  [MFA Block] Input shape: torch.Size([32, 7, 120])
  [MFA Block] After global average pooling shape: torch.Size([32, 7, 1])
  [MFA Block] Attention weights shape: torch.Size([32, 7, 1])
  [MFA Block] Output after attention weighting shape: torch.Size([32, 7, 120])
[STFTnet] Shape after channel projection conv: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Input shape: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Output shape: torch.Size([32, 64, 120])
[STFTnet] Shape after permuting for transformer: torch.Size([32, 120, 64])
  [Transformer Encoder] Input shape (N, L, C): torch.Size([32, 120, 64])
  [Transformer Encoder] Output shape: torch.Size([32, 120, 64])
[STFTnet] Shape after taking last time step: torch.Size([32, 64])
  [Classifier] Input shape: torch.Size([32, 64])
  [Classifier] Output shape: torch.Size([32, 12])

[STFTnet] Initial input shape: torch.Size([32, 120, 7])
[STFTnet] Shape after permuting for conv: torch.Size([32, 7, 120])
  [MFA Block] Input shape: torch.Size([32, 7, 120])
  [MFA Block] After global average pooling shape: torch.Size([32, 7, 1])
  [MFA Block] Attention weights shape: torch.Size([32, 7, 1])
  [MFA Block] Output after attention weighting shape: torch.Size([32, 7, 120])
[STFTnet] Shape after channel projection conv: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Input shape: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Output shape: torch.Size([32, 64, 120])
[STFTnet] Shape after permuting for transformer: torch.Size([32, 120, 64])
  [Transformer Encoder] Input shape (N, L, C): torch.Size([32, 120, 64])
  [Transformer Encoder] Output shape: torch.Size([32, 120, 64])
[STFTnet] Shape after taking last time step: torch.Size([32, 64])
  [Classifier] Input shape: torch.Size([32, 64])
  [Classifier] Output shape: torch.Size([32, 12])

[STFTnet] Initial input shape: torch.Size([32, 120, 7])
[STFTnet] Shape after permuting for conv: torch.Size([32, 7, 120])
  [MFA Block] Input shape: torch.Size([32, 7, 120])
  [MFA Block] After global average pooling shape: torch.Size([32, 7, 1])
  [MFA Block] Attention weights shape: torch.Size([32, 7, 1])
  [MFA Block] Output after attention weighting shape: torch.Size([32, 7, 120])
[STFTnet] Shape after channel projection conv: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Input shape: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Output shape: torch.Size([32, 64, 120])
[STFTnet] Shape after permuting for transformer: torch.Size([32, 120, 64])
  [Transformer Encoder] Input shape (N, L, C): torch.Size([32, 120, 64])
  [Transformer Encoder] Output shape: torch.Size([32, 120, 64])
[STFTnet] Shape after taking last time step: torch.Size([32, 64])
  [Classifier] Input shape: torch.Size([32, 64])
  [Classifier] Output shape: torch.Size([32, 12])

[STFTnet] Initial input shape: torch.Size([32, 120, 7])
[STFTnet] Shape after permuting for conv: torch.Size([32, 7, 120])
  [MFA Block] Input shape: torch.Size([32, 7, 120])
  [MFA Block] After global average pooling shape: torch.Size([32, 7, 1])
  [MFA Block] Attention weights shape: torch.Size([32, 7, 1])
  [MFA Block] Output after attention weighting shape: torch.Size([32, 7, 120])
[STFTnet] Shape after channel projection conv: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Input shape: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Output shape: torch.Size([32, 64, 120])
[STFTnet] Shape after permuting for transformer: torch.Size([32, 120, 64])
  [Transformer Encoder] Input shape (N, L, C): torch.Size([32, 120, 64])
  [Transformer Encoder] Output shape: torch.Size([32, 120, 64])
[STFTnet] Shape after taking last time step: torch.Size([32, 64])
  [Classifier] Input shape: torch.Size([32, 64])
  [Classifier] Output shape: torch.Size([32, 12])

[STFTnet] Initial input shape: torch.Size([32, 120, 7])
[STFTnet] Shape after permuting for conv: torch.Size([32, 7, 120])
  [MFA Block] Input shape: torch.Size([32, 7, 120])
  [MFA Block] After global average pooling shape: torch.Size([32, 7, 1])
  [MFA Block] Attention weights shape: torch.Size([32, 7, 1])
  [MFA Block] Output after attention weighting shape: torch.Size([32, 7, 120])
[STFTnet] Shape after channel projection conv: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Input shape: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Output shape: torch.Size([32, 64, 120])
[STFTnet] Shape after permuting for transformer: torch.Size([32, 120, 64])
  [Transformer Encoder] Input shape (N, L, C): torch.Size([32, 120, 64])
  [Transformer Encoder] Output shape: torch.Size([32, 120, 64])
[STFTnet] Shape after taking last time step: torch.Size([32, 64])
  [Classifier] Input shape: torch.Size([32, 64])
  [Classifier] Output shape: torch.Size([32, 12])

--- Completed a few batches for validation purposes; ending this epoch early. ---

========================================================
Epoch 31, Loss: 1.6690, Accuracy: 0.9792
========================================================


[STFTnet] Initial input shape: torch.Size([32, 120, 7])
[STFTnet] Shape after permuting for conv: torch.Size([32, 7, 120])
  [MFA Block] Input shape: torch.Size([32, 7, 120])
  [MFA Block] After global average pooling shape: torch.Size([32, 7, 1])
  [MFA Block] Attention weights shape: torch.Size([32, 7, 1])
  [MFA Block] Output after attention weighting shape: torch.Size([32, 7, 120])
[STFTnet] Shape after channel projection conv: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Input shape: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Output shape: torch.Size([32, 64, 120])
[STFTnet] Shape after permuting for transformer: torch.Size([32, 120, 64])
  [Transformer Encoder] Input shape (N, L, C): torch.Size([32, 120, 64])
  [Transformer Encoder] Output shape: torch.Size([32, 120, 64])
[STFTnet] Shape after taking last time step: torch.Size([32, 64])
  [Classifier] Input shape: torch.Size([32, 64])
  [Classifier] Output shape: torch.Size([32, 12])

[STFTnet] Initial input shape: torch.Size([32, 120, 7])
[STFTnet] Shape after permuting for conv: torch.Size([32, 7, 120])
  [MFA Block] Input shape: torch.Size([32, 7, 120])
  [MFA Block] After global average pooling shape: torch.Size([32, 7, 1])
  [MFA Block] Attention weights shape: torch.Size([32, 7, 1])
  [MFA Block] Output after attention weighting shape: torch.Size([32, 7, 120])
[STFTnet] Shape after channel projection conv: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Input shape: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Output shape: torch.Size([32, 64, 120])
[STFTnet] Shape after permuting for transformer: torch.Size([32, 120, 64])
  [Transformer Encoder] Input shape (N, L, C): torch.Size([32, 120, 64])
  [Transformer Encoder] Output shape: torch.Size([32, 120, 64])
[STFTnet] Shape after taking last time step: torch.Size([32, 64])
  [Classifier] Input shape: torch.Size([32, 64])
  [Classifier] Output shape: torch.Size([32, 12])

[STFTnet] Initial input shape: torch.Size([32, 120, 7])
[STFTnet] Shape after permuting for conv: torch.Size([32, 7, 120])
  [MFA Block] Input shape: torch.Size([32, 7, 120])
  [MFA Block] After global average pooling shape: torch.Size([32, 7, 1])
  [MFA Block] Attention weights shape: torch.Size([32, 7, 1])
  [MFA Block] Output after attention weighting shape: torch.Size([32, 7, 120])
[STFTnet] Shape after channel projection conv: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Input shape: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Output shape: torch.Size([32, 64, 120])
[STFTnet] Shape after permuting for transformer: torch.Size([32, 120, 64])
  [Transformer Encoder] Input shape (N, L, C): torch.Size([32, 120, 64])
  [Transformer Encoder] Output shape: torch.Size([32, 120, 64])
[STFTnet] Shape after taking last time step: torch.Size([32, 64])
  [Classifier] Input shape: torch.Size([32, 64])
  [Classifier] Output shape: torch.Size([32, 12])

[STFTnet] Initial input shape: torch.Size([32, 120, 7])
[STFTnet] Shape after permuting for conv: torch.Size([32, 7, 120])
  [MFA Block] Input shape: torch.Size([32, 7, 120])
  [MFA Block] After global average pooling shape: torch.Size([32, 7, 1])
  [MFA Block] Attention weights shape: torch.Size([32, 7, 1])
  [MFA Block] Output after attention weighting shape: torch.Size([32, 7, 120])
[STFTnet] Shape after channel projection conv: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Input shape: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Output shape: torch.Size([32, 64, 120])
[STFTnet] Shape after permuting for transformer: torch.Size([32, 120, 64])
  [Transformer Encoder] Input shape (N, L, C): torch.Size([32, 120, 64])
  [Transformer Encoder] Output shape: torch.Size([32, 120, 64])
[STFTnet] Shape after taking last time step: torch.Size([32, 64])
  [Classifier] Input shape: torch.Size([32, 64])
  [Classifier] Output shape: torch.Size([32, 12])

[STFTnet] Initial input shape: torch.Size([32, 120, 7])
[STFTnet] Shape after permuting for conv: torch.Size([32, 7, 120])
  [MFA Block] Input shape: torch.Size([32, 7, 120])
  [MFA Block] After global average pooling shape: torch.Size([32, 7, 1])
  [MFA Block] Attention weights shape: torch.Size([32, 7, 1])
  [MFA Block] Output after attention weighting shape: torch.Size([32, 7, 120])
[STFTnet] Shape after channel projection conv: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Input shape: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Output shape: torch.Size([32, 64, 120])
[STFTnet] Shape after permuting for transformer: torch.Size([32, 120, 64])
  [Transformer Encoder] Input shape (N, L, C): torch.Size([32, 120, 64])
  [Transformer Encoder] Output shape: torch.Size([32, 120, 64])
[STFTnet] Shape after taking last time step: torch.Size([32, 64])
  [Classifier] Input shape: torch.Size([32, 64])
  [Classifier] Output shape: torch.Size([32, 12])

[STFTnet] Initial input shape: torch.Size([32, 120, 7])
[STFTnet] Shape after permuting for conv: torch.Size([32, 7, 120])
  [MFA Block] Input shape: torch.Size([32, 7, 120])
  [MFA Block] After global average pooling shape: torch.Size([32, 7, 1])
  [MFA Block] Attention weights shape: torch.Size([32, 7, 1])
  [MFA Block] Output after attention weighting shape: torch.Size([32, 7, 120])
[STFTnet] Shape after channel projection conv: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Input shape: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Output shape: torch.Size([32, 64, 120])
[STFTnet] Shape after permuting for transformer: torch.Size([32, 120, 64])
  [Transformer Encoder] Input shape (N, L, C): torch.Size([32, 120, 64])
  [Transformer Encoder] Output shape: torch.Size([32, 120, 64])
[STFTnet] Shape after taking last time step: torch.Size([32, 64])
  [Classifier] Input shape: torch.Size([32, 64])
  [Classifier] Output shape: torch.Size([32, 12])

--- Completed a few batches for validation purposes; ending this epoch early. ---

========================================================
Epoch 32, Loss: 1.6529, Accuracy: 0.9896
========================================================


[STFTnet] Initial input shape: torch.Size([32, 120, 7])
[STFTnet] Shape after permuting for conv: torch.Size([32, 7, 120])
  [MFA Block] Input shape: torch.Size([32, 7, 120])
  [MFA Block] After global average pooling shape: torch.Size([32, 7, 1])
  [MFA Block] Attention weights shape: torch.Size([32, 7, 1])
  [MFA Block] Output after attention weighting shape: torch.Size([32, 7, 120])
[STFTnet] Shape after channel projection conv: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Input shape: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Output shape: torch.Size([32, 64, 120])
[STFTnet] Shape after permuting for transformer: torch.Size([32, 120, 64])
  [Transformer Encoder] Input shape (N, L, C): torch.Size([32, 120, 64])
  [Transformer Encoder] Output shape: torch.Size([32, 120, 64])
[STFTnet] Shape after taking last time step: torch.Size([32, 64])
  [Classifier] Input shape: torch.Size([32, 64])
  [Classifier] Output shape: torch.Size([32, 12])

[STFTnet] Initial input shape: torch.Size([32, 120, 7])
[STFTnet] Shape after permuting for conv: torch.Size([32, 7, 120])
  [MFA Block] Input shape: torch.Size([32, 7, 120])
  [MFA Block] After global average pooling shape: torch.Size([32, 7, 1])
  [MFA Block] Attention weights shape: torch.Size([32, 7, 1])
  [MFA Block] Output after attention weighting shape: torch.Size([32, 7, 120])
[STFTnet] Shape after channel projection conv: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Input shape: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Output shape: torch.Size([32, 64, 120])
[STFTnet] Shape after permuting for transformer: torch.Size([32, 120, 64])
  [Transformer Encoder] Input shape (N, L, C): torch.Size([32, 120, 64])
  [Transformer Encoder] Output shape: torch.Size([32, 120, 64])
[STFTnet] Shape after taking last time step: torch.Size([32, 64])
  [Classifier] Input shape: torch.Size([32, 64])
  [Classifier] Output shape: torch.Size([32, 12])

[STFTnet] Initial input shape: torch.Size([32, 120, 7])
[STFTnet] Shape after permuting for conv: torch.Size([32, 7, 120])
  [MFA Block] Input shape: torch.Size([32, 7, 120])
  [MFA Block] After global average pooling shape: torch.Size([32, 7, 1])
  [MFA Block] Attention weights shape: torch.Size([32, 7, 1])
  [MFA Block] Output after attention weighting shape: torch.Size([32, 7, 120])
[STFTnet] Shape after channel projection conv: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Input shape: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Output shape: torch.Size([32, 64, 120])
[STFTnet] Shape after permuting for transformer: torch.Size([32, 120, 64])
  [Transformer Encoder] Input shape (N, L, C): torch.Size([32, 120, 64])
  [Transformer Encoder] Output shape: torch.Size([32, 120, 64])
[STFTnet] Shape after taking last time step: torch.Size([32, 64])
  [Classifier] Input shape: torch.Size([32, 64])
  [Classifier] Output shape: torch.Size([32, 12])

[STFTnet] Initial input shape: torch.Size([32, 120, 7])
[STFTnet] Shape after permuting for conv: torch.Size([32, 7, 120])
  [MFA Block] Input shape: torch.Size([32, 7, 120])
  [MFA Block] After global average pooling shape: torch.Size([32, 7, 1])
  [MFA Block] Attention weights shape: torch.Size([32, 7, 1])
  [MFA Block] Output after attention weighting shape: torch.Size([32, 7, 120])
[STFTnet] Shape after channel projection conv: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Input shape: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Output shape: torch.Size([32, 64, 120])
[STFTnet] Shape after permuting for transformer: torch.Size([32, 120, 64])
  [Transformer Encoder] Input shape (N, L, C): torch.Size([32, 120, 64])
  [Transformer Encoder] Output shape: torch.Size([32, 120, 64])
[STFTnet] Shape after taking last time step: torch.Size([32, 64])
  [Classifier] Input shape: torch.Size([32, 64])
  [Classifier] Output shape: torch.Size([32, 12])

[STFTnet] Initial input shape: torch.Size([32, 120, 7])
[STFTnet] Shape after permuting for conv: torch.Size([32, 7, 120])
  [MFA Block] Input shape: torch.Size([32, 7, 120])
  [MFA Block] After global average pooling shape: torch.Size([32, 7, 1])
  [MFA Block] Attention weights shape: torch.Size([32, 7, 1])
  [MFA Block] Output after attention weighting shape: torch.Size([32, 7, 120])
[STFTnet] Shape after channel projection conv: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Input shape: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Output shape: torch.Size([32, 64, 120])
[STFTnet] Shape after permuting for transformer: torch.Size([32, 120, 64])
  [Transformer Encoder] Input shape (N, L, C): torch.Size([32, 120, 64])
  [Transformer Encoder] Output shape: torch.Size([32, 120, 64])
[STFTnet] Shape after taking last time step: torch.Size([32, 64])
  [Classifier] Input shape: torch.Size([32, 64])
  [Classifier] Output shape: torch.Size([32, 12])

[STFTnet] Initial input shape: torch.Size([32, 120, 7])
[STFTnet] Shape after permuting for conv: torch.Size([32, 7, 120])
  [MFA Block] Input shape: torch.Size([32, 7, 120])
  [MFA Block] After global average pooling shape: torch.Size([32, 7, 1])
  [MFA Block] Attention weights shape: torch.Size([32, 7, 1])
  [MFA Block] Output after attention weighting shape: torch.Size([32, 7, 120])
[STFTnet] Shape after channel projection conv: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Input shape: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Output shape: torch.Size([32, 64, 120])
[STFTnet] Shape after permuting for transformer: torch.Size([32, 120, 64])
  [Transformer Encoder] Input shape (N, L, C): torch.Size([32, 120, 64])
  [Transformer Encoder] Output shape: torch.Size([32, 120, 64])
[STFTnet] Shape after taking last time step: torch.Size([32, 64])
  [Classifier] Input shape: torch.Size([32, 64])
  [Classifier] Output shape: torch.Size([32, 12])

--- Completed a few batches for validation purposes; ending this epoch early. ---

========================================================
Epoch 33, Loss: 1.6481, Accuracy: 0.9948
========================================================


[STFTnet] Initial input shape: torch.Size([32, 120, 7])
[STFTnet] Shape after permuting for conv: torch.Size([32, 7, 120])
  [MFA Block] Input shape: torch.Size([32, 7, 120])
  [MFA Block] After global average pooling shape: torch.Size([32, 7, 1])
  [MFA Block] Attention weights shape: torch.Size([32, 7, 1])
  [MFA Block] Output after attention weighting shape: torch.Size([32, 7, 120])
[STFTnet] Shape after channel projection conv: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Input shape: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Output shape: torch.Size([32, 64, 120])
[STFTnet] Shape after permuting for transformer: torch.Size([32, 120, 64])
  [Transformer Encoder] Input shape (N, L, C): torch.Size([32, 120, 64])
  [Transformer Encoder] Output shape: torch.Size([32, 120, 64])
[STFTnet] Shape after taking last time step: torch.Size([32, 64])
  [Classifier] Input shape: torch.Size([32, 64])
  [Classifier] Output shape: torch.Size([32, 12])

[STFTnet] Initial input shape: torch.Size([32, 120, 7])
[STFTnet] Shape after permuting for conv: torch.Size([32, 7, 120])
  [MFA Block] Input shape: torch.Size([32, 7, 120])
  [MFA Block] After global average pooling shape: torch.Size([32, 7, 1])
  [MFA Block] Attention weights shape: torch.Size([32, 7, 1])
  [MFA Block] Output after attention weighting shape: torch.Size([32, 7, 120])
[STFTnet] Shape after channel projection conv: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Input shape: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Output shape: torch.Size([32, 64, 120])
[STFTnet] Shape after permuting for transformer: torch.Size([32, 120, 64])
  [Transformer Encoder] Input shape (N, L, C): torch.Size([32, 120, 64])
  [Transformer Encoder] Output shape: torch.Size([32, 120, 64])
[STFTnet] Shape after taking last time step: torch.Size([32, 64])
  [Classifier] Input shape: torch.Size([32, 64])
  [Classifier] Output shape: torch.Size([32, 12])

[STFTnet] Initial input shape: torch.Size([32, 120, 7])
[STFTnet] Shape after permuting for conv: torch.Size([32, 7, 120])
  [MFA Block] Input shape: torch.Size([32, 7, 120])
  [MFA Block] After global average pooling shape: torch.Size([32, 7, 1])
  [MFA Block] Attention weights shape: torch.Size([32, 7, 1])
  [MFA Block] Output after attention weighting shape: torch.Size([32, 7, 120])
[STFTnet] Shape after channel projection conv: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Input shape: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Output shape: torch.Size([32, 64, 120])
[STFTnet] Shape after permuting for transformer: torch.Size([32, 120, 64])
  [Transformer Encoder] Input shape (N, L, C): torch.Size([32, 120, 64])
  [Transformer Encoder] Output shape: torch.Size([32, 120, 64])
[STFTnet] Shape after taking last time step: torch.Size([32, 64])
  [Classifier] Input shape: torch.Size([32, 64])
  [Classifier] Output shape: torch.Size([32, 12])

[STFTnet] Initial input shape: torch.Size([32, 120, 7])
[STFTnet] Shape after permuting for conv: torch.Size([32, 7, 120])
  [MFA Block] Input shape: torch.Size([32, 7, 120])
  [MFA Block] After global average pooling shape: torch.Size([32, 7, 1])
  [MFA Block] Attention weights shape: torch.Size([32, 7, 1])
  [MFA Block] Output after attention weighting shape: torch.Size([32, 7, 120])
[STFTnet] Shape after channel projection conv: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Input shape: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Output shape: torch.Size([32, 64, 120])
[STFTnet] Shape after permuting for transformer: torch.Size([32, 120, 64])
  [Transformer Encoder] Input shape (N, L, C): torch.Size([32, 120, 64])
  [Transformer Encoder] Output shape: torch.Size([32, 120, 64])
[STFTnet] Shape after taking last time step: torch.Size([32, 64])
  [Classifier] Input shape: torch.Size([32, 64])
  [Classifier] Output shape: torch.Size([32, 12])

[STFTnet] Initial input shape: torch.Size([32, 120, 7])
[STFTnet] Shape after permuting for conv: torch.Size([32, 7, 120])
  [MFA Block] Input shape: torch.Size([32, 7, 120])
  [MFA Block] After global average pooling shape: torch.Size([32, 7, 1])
  [MFA Block] Attention weights shape: torch.Size([32, 7, 1])
  [MFA Block] Output after attention weighting shape: torch.Size([32, 7, 120])
[STFTnet] Shape after channel projection conv: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Input shape: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Output shape: torch.Size([32, 64, 120])
[STFTnet] Shape after permuting for transformer: torch.Size([32, 120, 64])
  [Transformer Encoder] Input shape (N, L, C): torch.Size([32, 120, 64])
  [Transformer Encoder] Output shape: torch.Size([32, 120, 64])
[STFTnet] Shape after taking last time step: torch.Size([32, 64])
  [Classifier] Input shape: torch.Size([32, 64])
  [Classifier] Output shape: torch.Size([32, 12])

[STFTnet] Initial input shape: torch.Size([32, 120, 7])
[STFTnet] Shape after permuting for conv: torch.Size([32, 7, 120])
  [MFA Block] Input shape: torch.Size([32, 7, 120])
  [MFA Block] After global average pooling shape: torch.Size([32, 7, 1])
  [MFA Block] Attention weights shape: torch.Size([32, 7, 1])
  [MFA Block] Output after attention weighting shape: torch.Size([32, 7, 120])
[STFTnet] Shape after channel projection conv: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Input shape: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Output shape: torch.Size([32, 64, 120])
[STFTnet] Shape after permuting for transformer: torch.Size([32, 120, 64])
  [Transformer Encoder] Input shape (N, L, C): torch.Size([32, 120, 64])
  [Transformer Encoder] Output shape: torch.Size([32, 120, 64])
[STFTnet] Shape after taking last time step: torch.Size([32, 64])
  [Classifier] Input shape: torch.Size([32, 64])
  [Classifier] Output shape: torch.Size([32, 12])

--- Completed a few batches for validation purposes; ending this epoch early. ---

========================================================
Epoch 34, Loss: 1.6381, Accuracy: 1.0000
========================================================


[STFTnet] Initial input shape: torch.Size([32, 120, 7])
[STFTnet] Shape after permuting for conv: torch.Size([32, 7, 120])
  [MFA Block] Input shape: torch.Size([32, 7, 120])
  [MFA Block] After global average pooling shape: torch.Size([32, 7, 1])
  [MFA Block] Attention weights shape: torch.Size([32, 7, 1])
  [MFA Block] Output after attention weighting shape: torch.Size([32, 7, 120])
[STFTnet] Shape after channel projection conv: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Input shape: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Output shape: torch.Size([32, 64, 120])
[STFTnet] Shape after permuting for transformer: torch.Size([32, 120, 64])
  [Transformer Encoder] Input shape (N, L, C): torch.Size([32, 120, 64])
  [Transformer Encoder] Output shape: torch.Size([32, 120, 64])
[STFTnet] Shape after taking last time step: torch.Size([32, 64])
  [Classifier] Input shape: torch.Size([32, 64])
  [Classifier] Output shape: torch.Size([32, 12])

[STFTnet] Initial input shape: torch.Size([32, 120, 7])
[STFTnet] Shape after permuting for conv: torch.Size([32, 7, 120])
  [MFA Block] Input shape: torch.Size([32, 7, 120])
  [MFA Block] After global average pooling shape: torch.Size([32, 7, 1])
  [MFA Block] Attention weights shape: torch.Size([32, 7, 1])
  [MFA Block] Output after attention weighting shape: torch.Size([32, 7, 120])
[STFTnet] Shape after channel projection conv: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Input shape: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Output shape: torch.Size([32, 64, 120])
[STFTnet] Shape after permuting for transformer: torch.Size([32, 120, 64])
  [Transformer Encoder] Input shape (N, L, C): torch.Size([32, 120, 64])
  [Transformer Encoder] Output shape: torch.Size([32, 120, 64])
[STFTnet] Shape after taking last time step: torch.Size([32, 64])
  [Classifier] Input shape: torch.Size([32, 64])
  [Classifier] Output shape: torch.Size([32, 12])

[STFTnet] Initial input shape: torch.Size([32, 120, 7])
[STFTnet] Shape after permuting for conv: torch.Size([32, 7, 120])
  [MFA Block] Input shape: torch.Size([32, 7, 120])
  [MFA Block] After global average pooling shape: torch.Size([32, 7, 1])
  [MFA Block] Attention weights shape: torch.Size([32, 7, 1])
  [MFA Block] Output after attention weighting shape: torch.Size([32, 7, 120])
[STFTnet] Shape after channel projection conv: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Input shape: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Output shape: torch.Size([32, 64, 120])
[STFTnet] Shape after permuting for transformer: torch.Size([32, 120, 64])
  [Transformer Encoder] Input shape (N, L, C): torch.Size([32, 120, 64])
  [Transformer Encoder] Output shape: torch.Size([32, 120, 64])
[STFTnet] Shape after taking last time step: torch.Size([32, 64])
  [Classifier] Input shape: torch.Size([32, 64])
  [Classifier] Output shape: torch.Size([32, 12])

[STFTnet] Initial input shape: torch.Size([32, 120, 7])
[STFTnet] Shape after permuting for conv: torch.Size([32, 7, 120])
  [MFA Block] Input shape: torch.Size([32, 7, 120])
  [MFA Block] After global average pooling shape: torch.Size([32, 7, 1])
  [MFA Block] Attention weights shape: torch.Size([32, 7, 1])
  [MFA Block] Output after attention weighting shape: torch.Size([32, 7, 120])
[STFTnet] Shape after channel projection conv: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Input shape: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Output shape: torch.Size([32, 64, 120])
[STFTnet] Shape after permuting for transformer: torch.Size([32, 120, 64])
  [Transformer Encoder] Input shape (N, L, C): torch.Size([32, 120, 64])
  [Transformer Encoder] Output shape: torch.Size([32, 120, 64])
[STFTnet] Shape after taking last time step: torch.Size([32, 64])
  [Classifier] Input shape: torch.Size([32, 64])
  [Classifier] Output shape: torch.Size([32, 12])

[STFTnet] Initial input shape: torch.Size([32, 120, 7])
[STFTnet] Shape after permuting for conv: torch.Size([32, 7, 120])
  [MFA Block] Input shape: torch.Size([32, 7, 120])
  [MFA Block] After global average pooling shape: torch.Size([32, 7, 1])
  [MFA Block] Attention weights shape: torch.Size([32, 7, 1])
  [MFA Block] Output after attention weighting shape: torch.Size([32, 7, 120])
[STFTnet] Shape after channel projection conv: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Input shape: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Output shape: torch.Size([32, 64, 120])
[STFTnet] Shape after permuting for transformer: torch.Size([32, 120, 64])
  [Transformer Encoder] Input shape (N, L, C): torch.Size([32, 120, 64])
  [Transformer Encoder] Output shape: torch.Size([32, 120, 64])
[STFTnet] Shape after taking last time step: torch.Size([32, 64])
  [Classifier] Input shape: torch.Size([32, 64])
  [Classifier] Output shape: torch.Size([32, 12])

[STFTnet] Initial input shape: torch.Size([32, 120, 7])
[STFTnet] Shape after permuting for conv: torch.Size([32, 7, 120])
  [MFA Block] Input shape: torch.Size([32, 7, 120])
  [MFA Block] After global average pooling shape: torch.Size([32, 7, 1])
  [MFA Block] Attention weights shape: torch.Size([32, 7, 1])
  [MFA Block] Output after attention weighting shape: torch.Size([32, 7, 120])
[STFTnet] Shape after channel projection conv: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Input shape: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Output shape: torch.Size([32, 64, 120])
[STFTnet] Shape after permuting for transformer: torch.Size([32, 120, 64])
  [Transformer Encoder] Input shape (N, L, C): torch.Size([32, 120, 64])
  [Transformer Encoder] Output shape: torch.Size([32, 120, 64])
[STFTnet] Shape after taking last time step: torch.Size([32, 64])
  [Classifier] Input shape: torch.Size([32, 64])
  [Classifier] Output shape: torch.Size([32, 12])

--- Completed a few batches for validation purposes; ending this epoch early. ---

========================================================
Epoch 35, Loss: 1.6324, Accuracy: 1.0000
========================================================


[STFTnet] Initial input shape: torch.Size([32, 120, 7])
[STFTnet] Shape after permuting for conv: torch.Size([32, 7, 120])
  [MFA Block] Input shape: torch.Size([32, 7, 120])
  [MFA Block] After global average pooling shape: torch.Size([32, 7, 1])
  [MFA Block] Attention weights shape: torch.Size([32, 7, 1])
  [MFA Block] Output after attention weighting shape: torch.Size([32, 7, 120])
[STFTnet] Shape after channel projection conv: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Input shape: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Output shape: torch.Size([32, 64, 120])
[STFTnet] Shape after permuting for transformer: torch.Size([32, 120, 64])
  [Transformer Encoder] Input shape (N, L, C): torch.Size([32, 120, 64])
  [Transformer Encoder] Output shape: torch.Size([32, 120, 64])
[STFTnet] Shape after taking last time step: torch.Size([32, 64])
  [Classifier] Input shape: torch.Size([32, 64])
  [Classifier] Output shape: torch.Size([32, 12])

[STFTnet] Initial input shape: torch.Size([32, 120, 7])
[STFTnet] Shape after permuting for conv: torch.Size([32, 7, 120])
  [MFA Block] Input shape: torch.Size([32, 7, 120])
  [MFA Block] After global average pooling shape: torch.Size([32, 7, 1])
  [MFA Block] Attention weights shape: torch.Size([32, 7, 1])
  [MFA Block] Output after attention weighting shape: torch.Size([32, 7, 120])
[STFTnet] Shape after channel projection conv: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Input shape: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Output shape: torch.Size([32, 64, 120])
[STFTnet] Shape after permuting for transformer: torch.Size([32, 120, 64])
  [Transformer Encoder] Input shape (N, L, C): torch.Size([32, 120, 64])
  [Transformer Encoder] Output shape: torch.Size([32, 120, 64])
[STFTnet] Shape after taking last time step: torch.Size([32, 64])
  [Classifier] Input shape: torch.Size([32, 64])
  [Classifier] Output shape: torch.Size([32, 12])

[STFTnet] Initial input shape: torch.Size([32, 120, 7])
[STFTnet] Shape after permuting for conv: torch.Size([32, 7, 120])
  [MFA Block] Input shape: torch.Size([32, 7, 120])
  [MFA Block] After global average pooling shape: torch.Size([32, 7, 1])
  [MFA Block] Attention weights shape: torch.Size([32, 7, 1])
  [MFA Block] Output after attention weighting shape: torch.Size([32, 7, 120])
[STFTnet] Shape after channel projection conv: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Input shape: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Output shape: torch.Size([32, 64, 120])
[STFTnet] Shape after permuting for transformer: torch.Size([32, 120, 64])
  [Transformer Encoder] Input shape (N, L, C): torch.Size([32, 120, 64])
  [Transformer Encoder] Output shape: torch.Size([32, 120, 64])
[STFTnet] Shape after taking last time step: torch.Size([32, 64])
  [Classifier] Input shape: torch.Size([32, 64])
  [Classifier] Output shape: torch.Size([32, 12])

[STFTnet] Initial input shape: torch.Size([32, 120, 7])
[STFTnet] Shape after permuting for conv: torch.Size([32, 7, 120])
  [MFA Block] Input shape: torch.Size([32, 7, 120])
  [MFA Block] After global average pooling shape: torch.Size([32, 7, 1])
  [MFA Block] Attention weights shape: torch.Size([32, 7, 1])
  [MFA Block] Output after attention weighting shape: torch.Size([32, 7, 120])
[STFTnet] Shape after channel projection conv: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Input shape: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Output shape: torch.Size([32, 64, 120])
[STFTnet] Shape after permuting for transformer: torch.Size([32, 120, 64])
  [Transformer Encoder] Input shape (N, L, C): torch.Size([32, 120, 64])
  [Transformer Encoder] Output shape: torch.Size([32, 120, 64])
[STFTnet] Shape after taking last time step: torch.Size([32, 64])
  [Classifier] Input shape: torch.Size([32, 64])
  [Classifier] Output shape: torch.Size([32, 12])

[STFTnet] Initial input shape: torch.Size([32, 120, 7])
[STFTnet] Shape after permuting for conv: torch.Size([32, 7, 120])
  [MFA Block] Input shape: torch.Size([32, 7, 120])
  [MFA Block] After global average pooling shape: torch.Size([32, 7, 1])
  [MFA Block] Attention weights shape: torch.Size([32, 7, 1])
  [MFA Block] Output after attention weighting shape: torch.Size([32, 7, 120])
[STFTnet] Shape after channel projection conv: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Input shape: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Output shape: torch.Size([32, 64, 120])
[STFTnet] Shape after permuting for transformer: torch.Size([32, 120, 64])
  [Transformer Encoder] Input shape (N, L, C): torch.Size([32, 120, 64])
  [Transformer Encoder] Output shape: torch.Size([32, 120, 64])
[STFTnet] Shape after taking last time step: torch.Size([32, 64])
  [Classifier] Input shape: torch.Size([32, 64])
  [Classifier] Output shape: torch.Size([32, 12])

[STFTnet] Initial input shape: torch.Size([32, 120, 7])
[STFTnet] Shape after permuting for conv: torch.Size([32, 7, 120])
  [MFA Block] Input shape: torch.Size([32, 7, 120])
  [MFA Block] After global average pooling shape: torch.Size([32, 7, 1])
  [MFA Block] Attention weights shape: torch.Size([32, 7, 1])
  [MFA Block] Output after attention weighting shape: torch.Size([32, 7, 120])
[STFTnet] Shape after channel projection conv: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Input shape: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Output shape: torch.Size([32, 64, 120])
[STFTnet] Shape after permuting for transformer: torch.Size([32, 120, 64])
  [Transformer Encoder] Input shape (N, L, C): torch.Size([32, 120, 64])
  [Transformer Encoder] Output shape: torch.Size([32, 120, 64])
[STFTnet] Shape after taking last time step: torch.Size([32, 64])
  [Classifier] Input shape: torch.Size([32, 64])
  [Classifier] Output shape: torch.Size([32, 12])

--- Completed a few batches for validation purposes; ending this epoch early. ---

========================================================
Epoch 36, Loss: 1.6321, Accuracy: 1.0000
========================================================


[STFTnet] Initial input shape: torch.Size([32, 120, 7])
[STFTnet] Shape after permuting for conv: torch.Size([32, 7, 120])
  [MFA Block] Input shape: torch.Size([32, 7, 120])
  [MFA Block] After global average pooling shape: torch.Size([32, 7, 1])
  [MFA Block] Attention weights shape: torch.Size([32, 7, 1])
  [MFA Block] Output after attention weighting shape: torch.Size([32, 7, 120])
[STFTnet] Shape after channel projection conv: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Input shape: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Output shape: torch.Size([32, 64, 120])
[STFTnet] Shape after permuting for transformer: torch.Size([32, 120, 64])
  [Transformer Encoder] Input shape (N, L, C): torch.Size([32, 120, 64])
  [Transformer Encoder] Output shape: torch.Size([32, 120, 64])
[STFTnet] Shape after taking last time step: torch.Size([32, 64])
  [Classifier] Input shape: torch.Size([32, 64])
  [Classifier] Output shape: torch.Size([32, 12])

[STFTnet] Initial input shape: torch.Size([32, 120, 7])
[STFTnet] Shape after permuting for conv: torch.Size([32, 7, 120])
  [MFA Block] Input shape: torch.Size([32, 7, 120])
  [MFA Block] After global average pooling shape: torch.Size([32, 7, 1])
  [MFA Block] Attention weights shape: torch.Size([32, 7, 1])
  [MFA Block] Output after attention weighting shape: torch.Size([32, 7, 120])
[STFTnet] Shape after channel projection conv: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Input shape: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Output shape: torch.Size([32, 64, 120])
[STFTnet] Shape after permuting for transformer: torch.Size([32, 120, 64])
  [Transformer Encoder] Input shape (N, L, C): torch.Size([32, 120, 64])
  [Transformer Encoder] Output shape: torch.Size([32, 120, 64])
[STFTnet] Shape after taking last time step: torch.Size([32, 64])
  [Classifier] Input shape: torch.Size([32, 64])
  [Classifier] Output shape: torch.Size([32, 12])

[STFTnet] Initial input shape: torch.Size([32, 120, 7])
[STFTnet] Shape after permuting for conv: torch.Size([32, 7, 120])
  [MFA Block] Input shape: torch.Size([32, 7, 120])
  [MFA Block] After global average pooling shape: torch.Size([32, 7, 1])
  [MFA Block] Attention weights shape: torch.Size([32, 7, 1])
  [MFA Block] Output after attention weighting shape: torch.Size([32, 7, 120])
[STFTnet] Shape after channel projection conv: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Input shape: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Output shape: torch.Size([32, 64, 120])
[STFTnet] Shape after permuting for transformer: torch.Size([32, 120, 64])
  [Transformer Encoder] Input shape (N, L, C): torch.Size([32, 120, 64])
  [Transformer Encoder] Output shape: torch.Size([32, 120, 64])
[STFTnet] Shape after taking last time step: torch.Size([32, 64])
  [Classifier] Input shape: torch.Size([32, 64])
  [Classifier] Output shape: torch.Size([32, 12])

[STFTnet] Initial input shape: torch.Size([32, 120, 7])
[STFTnet] Shape after permuting for conv: torch.Size([32, 7, 120])
  [MFA Block] Input shape: torch.Size([32, 7, 120])
  [MFA Block] After global average pooling shape: torch.Size([32, 7, 1])
  [MFA Block] Attention weights shape: torch.Size([32, 7, 1])
  [MFA Block] Output after attention weighting shape: torch.Size([32, 7, 120])
[STFTnet] Shape after channel projection conv: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Input shape: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Output shape: torch.Size([32, 64, 120])
[STFTnet] Shape after permuting for transformer: torch.Size([32, 120, 64])
  [Transformer Encoder] Input shape (N, L, C): torch.Size([32, 120, 64])
  [Transformer Encoder] Output shape: torch.Size([32, 120, 64])
[STFTnet] Shape after taking last time step: torch.Size([32, 64])
  [Classifier] Input shape: torch.Size([32, 64])
  [Classifier] Output shape: torch.Size([32, 12])

[STFTnet] Initial input shape: torch.Size([32, 120, 7])
[STFTnet] Shape after permuting for conv: torch.Size([32, 7, 120])
  [MFA Block] Input shape: torch.Size([32, 7, 120])
  [MFA Block] After global average pooling shape: torch.Size([32, 7, 1])
  [MFA Block] Attention weights shape: torch.Size([32, 7, 1])
  [MFA Block] Output after attention weighting shape: torch.Size([32, 7, 120])
[STFTnet] Shape after channel projection conv: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Input shape: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Output shape: torch.Size([32, 64, 120])
[STFTnet] Shape after permuting for transformer: torch.Size([32, 120, 64])
  [Transformer Encoder] Input shape (N, L, C): torch.Size([32, 120, 64])
  [Transformer Encoder] Output shape: torch.Size([32, 120, 64])
[STFTnet] Shape after taking last time step: torch.Size([32, 64])
  [Classifier] Input shape: torch.Size([32, 64])
  [Classifier] Output shape: torch.Size([32, 12])

[STFTnet] Initial input shape: torch.Size([32, 120, 7])
[STFTnet] Shape after permuting for conv: torch.Size([32, 7, 120])
  [MFA Block] Input shape: torch.Size([32, 7, 120])
  [MFA Block] After global average pooling shape: torch.Size([32, 7, 1])
  [MFA Block] Attention weights shape: torch.Size([32, 7, 1])
  [MFA Block] Output after attention weighting shape: torch.Size([32, 7, 120])
[STFTnet] Shape after channel projection conv: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Input shape: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Output shape: torch.Size([32, 64, 120])
[STFTnet] Shape after permuting for transformer: torch.Size([32, 120, 64])
  [Transformer Encoder] Input shape (N, L, C): torch.Size([32, 120, 64])
  [Transformer Encoder] Output shape: torch.Size([32, 120, 64])
[STFTnet] Shape after taking last time step: torch.Size([32, 64])
  [Classifier] Input shape: torch.Size([32, 64])
  [Classifier] Output shape: torch.Size([32, 12])

--- Completed a few batches for validation purposes; ending this epoch early. ---

========================================================
Epoch 37, Loss: 1.6315, Accuracy: 1.0000
========================================================


[STFTnet] Initial input shape: torch.Size([32, 120, 7])
[STFTnet] Shape after permuting for conv: torch.Size([32, 7, 120])
  [MFA Block] Input shape: torch.Size([32, 7, 120])
  [MFA Block] After global average pooling shape: torch.Size([32, 7, 1])
  [MFA Block] Attention weights shape: torch.Size([32, 7, 1])
  [MFA Block] Output after attention weighting shape: torch.Size([32, 7, 120])
[STFTnet] Shape after channel projection conv: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Input shape: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Output shape: torch.Size([32, 64, 120])
[STFTnet] Shape after permuting for transformer: torch.Size([32, 120, 64])
  [Transformer Encoder] Input shape (N, L, C): torch.Size([32, 120, 64])
  [Transformer Encoder] Output shape: torch.Size([32, 120, 64])
[STFTnet] Shape after taking last time step: torch.Size([32, 64])
  [Classifier] Input shape: torch.Size([32, 64])
  [Classifier] Output shape: torch.Size([32, 12])

[STFTnet] Initial input shape: torch.Size([32, 120, 7])
[STFTnet] Shape after permuting for conv: torch.Size([32, 7, 120])
  [MFA Block] Input shape: torch.Size([32, 7, 120])
  [MFA Block] After global average pooling shape: torch.Size([32, 7, 1])
  [MFA Block] Attention weights shape: torch.Size([32, 7, 1])
  [MFA Block] Output after attention weighting shape: torch.Size([32, 7, 120])
[STFTnet] Shape after channel projection conv: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Input shape: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Output shape: torch.Size([32, 64, 120])
[STFTnet] Shape after permuting for transformer: torch.Size([32, 120, 64])
  [Transformer Encoder] Input shape (N, L, C): torch.Size([32, 120, 64])
  [Transformer Encoder] Output shape: torch.Size([32, 120, 64])
[STFTnet] Shape after taking last time step: torch.Size([32, 64])
  [Classifier] Input shape: torch.Size([32, 64])
  [Classifier] Output shape: torch.Size([32, 12])

[STFTnet] Initial input shape: torch.Size([32, 120, 7])
[STFTnet] Shape after permuting for conv: torch.Size([32, 7, 120])
  [MFA Block] Input shape: torch.Size([32, 7, 120])
  [MFA Block] After global average pooling shape: torch.Size([32, 7, 1])
  [MFA Block] Attention weights shape: torch.Size([32, 7, 1])
  [MFA Block] Output after attention weighting shape: torch.Size([32, 7, 120])
[STFTnet] Shape after channel projection conv: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Input shape: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Output shape: torch.Size([32, 64, 120])
[STFTnet] Shape after permuting for transformer: torch.Size([32, 120, 64])
  [Transformer Encoder] Input shape (N, L, C): torch.Size([32, 120, 64])
  [Transformer Encoder] Output shape: torch.Size([32, 120, 64])
[STFTnet] Shape after taking last time step: torch.Size([32, 64])
  [Classifier] Input shape: torch.Size([32, 64])
  [Classifier] Output shape: torch.Size([32, 12])

[STFTnet] Initial input shape: torch.Size([32, 120, 7])
[STFTnet] Shape after permuting for conv: torch.Size([32, 7, 120])
  [MFA Block] Input shape: torch.Size([32, 7, 120])
  [MFA Block] After global average pooling shape: torch.Size([32, 7, 1])
  [MFA Block] Attention weights shape: torch.Size([32, 7, 1])
  [MFA Block] Output after attention weighting shape: torch.Size([32, 7, 120])
[STFTnet] Shape after channel projection conv: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Input shape: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Output shape: torch.Size([32, 64, 120])
[STFTnet] Shape after permuting for transformer: torch.Size([32, 120, 64])
  [Transformer Encoder] Input shape (N, L, C): torch.Size([32, 120, 64])
  [Transformer Encoder] Output shape: torch.Size([32, 120, 64])
[STFTnet] Shape after taking last time step: torch.Size([32, 64])
  [Classifier] Input shape: torch.Size([32, 64])
  [Classifier] Output shape: torch.Size([32, 12])

[STFTnet] Initial input shape: torch.Size([32, 120, 7])
[STFTnet] Shape after permuting for conv: torch.Size([32, 7, 120])
  [MFA Block] Input shape: torch.Size([32, 7, 120])
  [MFA Block] After global average pooling shape: torch.Size([32, 7, 1])
  [MFA Block] Attention weights shape: torch.Size([32, 7, 1])
  [MFA Block] Output after attention weighting shape: torch.Size([32, 7, 120])
[STFTnet] Shape after channel projection conv: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Input shape: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Output shape: torch.Size([32, 64, 120])
[STFTnet] Shape after permuting for transformer: torch.Size([32, 120, 64])
  [Transformer Encoder] Input shape (N, L, C): torch.Size([32, 120, 64])
  [Transformer Encoder] Output shape: torch.Size([32, 120, 64])
[STFTnet] Shape after taking last time step: torch.Size([32, 64])
  [Classifier] Input shape: torch.Size([32, 64])
  [Classifier] Output shape: torch.Size([32, 12])

[STFTnet] Initial input shape: torch.Size([32, 120, 7])
[STFTnet] Shape after permuting for conv: torch.Size([32, 7, 120])
  [MFA Block] Input shape: torch.Size([32, 7, 120])
  [MFA Block] After global average pooling shape: torch.Size([32, 7, 1])
  [MFA Block] Attention weights shape: torch.Size([32, 7, 1])
  [MFA Block] Output after attention weighting shape: torch.Size([32, 7, 120])
[STFTnet] Shape after channel projection conv: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Input shape: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Output shape: torch.Size([32, 64, 120])
[STFTnet] Shape after permuting for transformer: torch.Size([32, 120, 64])
  [Transformer Encoder] Input shape (N, L, C): torch.Size([32, 120, 64])
  [Transformer Encoder] Output shape: torch.Size([32, 120, 64])
[STFTnet] Shape after taking last time step: torch.Size([32, 64])
  [Classifier] Input shape: torch.Size([32, 64])
  [Classifier] Output shape: torch.Size([32, 12])

--- Completed a few batches for validation purposes; ending this epoch early. ---

========================================================
Epoch 38, Loss: 1.6380, Accuracy: 0.9948
========================================================


[STFTnet] Initial input shape: torch.Size([32, 120, 7])
[STFTnet] Shape after permuting for conv: torch.Size([32, 7, 120])
  [MFA Block] Input shape: torch.Size([32, 7, 120])
  [MFA Block] After global average pooling shape: torch.Size([32, 7, 1])
  [MFA Block] Attention weights shape: torch.Size([32, 7, 1])
  [MFA Block] Output after attention weighting shape: torch.Size([32, 7, 120])
[STFTnet] Shape after channel projection conv: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Input shape: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Output shape: torch.Size([32, 64, 120])
[STFTnet] Shape after permuting for transformer: torch.Size([32, 120, 64])
  [Transformer Encoder] Input shape (N, L, C): torch.Size([32, 120, 64])
  [Transformer Encoder] Output shape: torch.Size([32, 120, 64])
[STFTnet] Shape after taking last time step: torch.Size([32, 64])
  [Classifier] Input shape: torch.Size([32, 64])
  [Classifier] Output shape: torch.Size([32, 12])

[STFTnet] Initial input shape: torch.Size([32, 120, 7])
[STFTnet] Shape after permuting for conv: torch.Size([32, 7, 120])
  [MFA Block] Input shape: torch.Size([32, 7, 120])
  [MFA Block] After global average pooling shape: torch.Size([32, 7, 1])
  [MFA Block] Attention weights shape: torch.Size([32, 7, 1])
  [MFA Block] Output after attention weighting shape: torch.Size([32, 7, 120])
[STFTnet] Shape after channel projection conv: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Input shape: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Output shape: torch.Size([32, 64, 120])
[STFTnet] Shape after permuting for transformer: torch.Size([32, 120, 64])
  [Transformer Encoder] Input shape (N, L, C): torch.Size([32, 120, 64])
  [Transformer Encoder] Output shape: torch.Size([32, 120, 64])
[STFTnet] Shape after taking last time step: torch.Size([32, 64])
  [Classifier] Input shape: torch.Size([32, 64])
  [Classifier] Output shape: torch.Size([32, 12])

[STFTnet] Initial input shape: torch.Size([32, 120, 7])
[STFTnet] Shape after permuting for conv: torch.Size([32, 7, 120])
  [MFA Block] Input shape: torch.Size([32, 7, 120])
  [MFA Block] After global average pooling shape: torch.Size([32, 7, 1])
  [MFA Block] Attention weights shape: torch.Size([32, 7, 1])
  [MFA Block] Output after attention weighting shape: torch.Size([32, 7, 120])
[STFTnet] Shape after channel projection conv: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Input shape: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Output shape: torch.Size([32, 64, 120])
[STFTnet] Shape after permuting for transformer: torch.Size([32, 120, 64])
  [Transformer Encoder] Input shape (N, L, C): torch.Size([32, 120, 64])
  [Transformer Encoder] Output shape: torch.Size([32, 120, 64])
[STFTnet] Shape after taking last time step: torch.Size([32, 64])
  [Classifier] Input shape: torch.Size([32, 64])
  [Classifier] Output shape: torch.Size([32, 12])

[STFTnet] Initial input shape: torch.Size([32, 120, 7])
[STFTnet] Shape after permuting for conv: torch.Size([32, 7, 120])
  [MFA Block] Input shape: torch.Size([32, 7, 120])
  [MFA Block] After global average pooling shape: torch.Size([32, 7, 1])
  [MFA Block] Attention weights shape: torch.Size([32, 7, 1])
  [MFA Block] Output after attention weighting shape: torch.Size([32, 7, 120])
[STFTnet] Shape after channel projection conv: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Input shape: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Output shape: torch.Size([32, 64, 120])
[STFTnet] Shape after permuting for transformer: torch.Size([32, 120, 64])
  [Transformer Encoder] Input shape (N, L, C): torch.Size([32, 120, 64])
  [Transformer Encoder] Output shape: torch.Size([32, 120, 64])
[STFTnet] Shape after taking last time step: torch.Size([32, 64])
  [Classifier] Input shape: torch.Size([32, 64])
  [Classifier] Output shape: torch.Size([32, 12])

[STFTnet] Initial input shape: torch.Size([32, 120, 7])
[STFTnet] Shape after permuting for conv: torch.Size([32, 7, 120])
  [MFA Block] Input shape: torch.Size([32, 7, 120])
  [MFA Block] After global average pooling shape: torch.Size([32, 7, 1])
  [MFA Block] Attention weights shape: torch.Size([32, 7, 1])
  [MFA Block] Output after attention weighting shape: torch.Size([32, 7, 120])
[STFTnet] Shape after channel projection conv: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Input shape: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Output shape: torch.Size([32, 64, 120])
[STFTnet] Shape after permuting for transformer: torch.Size([32, 120, 64])
  [Transformer Encoder] Input shape (N, L, C): torch.Size([32, 120, 64])
  [Transformer Encoder] Output shape: torch.Size([32, 120, 64])
[STFTnet] Shape after taking last time step: torch.Size([32, 64])
  [Classifier] Input shape: torch.Size([32, 64])
  [Classifier] Output shape: torch.Size([32, 12])

[STFTnet] Initial input shape: torch.Size([32, 120, 7])
[STFTnet] Shape after permuting for conv: torch.Size([32, 7, 120])
  [MFA Block] Input shape: torch.Size([32, 7, 120])
  [MFA Block] After global average pooling shape: torch.Size([32, 7, 1])
  [MFA Block] Attention weights shape: torch.Size([32, 7, 1])
  [MFA Block] Output after attention weighting shape: torch.Size([32, 7, 120])
[STFTnet] Shape after channel projection conv: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Input shape: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Output shape: torch.Size([32, 64, 120])
[STFTnet] Shape after permuting for transformer: torch.Size([32, 120, 64])
  [Transformer Encoder] Input shape (N, L, C): torch.Size([32, 120, 64])
  [Transformer Encoder] Output shape: torch.Size([32, 120, 64])
[STFTnet] Shape after taking last time step: torch.Size([32, 64])
  [Classifier] Input shape: torch.Size([32, 64])
  [Classifier] Output shape: torch.Size([32, 12])

--- Completed a few batches for validation purposes; ending this epoch early. ---

========================================================
Epoch 39, Loss: 1.6354, Accuracy: 0.9896
========================================================


[STFTnet] Initial input shape: torch.Size([32, 120, 7])
[STFTnet] Shape after permuting for conv: torch.Size([32, 7, 120])
  [MFA Block] Input shape: torch.Size([32, 7, 120])
  [MFA Block] After global average pooling shape: torch.Size([32, 7, 1])
  [MFA Block] Attention weights shape: torch.Size([32, 7, 1])
  [MFA Block] Output after attention weighting shape: torch.Size([32, 7, 120])
[STFTnet] Shape after channel projection conv: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Input shape: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Output shape: torch.Size([32, 64, 120])
[STFTnet] Shape after permuting for transformer: torch.Size([32, 120, 64])
  [Transformer Encoder] Input shape (N, L, C): torch.Size([32, 120, 64])
  [Transformer Encoder] Output shape: torch.Size([32, 120, 64])
[STFTnet] Shape after taking last time step: torch.Size([32, 64])
  [Classifier] Input shape: torch.Size([32, 64])
  [Classifier] Output shape: torch.Size([32, 12])

[STFTnet] Initial input shape: torch.Size([32, 120, 7])
[STFTnet] Shape after permuting for conv: torch.Size([32, 7, 120])
  [MFA Block] Input shape: torch.Size([32, 7, 120])
  [MFA Block] After global average pooling shape: torch.Size([32, 7, 1])
  [MFA Block] Attention weights shape: torch.Size([32, 7, 1])
  [MFA Block] Output after attention weighting shape: torch.Size([32, 7, 120])
[STFTnet] Shape after channel projection conv: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Input shape: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Output shape: torch.Size([32, 64, 120])
[STFTnet] Shape after permuting for transformer: torch.Size([32, 120, 64])
  [Transformer Encoder] Input shape (N, L, C): torch.Size([32, 120, 64])
  [Transformer Encoder] Output shape: torch.Size([32, 120, 64])
[STFTnet] Shape after taking last time step: torch.Size([32, 64])
  [Classifier] Input shape: torch.Size([32, 64])
  [Classifier] Output shape: torch.Size([32, 12])

[STFTnet] Initial input shape: torch.Size([32, 120, 7])
[STFTnet] Shape after permuting for conv: torch.Size([32, 7, 120])
  [MFA Block] Input shape: torch.Size([32, 7, 120])
  [MFA Block] After global average pooling shape: torch.Size([32, 7, 1])
  [MFA Block] Attention weights shape: torch.Size([32, 7, 1])
  [MFA Block] Output after attention weighting shape: torch.Size([32, 7, 120])
[STFTnet] Shape after channel projection conv: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Input shape: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Output shape: torch.Size([32, 64, 120])
[STFTnet] Shape after permuting for transformer: torch.Size([32, 120, 64])
  [Transformer Encoder] Input shape (N, L, C): torch.Size([32, 120, 64])
  [Transformer Encoder] Output shape: torch.Size([32, 120, 64])
[STFTnet] Shape after taking last time step: torch.Size([32, 64])
  [Classifier] Input shape: torch.Size([32, 64])
  [Classifier] Output shape: torch.Size([32, 12])

[STFTnet] Initial input shape: torch.Size([32, 120, 7])
[STFTnet] Shape after permuting for conv: torch.Size([32, 7, 120])
  [MFA Block] Input shape: torch.Size([32, 7, 120])
  [MFA Block] After global average pooling shape: torch.Size([32, 7, 1])
  [MFA Block] Attention weights shape: torch.Size([32, 7, 1])
  [MFA Block] Output after attention weighting shape: torch.Size([32, 7, 120])
[STFTnet] Shape after channel projection conv: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Input shape: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Output shape: torch.Size([32, 64, 120])
[STFTnet] Shape after permuting for transformer: torch.Size([32, 120, 64])
  [Transformer Encoder] Input shape (N, L, C): torch.Size([32, 120, 64])
  [Transformer Encoder] Output shape: torch.Size([32, 120, 64])
[STFTnet] Shape after taking last time step: torch.Size([32, 64])
  [Classifier] Input shape: torch.Size([32, 64])
  [Classifier] Output shape: torch.Size([32, 12])

[STFTnet] Initial input shape: torch.Size([32, 120, 7])
[STFTnet] Shape after permuting for conv: torch.Size([32, 7, 120])
  [MFA Block] Input shape: torch.Size([32, 7, 120])
  [MFA Block] After global average pooling shape: torch.Size([32, 7, 1])
  [MFA Block] Attention weights shape: torch.Size([32, 7, 1])
  [MFA Block] Output after attention weighting shape: torch.Size([32, 7, 120])
[STFTnet] Shape after channel projection conv: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Input shape: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Output shape: torch.Size([32, 64, 120])
[STFTnet] Shape after permuting for transformer: torch.Size([32, 120, 64])
  [Transformer Encoder] Input shape (N, L, C): torch.Size([32, 120, 64])
  [Transformer Encoder] Output shape: torch.Size([32, 120, 64])
[STFTnet] Shape after taking last time step: torch.Size([32, 64])
  [Classifier] Input shape: torch.Size([32, 64])
  [Classifier] Output shape: torch.Size([32, 12])

[STFTnet] Initial input shape: torch.Size([32, 120, 7])
[STFTnet] Shape after permuting for conv: torch.Size([32, 7, 120])
  [MFA Block] Input shape: torch.Size([32, 7, 120])
  [MFA Block] After global average pooling shape: torch.Size([32, 7, 1])
  [MFA Block] Attention weights shape: torch.Size([32, 7, 1])
  [MFA Block] Output after attention weighting shape: torch.Size([32, 7, 120])
[STFTnet] Shape after channel projection conv: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Input shape: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Output shape: torch.Size([32, 64, 120])
[STFTnet] Shape after permuting for transformer: torch.Size([32, 120, 64])
  [Transformer Encoder] Input shape (N, L, C): torch.Size([32, 120, 64])
  [Transformer Encoder] Output shape: torch.Size([32, 120, 64])
[STFTnet] Shape after taking last time step: torch.Size([32, 64])
  [Classifier] Input shape: torch.Size([32, 64])
  [Classifier] Output shape: torch.Size([32, 12])

--- Completed a few batches for validation purposes; ending this epoch early. ---

========================================================
Epoch 40, Loss: 1.6285, Accuracy: 0.9948
========================================================


[STFTnet] Initial input shape: torch.Size([32, 120, 7])
[STFTnet] Shape after permuting for conv: torch.Size([32, 7, 120])
  [MFA Block] Input shape: torch.Size([32, 7, 120])
  [MFA Block] After global average pooling shape: torch.Size([32, 7, 1])
  [MFA Block] Attention weights shape: torch.Size([32, 7, 1])
  [MFA Block] Output after attention weighting shape: torch.Size([32, 7, 120])
[STFTnet] Shape after channel projection conv: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Input shape: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Output shape: torch.Size([32, 64, 120])
[STFTnet] Shape after permuting for transformer: torch.Size([32, 120, 64])
  [Transformer Encoder] Input shape (N, L, C): torch.Size([32, 120, 64])
  [Transformer Encoder] Output shape: torch.Size([32, 120, 64])
[STFTnet] Shape after taking last time step: torch.Size([32, 64])
  [Classifier] Input shape: torch.Size([32, 64])
  [Classifier] Output shape: torch.Size([32, 12])

[STFTnet] Initial input shape: torch.Size([32, 120, 7])
[STFTnet] Shape after permuting for conv: torch.Size([32, 7, 120])
  [MFA Block] Input shape: torch.Size([32, 7, 120])
  [MFA Block] After global average pooling shape: torch.Size([32, 7, 1])
  [MFA Block] Attention weights shape: torch.Size([32, 7, 1])
  [MFA Block] Output after attention weighting shape: torch.Size([32, 7, 120])
[STFTnet] Shape after channel projection conv: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Input shape: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Output shape: torch.Size([32, 64, 120])
[STFTnet] Shape after permuting for transformer: torch.Size([32, 120, 64])
  [Transformer Encoder] Input shape (N, L, C): torch.Size([32, 120, 64])
  [Transformer Encoder] Output shape: torch.Size([32, 120, 64])
[STFTnet] Shape after taking last time step: torch.Size([32, 64])
  [Classifier] Input shape: torch.Size([32, 64])
  [Classifier] Output shape: torch.Size([32, 12])

[STFTnet] Initial input shape: torch.Size([32, 120, 7])
[STFTnet] Shape after permuting for conv: torch.Size([32, 7, 120])
  [MFA Block] Input shape: torch.Size([32, 7, 120])
  [MFA Block] After global average pooling shape: torch.Size([32, 7, 1])
  [MFA Block] Attention weights shape: torch.Size([32, 7, 1])
  [MFA Block] Output after attention weighting shape: torch.Size([32, 7, 120])
[STFTnet] Shape after channel projection conv: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Input shape: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Output shape: torch.Size([32, 64, 120])
[STFTnet] Shape after permuting for transformer: torch.Size([32, 120, 64])
  [Transformer Encoder] Input shape (N, L, C): torch.Size([32, 120, 64])
  [Transformer Encoder] Output shape: torch.Size([32, 120, 64])
[STFTnet] Shape after taking last time step: torch.Size([32, 64])
  [Classifier] Input shape: torch.Size([32, 64])
  [Classifier] Output shape: torch.Size([32, 12])

[STFTnet] Initial input shape: torch.Size([32, 120, 7])
[STFTnet] Shape after permuting for conv: torch.Size([32, 7, 120])
  [MFA Block] Input shape: torch.Size([32, 7, 120])
  [MFA Block] After global average pooling shape: torch.Size([32, 7, 1])
  [MFA Block] Attention weights shape: torch.Size([32, 7, 1])
  [MFA Block] Output after attention weighting shape: torch.Size([32, 7, 120])
[STFTnet] Shape after channel projection conv: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Input shape: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Output shape: torch.Size([32, 64, 120])
[STFTnet] Shape after permuting for transformer: torch.Size([32, 120, 64])
  [Transformer Encoder] Input shape (N, L, C): torch.Size([32, 120, 64])
  [Transformer Encoder] Output shape: torch.Size([32, 120, 64])
[STFTnet] Shape after taking last time step: torch.Size([32, 64])
  [Classifier] Input shape: torch.Size([32, 64])
  [Classifier] Output shape: torch.Size([32, 12])

[STFTnet] Initial input shape: torch.Size([32, 120, 7])
[STFTnet] Shape after permuting for conv: torch.Size([32, 7, 120])
  [MFA Block] Input shape: torch.Size([32, 7, 120])
  [MFA Block] After global average pooling shape: torch.Size([32, 7, 1])
  [MFA Block] Attention weights shape: torch.Size([32, 7, 1])
  [MFA Block] Output after attention weighting shape: torch.Size([32, 7, 120])
[STFTnet] Shape after channel projection conv: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Input shape: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Output shape: torch.Size([32, 64, 120])
[STFTnet] Shape after permuting for transformer: torch.Size([32, 120, 64])
  [Transformer Encoder] Input shape (N, L, C): torch.Size([32, 120, 64])
  [Transformer Encoder] Output shape: torch.Size([32, 120, 64])
[STFTnet] Shape after taking last time step: torch.Size([32, 64])
  [Classifier] Input shape: torch.Size([32, 64])
  [Classifier] Output shape: torch.Size([32, 12])

[STFTnet] Initial input shape: torch.Size([32, 120, 7])
[STFTnet] Shape after permuting for conv: torch.Size([32, 7, 120])
  [MFA Block] Input shape: torch.Size([32, 7, 120])
  [MFA Block] After global average pooling shape: torch.Size([32, 7, 1])
  [MFA Block] Attention weights shape: torch.Size([32, 7, 1])
  [MFA Block] Output after attention weighting shape: torch.Size([32, 7, 120])
[STFTnet] Shape after channel projection conv: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Input shape: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Output shape: torch.Size([32, 64, 120])
[STFTnet] Shape after permuting for transformer: torch.Size([32, 120, 64])
  [Transformer Encoder] Input shape (N, L, C): torch.Size([32, 120, 64])
  [Transformer Encoder] Output shape: torch.Size([32, 120, 64])
[STFTnet] Shape after taking last time step: torch.Size([32, 64])
  [Classifier] Input shape: torch.Size([32, 64])
  [Classifier] Output shape: torch.Size([32, 12])

--- Completed a few batches for validation purposes; ending this epoch early. ---

========================================================
Epoch 41, Loss: 1.6273, Accuracy: 1.0000
========================================================


[STFTnet] Initial input shape: torch.Size([32, 120, 7])
[STFTnet] Shape after permuting for conv: torch.Size([32, 7, 120])
  [MFA Block] Input shape: torch.Size([32, 7, 120])
  [MFA Block] After global average pooling shape: torch.Size([32, 7, 1])
  [MFA Block] Attention weights shape: torch.Size([32, 7, 1])
  [MFA Block] Output after attention weighting shape: torch.Size([32, 7, 120])
[STFTnet] Shape after channel projection conv: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Input shape: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Output shape: torch.Size([32, 64, 120])
[STFTnet] Shape after permuting for transformer: torch.Size([32, 120, 64])
  [Transformer Encoder] Input shape (N, L, C): torch.Size([32, 120, 64])
  [Transformer Encoder] Output shape: torch.Size([32, 120, 64])
[STFTnet] Shape after taking last time step: torch.Size([32, 64])
  [Classifier] Input shape: torch.Size([32, 64])
  [Classifier] Output shape: torch.Size([32, 12])

[STFTnet] Initial input shape: torch.Size([32, 120, 7])
[STFTnet] Shape after permuting for conv: torch.Size([32, 7, 120])
  [MFA Block] Input shape: torch.Size([32, 7, 120])
  [MFA Block] After global average pooling shape: torch.Size([32, 7, 1])
  [MFA Block] Attention weights shape: torch.Size([32, 7, 1])
  [MFA Block] Output after attention weighting shape: torch.Size([32, 7, 120])
[STFTnet] Shape after channel projection conv: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Input shape: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Output shape: torch.Size([32, 64, 120])
[STFTnet] Shape after permuting for transformer: torch.Size([32, 120, 64])
  [Transformer Encoder] Input shape (N, L, C): torch.Size([32, 120, 64])
  [Transformer Encoder] Output shape: torch.Size([32, 120, 64])
[STFTnet] Shape after taking last time step: torch.Size([32, 64])
  [Classifier] Input shape: torch.Size([32, 64])
  [Classifier] Output shape: torch.Size([32, 12])

[STFTnet] Initial input shape: torch.Size([32, 120, 7])
[STFTnet] Shape after permuting for conv: torch.Size([32, 7, 120])
  [MFA Block] Input shape: torch.Size([32, 7, 120])
  [MFA Block] After global average pooling shape: torch.Size([32, 7, 1])
  [MFA Block] Attention weights shape: torch.Size([32, 7, 1])
  [MFA Block] Output after attention weighting shape: torch.Size([32, 7, 120])
[STFTnet] Shape after channel projection conv: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Input shape: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Output shape: torch.Size([32, 64, 120])
[STFTnet] Shape after permuting for transformer: torch.Size([32, 120, 64])
  [Transformer Encoder] Input shape (N, L, C): torch.Size([32, 120, 64])
  [Transformer Encoder] Output shape: torch.Size([32, 120, 64])
[STFTnet] Shape after taking last time step: torch.Size([32, 64])
  [Classifier] Input shape: torch.Size([32, 64])
  [Classifier] Output shape: torch.Size([32, 12])

[STFTnet] Initial input shape: torch.Size([32, 120, 7])
[STFTnet] Shape after permuting for conv: torch.Size([32, 7, 120])
  [MFA Block] Input shape: torch.Size([32, 7, 120])
  [MFA Block] After global average pooling shape: torch.Size([32, 7, 1])
  [MFA Block] Attention weights shape: torch.Size([32, 7, 1])
  [MFA Block] Output after attention weighting shape: torch.Size([32, 7, 120])
[STFTnet] Shape after channel projection conv: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Input shape: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Output shape: torch.Size([32, 64, 120])
[STFTnet] Shape after permuting for transformer: torch.Size([32, 120, 64])
  [Transformer Encoder] Input shape (N, L, C): torch.Size([32, 120, 64])
  [Transformer Encoder] Output shape: torch.Size([32, 120, 64])
[STFTnet] Shape after taking last time step: torch.Size([32, 64])
  [Classifier] Input shape: torch.Size([32, 64])
  [Classifier] Output shape: torch.Size([32, 12])

[STFTnet] Initial input shape: torch.Size([32, 120, 7])
[STFTnet] Shape after permuting for conv: torch.Size([32, 7, 120])
  [MFA Block] Input shape: torch.Size([32, 7, 120])
  [MFA Block] After global average pooling shape: torch.Size([32, 7, 1])
  [MFA Block] Attention weights shape: torch.Size([32, 7, 1])
  [MFA Block] Output after attention weighting shape: torch.Size([32, 7, 120])
[STFTnet] Shape after channel projection conv: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Input shape: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Output shape: torch.Size([32, 64, 120])
[STFTnet] Shape after permuting for transformer: torch.Size([32, 120, 64])
  [Transformer Encoder] Input shape (N, L, C): torch.Size([32, 120, 64])
  [Transformer Encoder] Output shape: torch.Size([32, 120, 64])
[STFTnet] Shape after taking last time step: torch.Size([32, 64])
  [Classifier] Input shape: torch.Size([32, 64])
  [Classifier] Output shape: torch.Size([32, 12])

[STFTnet] Initial input shape: torch.Size([32, 120, 7])
[STFTnet] Shape after permuting for conv: torch.Size([32, 7, 120])
  [MFA Block] Input shape: torch.Size([32, 7, 120])
  [MFA Block] After global average pooling shape: torch.Size([32, 7, 1])
  [MFA Block] Attention weights shape: torch.Size([32, 7, 1])
  [MFA Block] Output after attention weighting shape: torch.Size([32, 7, 120])
[STFTnet] Shape after channel projection conv: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Input shape: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Output shape: torch.Size([32, 64, 120])
[STFTnet] Shape after permuting for transformer: torch.Size([32, 120, 64])
  [Transformer Encoder] Input shape (N, L, C): torch.Size([32, 120, 64])
  [Transformer Encoder] Output shape: torch.Size([32, 120, 64])
[STFTnet] Shape after taking last time step: torch.Size([32, 64])
  [Classifier] Input shape: torch.Size([32, 64])
  [Classifier] Output shape: torch.Size([32, 12])

--- Completed a few batches for validation purposes; ending this epoch early. ---

========================================================
Epoch 42, Loss: 1.6263, Accuracy: 1.0000
========================================================


[STFTnet] Initial input shape: torch.Size([32, 120, 7])
[STFTnet] Shape after permuting for conv: torch.Size([32, 7, 120])
  [MFA Block] Input shape: torch.Size([32, 7, 120])
  [MFA Block] After global average pooling shape: torch.Size([32, 7, 1])
  [MFA Block] Attention weights shape: torch.Size([32, 7, 1])
  [MFA Block] Output after attention weighting shape: torch.Size([32, 7, 120])
[STFTnet] Shape after channel projection conv: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Input shape: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Output shape: torch.Size([32, 64, 120])
[STFTnet] Shape after permuting for transformer: torch.Size([32, 120, 64])
  [Transformer Encoder] Input shape (N, L, C): torch.Size([32, 120, 64])
  [Transformer Encoder] Output shape: torch.Size([32, 120, 64])
[STFTnet] Shape after taking last time step: torch.Size([32, 64])
  [Classifier] Input shape: torch.Size([32, 64])
  [Classifier] Output shape: torch.Size([32, 12])

[STFTnet] Initial input shape: torch.Size([32, 120, 7])
[STFTnet] Shape after permuting for conv: torch.Size([32, 7, 120])
  [MFA Block] Input shape: torch.Size([32, 7, 120])
  [MFA Block] After global average pooling shape: torch.Size([32, 7, 1])
  [MFA Block] Attention weights shape: torch.Size([32, 7, 1])
  [MFA Block] Output after attention weighting shape: torch.Size([32, 7, 120])
[STFTnet] Shape after channel projection conv: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Input shape: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Output shape: torch.Size([32, 64, 120])
[STFTnet] Shape after permuting for transformer: torch.Size([32, 120, 64])
  [Transformer Encoder] Input shape (N, L, C): torch.Size([32, 120, 64])
  [Transformer Encoder] Output shape: torch.Size([32, 120, 64])
[STFTnet] Shape after taking last time step: torch.Size([32, 64])
  [Classifier] Input shape: torch.Size([32, 64])
  [Classifier] Output shape: torch.Size([32, 12])

[STFTnet] Initial input shape: torch.Size([32, 120, 7])
[STFTnet] Shape after permuting for conv: torch.Size([32, 7, 120])
  [MFA Block] Input shape: torch.Size([32, 7, 120])
  [MFA Block] After global average pooling shape: torch.Size([32, 7, 1])
  [MFA Block] Attention weights shape: torch.Size([32, 7, 1])
  [MFA Block] Output after attention weighting shape: torch.Size([32, 7, 120])
[STFTnet] Shape after channel projection conv: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Input shape: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Output shape: torch.Size([32, 64, 120])
[STFTnet] Shape after permuting for transformer: torch.Size([32, 120, 64])
  [Transformer Encoder] Input shape (N, L, C): torch.Size([32, 120, 64])
  [Transformer Encoder] Output shape: torch.Size([32, 120, 64])
[STFTnet] Shape after taking last time step: torch.Size([32, 64])
  [Classifier] Input shape: torch.Size([32, 64])
  [Classifier] Output shape: torch.Size([32, 12])

[STFTnet] Initial input shape: torch.Size([32, 120, 7])
[STFTnet] Shape after permuting for conv: torch.Size([32, 7, 120])
  [MFA Block] Input shape: torch.Size([32, 7, 120])
  [MFA Block] After global average pooling shape: torch.Size([32, 7, 1])
  [MFA Block] Attention weights shape: torch.Size([32, 7, 1])
  [MFA Block] Output after attention weighting shape: torch.Size([32, 7, 120])
[STFTnet] Shape after channel projection conv: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Input shape: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Output shape: torch.Size([32, 64, 120])
[STFTnet] Shape after permuting for transformer: torch.Size([32, 120, 64])
  [Transformer Encoder] Input shape (N, L, C): torch.Size([32, 120, 64])
  [Transformer Encoder] Output shape: torch.Size([32, 120, 64])
[STFTnet] Shape after taking last time step: torch.Size([32, 64])
  [Classifier] Input shape: torch.Size([32, 64])
  [Classifier] Output shape: torch.Size([32, 12])

[STFTnet] Initial input shape: torch.Size([32, 120, 7])
[STFTnet] Shape after permuting for conv: torch.Size([32, 7, 120])
  [MFA Block] Input shape: torch.Size([32, 7, 120])
  [MFA Block] After global average pooling shape: torch.Size([32, 7, 1])
  [MFA Block] Attention weights shape: torch.Size([32, 7, 1])
  [MFA Block] Output after attention weighting shape: torch.Size([32, 7, 120])
[STFTnet] Shape after channel projection conv: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Input shape: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Output shape: torch.Size([32, 64, 120])
[STFTnet] Shape after permuting for transformer: torch.Size([32, 120, 64])
  [Transformer Encoder] Input shape (N, L, C): torch.Size([32, 120, 64])
  [Transformer Encoder] Output shape: torch.Size([32, 120, 64])
[STFTnet] Shape after taking last time step: torch.Size([32, 64])
  [Classifier] Input shape: torch.Size([32, 64])
  [Classifier] Output shape: torch.Size([32, 12])

[STFTnet] Initial input shape: torch.Size([32, 120, 7])
[STFTnet] Shape after permuting for conv: torch.Size([32, 7, 120])
  [MFA Block] Input shape: torch.Size([32, 7, 120])
  [MFA Block] After global average pooling shape: torch.Size([32, 7, 1])
  [MFA Block] Attention weights shape: torch.Size([32, 7, 1])
  [MFA Block] Output after attention weighting shape: torch.Size([32, 7, 120])
[STFTnet] Shape after channel projection conv: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Input shape: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Output shape: torch.Size([32, 64, 120])
[STFTnet] Shape after permuting for transformer: torch.Size([32, 120, 64])
  [Transformer Encoder] Input shape (N, L, C): torch.Size([32, 120, 64])
  [Transformer Encoder] Output shape: torch.Size([32, 120, 64])
[STFTnet] Shape after taking last time step: torch.Size([32, 64])
  [Classifier] Input shape: torch.Size([32, 64])
  [Classifier] Output shape: torch.Size([32, 12])

--- Completed a few batches for validation purposes; ending this epoch early. ---

========================================================
Epoch 43, Loss: 1.6299, Accuracy: 0.9948
========================================================


[STFTnet] Initial input shape: torch.Size([32, 120, 7])
[STFTnet] Shape after permuting for conv: torch.Size([32, 7, 120])
  [MFA Block] Input shape: torch.Size([32, 7, 120])
  [MFA Block] After global average pooling shape: torch.Size([32, 7, 1])
  [MFA Block] Attention weights shape: torch.Size([32, 7, 1])
  [MFA Block] Output after attention weighting shape: torch.Size([32, 7, 120])
[STFTnet] Shape after channel projection conv: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Input shape: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Output shape: torch.Size([32, 64, 120])
[STFTnet] Shape after permuting for transformer: torch.Size([32, 120, 64])
  [Transformer Encoder] Input shape (N, L, C): torch.Size([32, 120, 64])
  [Transformer Encoder] Output shape: torch.Size([32, 120, 64])
[STFTnet] Shape after taking last time step: torch.Size([32, 64])
  [Classifier] Input shape: torch.Size([32, 64])
  [Classifier] Output shape: torch.Size([32, 12])

[STFTnet] Initial input shape: torch.Size([32, 120, 7])
[STFTnet] Shape after permuting for conv: torch.Size([32, 7, 120])
  [MFA Block] Input shape: torch.Size([32, 7, 120])
  [MFA Block] After global average pooling shape: torch.Size([32, 7, 1])
  [MFA Block] Attention weights shape: torch.Size([32, 7, 1])
  [MFA Block] Output after attention weighting shape: torch.Size([32, 7, 120])
[STFTnet] Shape after channel projection conv: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Input shape: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Output shape: torch.Size([32, 64, 120])
[STFTnet] Shape after permuting for transformer: torch.Size([32, 120, 64])
  [Transformer Encoder] Input shape (N, L, C): torch.Size([32, 120, 64])
  [Transformer Encoder] Output shape: torch.Size([32, 120, 64])
[STFTnet] Shape after taking last time step: torch.Size([32, 64])
  [Classifier] Input shape: torch.Size([32, 64])
  [Classifier] Output shape: torch.Size([32, 12])

[STFTnet] Initial input shape: torch.Size([32, 120, 7])
[STFTnet] Shape after permuting for conv: torch.Size([32, 7, 120])
  [MFA Block] Input shape: torch.Size([32, 7, 120])
  [MFA Block] After global average pooling shape: torch.Size([32, 7, 1])
  [MFA Block] Attention weights shape: torch.Size([32, 7, 1])
  [MFA Block] Output after attention weighting shape: torch.Size([32, 7, 120])
[STFTnet] Shape after channel projection conv: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Input shape: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Output shape: torch.Size([32, 64, 120])
[STFTnet] Shape after permuting for transformer: torch.Size([32, 120, 64])
  [Transformer Encoder] Input shape (N, L, C): torch.Size([32, 120, 64])
  [Transformer Encoder] Output shape: torch.Size([32, 120, 64])
[STFTnet] Shape after taking last time step: torch.Size([32, 64])
  [Classifier] Input shape: torch.Size([32, 64])
  [Classifier] Output shape: torch.Size([32, 12])

[STFTnet] Initial input shape: torch.Size([32, 120, 7])
[STFTnet] Shape after permuting for conv: torch.Size([32, 7, 120])
  [MFA Block] Input shape: torch.Size([32, 7, 120])
  [MFA Block] After global average pooling shape: torch.Size([32, 7, 1])
  [MFA Block] Attention weights shape: torch.Size([32, 7, 1])
  [MFA Block] Output after attention weighting shape: torch.Size([32, 7, 120])
[STFTnet] Shape after channel projection conv: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Input shape: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Output shape: torch.Size([32, 64, 120])
[STFTnet] Shape after permuting for transformer: torch.Size([32, 120, 64])
  [Transformer Encoder] Input shape (N, L, C): torch.Size([32, 120, 64])
  [Transformer Encoder] Output shape: torch.Size([32, 120, 64])
[STFTnet] Shape after taking last time step: torch.Size([32, 64])
  [Classifier] Input shape: torch.Size([32, 64])
  [Classifier] Output shape: torch.Size([32, 12])

[STFTnet] Initial input shape: torch.Size([32, 120, 7])
[STFTnet] Shape after permuting for conv: torch.Size([32, 7, 120])
  [MFA Block] Input shape: torch.Size([32, 7, 120])
  [MFA Block] After global average pooling shape: torch.Size([32, 7, 1])
  [MFA Block] Attention weights shape: torch.Size([32, 7, 1])
  [MFA Block] Output after attention weighting shape: torch.Size([32, 7, 120])
[STFTnet] Shape after channel projection conv: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Input shape: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Output shape: torch.Size([32, 64, 120])
[STFTnet] Shape after permuting for transformer: torch.Size([32, 120, 64])
  [Transformer Encoder] Input shape (N, L, C): torch.Size([32, 120, 64])
  [Transformer Encoder] Output shape: torch.Size([32, 120, 64])
[STFTnet] Shape after taking last time step: torch.Size([32, 64])
  [Classifier] Input shape: torch.Size([32, 64])
  [Classifier] Output shape: torch.Size([32, 12])

[STFTnet] Initial input shape: torch.Size([32, 120, 7])
[STFTnet] Shape after permuting for conv: torch.Size([32, 7, 120])
  [MFA Block] Input shape: torch.Size([32, 7, 120])
  [MFA Block] After global average pooling shape: torch.Size([32, 7, 1])
  [MFA Block] Attention weights shape: torch.Size([32, 7, 1])
  [MFA Block] Output after attention weighting shape: torch.Size([32, 7, 120])
[STFTnet] Shape after channel projection conv: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Input shape: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Output shape: torch.Size([32, 64, 120])
[STFTnet] Shape after permuting for transformer: torch.Size([32, 120, 64])
  [Transformer Encoder] Input shape (N, L, C): torch.Size([32, 120, 64])
  [Transformer Encoder] Output shape: torch.Size([32, 120, 64])
[STFTnet] Shape after taking last time step: torch.Size([32, 64])
  [Classifier] Input shape: torch.Size([32, 64])
  [Classifier] Output shape: torch.Size([32, 12])

--- Completed a few batches for validation purposes; ending this epoch early. ---

========================================================
Epoch 44, Loss: 1.6241, Accuracy: 1.0000
========================================================


[STFTnet] Initial input shape: torch.Size([32, 120, 7])
[STFTnet] Shape after permuting for conv: torch.Size([32, 7, 120])
  [MFA Block] Input shape: torch.Size([32, 7, 120])
  [MFA Block] After global average pooling shape: torch.Size([32, 7, 1])
  [MFA Block] Attention weights shape: torch.Size([32, 7, 1])
  [MFA Block] Output after attention weighting shape: torch.Size([32, 7, 120])
[STFTnet] Shape after channel projection conv: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Input shape: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Output shape: torch.Size([32, 64, 120])
[STFTnet] Shape after permuting for transformer: torch.Size([32, 120, 64])
  [Transformer Encoder] Input shape (N, L, C): torch.Size([32, 120, 64])
  [Transformer Encoder] Output shape: torch.Size([32, 120, 64])
[STFTnet] Shape after taking last time step: torch.Size([32, 64])
  [Classifier] Input shape: torch.Size([32, 64])
  [Classifier] Output shape: torch.Size([32, 12])

[STFTnet] Initial input shape: torch.Size([32, 120, 7])
[STFTnet] Shape after permuting for conv: torch.Size([32, 7, 120])
  [MFA Block] Input shape: torch.Size([32, 7, 120])
  [MFA Block] After global average pooling shape: torch.Size([32, 7, 1])
  [MFA Block] Attention weights shape: torch.Size([32, 7, 1])
  [MFA Block] Output after attention weighting shape: torch.Size([32, 7, 120])
[STFTnet] Shape after channel projection conv: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Input shape: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Output shape: torch.Size([32, 64, 120])
[STFTnet] Shape after permuting for transformer: torch.Size([32, 120, 64])
  [Transformer Encoder] Input shape (N, L, C): torch.Size([32, 120, 64])
  [Transformer Encoder] Output shape: torch.Size([32, 120, 64])
[STFTnet] Shape after taking last time step: torch.Size([32, 64])
  [Classifier] Input shape: torch.Size([32, 64])
  [Classifier] Output shape: torch.Size([32, 12])

[STFTnet] Initial input shape: torch.Size([32, 120, 7])
[STFTnet] Shape after permuting for conv: torch.Size([32, 7, 120])
  [MFA Block] Input shape: torch.Size([32, 7, 120])
  [MFA Block] After global average pooling shape: torch.Size([32, 7, 1])
  [MFA Block] Attention weights shape: torch.Size([32, 7, 1])
  [MFA Block] Output after attention weighting shape: torch.Size([32, 7, 120])
[STFTnet] Shape after channel projection conv: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Input shape: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Output shape: torch.Size([32, 64, 120])
[STFTnet] Shape after permuting for transformer: torch.Size([32, 120, 64])
  [Transformer Encoder] Input shape (N, L, C): torch.Size([32, 120, 64])
  [Transformer Encoder] Output shape: torch.Size([32, 120, 64])
[STFTnet] Shape after taking last time step: torch.Size([32, 64])
  [Classifier] Input shape: torch.Size([32, 64])
  [Classifier] Output shape: torch.Size([32, 12])

[STFTnet] Initial input shape: torch.Size([32, 120, 7])
[STFTnet] Shape after permuting for conv: torch.Size([32, 7, 120])
  [MFA Block] Input shape: torch.Size([32, 7, 120])
  [MFA Block] After global average pooling shape: torch.Size([32, 7, 1])
  [MFA Block] Attention weights shape: torch.Size([32, 7, 1])
  [MFA Block] Output after attention weighting shape: torch.Size([32, 7, 120])
[STFTnet] Shape after channel projection conv: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Input shape: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Output shape: torch.Size([32, 64, 120])
[STFTnet] Shape after permuting for transformer: torch.Size([32, 120, 64])
  [Transformer Encoder] Input shape (N, L, C): torch.Size([32, 120, 64])
  [Transformer Encoder] Output shape: torch.Size([32, 120, 64])
[STFTnet] Shape after taking last time step: torch.Size([32, 64])
  [Classifier] Input shape: torch.Size([32, 64])
  [Classifier] Output shape: torch.Size([32, 12])

[STFTnet] Initial input shape: torch.Size([32, 120, 7])
[STFTnet] Shape after permuting for conv: torch.Size([32, 7, 120])
  [MFA Block] Input shape: torch.Size([32, 7, 120])
  [MFA Block] After global average pooling shape: torch.Size([32, 7, 1])
  [MFA Block] Attention weights shape: torch.Size([32, 7, 1])
  [MFA Block] Output after attention weighting shape: torch.Size([32, 7, 120])
[STFTnet] Shape after channel projection conv: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Input shape: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Output shape: torch.Size([32, 64, 120])
[STFTnet] Shape after permuting for transformer: torch.Size([32, 120, 64])
  [Transformer Encoder] Input shape (N, L, C): torch.Size([32, 120, 64])
  [Transformer Encoder] Output shape: torch.Size([32, 120, 64])
[STFTnet] Shape after taking last time step: torch.Size([32, 64])
  [Classifier] Input shape: torch.Size([32, 64])
  [Classifier] Output shape: torch.Size([32, 12])

[STFTnet] Initial input shape: torch.Size([32, 120, 7])
[STFTnet] Shape after permuting for conv: torch.Size([32, 7, 120])
  [MFA Block] Input shape: torch.Size([32, 7, 120])
  [MFA Block] After global average pooling shape: torch.Size([32, 7, 1])
  [MFA Block] Attention weights shape: torch.Size([32, 7, 1])
  [MFA Block] Output after attention weighting shape: torch.Size([32, 7, 120])
[STFTnet] Shape after channel projection conv: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Input shape: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Output shape: torch.Size([32, 64, 120])
[STFTnet] Shape after permuting for transformer: torch.Size([32, 120, 64])
  [Transformer Encoder] Input shape (N, L, C): torch.Size([32, 120, 64])
  [Transformer Encoder] Output shape: torch.Size([32, 120, 64])
[STFTnet] Shape after taking last time step: torch.Size([32, 64])
  [Classifier] Input shape: torch.Size([32, 64])
  [Classifier] Output shape: torch.Size([32, 12])

--- Completed a few batches for validation purposes; ending this epoch early. ---

========================================================
Epoch 45, Loss: 1.6232, Accuracy: 1.0000
========================================================


[STFTnet] Initial input shape: torch.Size([32, 120, 7])
[STFTnet] Shape after permuting for conv: torch.Size([32, 7, 120])
  [MFA Block] Input shape: torch.Size([32, 7, 120])
  [MFA Block] After global average pooling shape: torch.Size([32, 7, 1])
  [MFA Block] Attention weights shape: torch.Size([32, 7, 1])
  [MFA Block] Output after attention weighting shape: torch.Size([32, 7, 120])
[STFTnet] Shape after channel projection conv: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Input shape: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Output shape: torch.Size([32, 64, 120])
[STFTnet] Shape after permuting for transformer: torch.Size([32, 120, 64])
  [Transformer Encoder] Input shape (N, L, C): torch.Size([32, 120, 64])
  [Transformer Encoder] Output shape: torch.Size([32, 120, 64])
[STFTnet] Shape after taking last time step: torch.Size([32, 64])
  [Classifier] Input shape: torch.Size([32, 64])
  [Classifier] Output shape: torch.Size([32, 12])

[STFTnet] Initial input shape: torch.Size([32, 120, 7])
[STFTnet] Shape after permuting for conv: torch.Size([32, 7, 120])
  [MFA Block] Input shape: torch.Size([32, 7, 120])
  [MFA Block] After global average pooling shape: torch.Size([32, 7, 1])
  [MFA Block] Attention weights shape: torch.Size([32, 7, 1])
  [MFA Block] Output after attention weighting shape: torch.Size([32, 7, 120])
[STFTnet] Shape after channel projection conv: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Input shape: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Output shape: torch.Size([32, 64, 120])
[STFTnet] Shape after permuting for transformer: torch.Size([32, 120, 64])
  [Transformer Encoder] Input shape (N, L, C): torch.Size([32, 120, 64])
  [Transformer Encoder] Output shape: torch.Size([32, 120, 64])
[STFTnet] Shape after taking last time step: torch.Size([32, 64])
  [Classifier] Input shape: torch.Size([32, 64])
  [Classifier] Output shape: torch.Size([32, 12])

[STFTnet] Initial input shape: torch.Size([32, 120, 7])
[STFTnet] Shape after permuting for conv: torch.Size([32, 7, 120])
  [MFA Block] Input shape: torch.Size([32, 7, 120])
  [MFA Block] After global average pooling shape: torch.Size([32, 7, 1])
  [MFA Block] Attention weights shape: torch.Size([32, 7, 1])
  [MFA Block] Output after attention weighting shape: torch.Size([32, 7, 120])
[STFTnet] Shape after channel projection conv: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Input shape: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Output shape: torch.Size([32, 64, 120])
[STFTnet] Shape after permuting for transformer: torch.Size([32, 120, 64])
  [Transformer Encoder] Input shape (N, L, C): torch.Size([32, 120, 64])
  [Transformer Encoder] Output shape: torch.Size([32, 120, 64])
[STFTnet] Shape after taking last time step: torch.Size([32, 64])
  [Classifier] Input shape: torch.Size([32, 64])
  [Classifier] Output shape: torch.Size([32, 12])

[STFTnet] Initial input shape: torch.Size([32, 120, 7])
[STFTnet] Shape after permuting for conv: torch.Size([32, 7, 120])
  [MFA Block] Input shape: torch.Size([32, 7, 120])
  [MFA Block] After global average pooling shape: torch.Size([32, 7, 1])
  [MFA Block] Attention weights shape: torch.Size([32, 7, 1])
  [MFA Block] Output after attention weighting shape: torch.Size([32, 7, 120])
[STFTnet] Shape after channel projection conv: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Input shape: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Output shape: torch.Size([32, 64, 120])
[STFTnet] Shape after permuting for transformer: torch.Size([32, 120, 64])
  [Transformer Encoder] Input shape (N, L, C): torch.Size([32, 120, 64])
  [Transformer Encoder] Output shape: torch.Size([32, 120, 64])
[STFTnet] Shape after taking last time step: torch.Size([32, 64])
  [Classifier] Input shape: torch.Size([32, 64])
  [Classifier] Output shape: torch.Size([32, 12])

[STFTnet] Initial input shape: torch.Size([32, 120, 7])
[STFTnet] Shape after permuting for conv: torch.Size([32, 7, 120])
  [MFA Block] Input shape: torch.Size([32, 7, 120])
  [MFA Block] After global average pooling shape: torch.Size([32, 7, 1])
  [MFA Block] Attention weights shape: torch.Size([32, 7, 1])
  [MFA Block] Output after attention weighting shape: torch.Size([32, 7, 120])
[STFTnet] Shape after channel projection conv: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Input shape: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Output shape: torch.Size([32, 64, 120])
[STFTnet] Shape after permuting for transformer: torch.Size([32, 120, 64])
  [Transformer Encoder] Input shape (N, L, C): torch.Size([32, 120, 64])
  [Transformer Encoder] Output shape: torch.Size([32, 120, 64])
[STFTnet] Shape after taking last time step: torch.Size([32, 64])
  [Classifier] Input shape: torch.Size([32, 64])
  [Classifier] Output shape: torch.Size([32, 12])

[STFTnet] Initial input shape: torch.Size([32, 120, 7])
[STFTnet] Shape after permuting for conv: torch.Size([32, 7, 120])
  [MFA Block] Input shape: torch.Size([32, 7, 120])
  [MFA Block] After global average pooling shape: torch.Size([32, 7, 1])
  [MFA Block] Attention weights shape: torch.Size([32, 7, 1])
  [MFA Block] Output after attention weighting shape: torch.Size([32, 7, 120])
[STFTnet] Shape after channel projection conv: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Input shape: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Output shape: torch.Size([32, 64, 120])
[STFTnet] Shape after permuting for transformer: torch.Size([32, 120, 64])
  [Transformer Encoder] Input shape (N, L, C): torch.Size([32, 120, 64])
  [Transformer Encoder] Output shape: torch.Size([32, 120, 64])
[STFTnet] Shape after taking last time step: torch.Size([32, 64])
  [Classifier] Input shape: torch.Size([32, 64])
  [Classifier] Output shape: torch.Size([32, 12])

--- Completed a few batches for validation purposes; ending this epoch early. ---

========================================================
Epoch 46, Loss: 1.6232, Accuracy: 1.0000
========================================================


[STFTnet] Initial input shape: torch.Size([32, 120, 7])
[STFTnet] Shape after permuting for conv: torch.Size([32, 7, 120])
  [MFA Block] Input shape: torch.Size([32, 7, 120])
  [MFA Block] After global average pooling shape: torch.Size([32, 7, 1])
  [MFA Block] Attention weights shape: torch.Size([32, 7, 1])
  [MFA Block] Output after attention weighting shape: torch.Size([32, 7, 120])
[STFTnet] Shape after channel projection conv: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Input shape: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Output shape: torch.Size([32, 64, 120])
[STFTnet] Shape after permuting for transformer: torch.Size([32, 120, 64])
  [Transformer Encoder] Input shape (N, L, C): torch.Size([32, 120, 64])
  [Transformer Encoder] Output shape: torch.Size([32, 120, 64])
[STFTnet] Shape after taking last time step: torch.Size([32, 64])
  [Classifier] Input shape: torch.Size([32, 64])
  [Classifier] Output shape: torch.Size([32, 12])

[STFTnet] Initial input shape: torch.Size([32, 120, 7])
[STFTnet] Shape after permuting for conv: torch.Size([32, 7, 120])
  [MFA Block] Input shape: torch.Size([32, 7, 120])
  [MFA Block] After global average pooling shape: torch.Size([32, 7, 1])
  [MFA Block] Attention weights shape: torch.Size([32, 7, 1])
  [MFA Block] Output after attention weighting shape: torch.Size([32, 7, 120])
[STFTnet] Shape after channel projection conv: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Input shape: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Output shape: torch.Size([32, 64, 120])
[STFTnet] Shape after permuting for transformer: torch.Size([32, 120, 64])
  [Transformer Encoder] Input shape (N, L, C): torch.Size([32, 120, 64])
  [Transformer Encoder] Output shape: torch.Size([32, 120, 64])
[STFTnet] Shape after taking last time step: torch.Size([32, 64])
  [Classifier] Input shape: torch.Size([32, 64])
  [Classifier] Output shape: torch.Size([32, 12])

[STFTnet] Initial input shape: torch.Size([32, 120, 7])
[STFTnet] Shape after permuting for conv: torch.Size([32, 7, 120])
  [MFA Block] Input shape: torch.Size([32, 7, 120])
  [MFA Block] After global average pooling shape: torch.Size([32, 7, 1])
  [MFA Block] Attention weights shape: torch.Size([32, 7, 1])
  [MFA Block] Output after attention weighting shape: torch.Size([32, 7, 120])
[STFTnet] Shape after channel projection conv: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Input shape: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Output shape: torch.Size([32, 64, 120])
[STFTnet] Shape after permuting for transformer: torch.Size([32, 120, 64])
  [Transformer Encoder] Input shape (N, L, C): torch.Size([32, 120, 64])
  [Transformer Encoder] Output shape: torch.Size([32, 120, 64])
[STFTnet] Shape after taking last time step: torch.Size([32, 64])
  [Classifier] Input shape: torch.Size([32, 64])
  [Classifier] Output shape: torch.Size([32, 12])

[STFTnet] Initial input shape: torch.Size([32, 120, 7])
[STFTnet] Shape after permuting for conv: torch.Size([32, 7, 120])
  [MFA Block] Input shape: torch.Size([32, 7, 120])
  [MFA Block] After global average pooling shape: torch.Size([32, 7, 1])
  [MFA Block] Attention weights shape: torch.Size([32, 7, 1])
  [MFA Block] Output after attention weighting shape: torch.Size([32, 7, 120])
[STFTnet] Shape after channel projection conv: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Input shape: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Output shape: torch.Size([32, 64, 120])
[STFTnet] Shape after permuting for transformer: torch.Size([32, 120, 64])
  [Transformer Encoder] Input shape (N, L, C): torch.Size([32, 120, 64])
  [Transformer Encoder] Output shape: torch.Size([32, 120, 64])
[STFTnet] Shape after taking last time step: torch.Size([32, 64])
  [Classifier] Input shape: torch.Size([32, 64])
  [Classifier] Output shape: torch.Size([32, 12])

[STFTnet] Initial input shape: torch.Size([32, 120, 7])
[STFTnet] Shape after permuting for conv: torch.Size([32, 7, 120])
  [MFA Block] Input shape: torch.Size([32, 7, 120])
  [MFA Block] After global average pooling shape: torch.Size([32, 7, 1])
  [MFA Block] Attention weights shape: torch.Size([32, 7, 1])
  [MFA Block] Output after attention weighting shape: torch.Size([32, 7, 120])
[STFTnet] Shape after channel projection conv: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Input shape: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Output shape: torch.Size([32, 64, 120])
[STFTnet] Shape after permuting for transformer: torch.Size([32, 120, 64])
  [Transformer Encoder] Input shape (N, L, C): torch.Size([32, 120, 64])
  [Transformer Encoder] Output shape: torch.Size([32, 120, 64])
[STFTnet] Shape after taking last time step: torch.Size([32, 64])
  [Classifier] Input shape: torch.Size([32, 64])
  [Classifier] Output shape: torch.Size([32, 12])

[STFTnet] Initial input shape: torch.Size([32, 120, 7])
[STFTnet] Shape after permuting for conv: torch.Size([32, 7, 120])
  [MFA Block] Input shape: torch.Size([32, 7, 120])
  [MFA Block] After global average pooling shape: torch.Size([32, 7, 1])
  [MFA Block] Attention weights shape: torch.Size([32, 7, 1])
  [MFA Block] Output after attention weighting shape: torch.Size([32, 7, 120])
[STFTnet] Shape after channel projection conv: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Input shape: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Output shape: torch.Size([32, 64, 120])
[STFTnet] Shape after permuting for transformer: torch.Size([32, 120, 64])
  [Transformer Encoder] Input shape (N, L, C): torch.Size([32, 120, 64])
  [Transformer Encoder] Output shape: torch.Size([32, 120, 64])
[STFTnet] Shape after taking last time step: torch.Size([32, 64])
  [Classifier] Input shape: torch.Size([32, 64])
  [Classifier] Output shape: torch.Size([32, 12])

--- Completed a few batches for validation purposes; ending this epoch early. ---

========================================================
Epoch 47, Loss: 1.6223, Accuracy: 1.0000
========================================================


[STFTnet] Initial input shape: torch.Size([32, 120, 7])
[STFTnet] Shape after permuting for conv: torch.Size([32, 7, 120])
  [MFA Block] Input shape: torch.Size([32, 7, 120])
  [MFA Block] After global average pooling shape: torch.Size([32, 7, 1])
  [MFA Block] Attention weights shape: torch.Size([32, 7, 1])
  [MFA Block] Output after attention weighting shape: torch.Size([32, 7, 120])
[STFTnet] Shape after channel projection conv: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Input shape: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Output shape: torch.Size([32, 64, 120])
[STFTnet] Shape after permuting for transformer: torch.Size([32, 120, 64])
  [Transformer Encoder] Input shape (N, L, C): torch.Size([32, 120, 64])
  [Transformer Encoder] Output shape: torch.Size([32, 120, 64])
[STFTnet] Shape after taking last time step: torch.Size([32, 64])
  [Classifier] Input shape: torch.Size([32, 64])
  [Classifier] Output shape: torch.Size([32, 12])

[STFTnet] Initial input shape: torch.Size([32, 120, 7])
[STFTnet] Shape after permuting for conv: torch.Size([32, 7, 120])
  [MFA Block] Input shape: torch.Size([32, 7, 120])
  [MFA Block] After global average pooling shape: torch.Size([32, 7, 1])
  [MFA Block] Attention weights shape: torch.Size([32, 7, 1])
  [MFA Block] Output after attention weighting shape: torch.Size([32, 7, 120])
[STFTnet] Shape after channel projection conv: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Input shape: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Output shape: torch.Size([32, 64, 120])
[STFTnet] Shape after permuting for transformer: torch.Size([32, 120, 64])
  [Transformer Encoder] Input shape (N, L, C): torch.Size([32, 120, 64])
  [Transformer Encoder] Output shape: torch.Size([32, 120, 64])
[STFTnet] Shape after taking last time step: torch.Size([32, 64])
  [Classifier] Input shape: torch.Size([32, 64])
  [Classifier] Output shape: torch.Size([32, 12])

[STFTnet] Initial input shape: torch.Size([32, 120, 7])
[STFTnet] Shape after permuting for conv: torch.Size([32, 7, 120])
  [MFA Block] Input shape: torch.Size([32, 7, 120])
  [MFA Block] After global average pooling shape: torch.Size([32, 7, 1])
  [MFA Block] Attention weights shape: torch.Size([32, 7, 1])
  [MFA Block] Output after attention weighting shape: torch.Size([32, 7, 120])
[STFTnet] Shape after channel projection conv: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Input shape: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Output shape: torch.Size([32, 64, 120])
[STFTnet] Shape after permuting for transformer: torch.Size([32, 120, 64])
  [Transformer Encoder] Input shape (N, L, C): torch.Size([32, 120, 64])
  [Transformer Encoder] Output shape: torch.Size([32, 120, 64])
[STFTnet] Shape after taking last time step: torch.Size([32, 64])
  [Classifier] Input shape: torch.Size([32, 64])
  [Classifier] Output shape: torch.Size([32, 12])

[STFTnet] Initial input shape: torch.Size([32, 120, 7])
[STFTnet] Shape after permuting for conv: torch.Size([32, 7, 120])
  [MFA Block] Input shape: torch.Size([32, 7, 120])
  [MFA Block] After global average pooling shape: torch.Size([32, 7, 1])
  [MFA Block] Attention weights shape: torch.Size([32, 7, 1])
  [MFA Block] Output after attention weighting shape: torch.Size([32, 7, 120])
[STFTnet] Shape after channel projection conv: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Input shape: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Output shape: torch.Size([32, 64, 120])
[STFTnet] Shape after permuting for transformer: torch.Size([32, 120, 64])
  [Transformer Encoder] Input shape (N, L, C): torch.Size([32, 120, 64])
  [Transformer Encoder] Output shape: torch.Size([32, 120, 64])
[STFTnet] Shape after taking last time step: torch.Size([32, 64])
  [Classifier] Input shape: torch.Size([32, 64])
  [Classifier] Output shape: torch.Size([32, 12])

[STFTnet] Initial input shape: torch.Size([32, 120, 7])
[STFTnet] Shape after permuting for conv: torch.Size([32, 7, 120])
  [MFA Block] Input shape: torch.Size([32, 7, 120])
  [MFA Block] After global average pooling shape: torch.Size([32, 7, 1])
  [MFA Block] Attention weights shape: torch.Size([32, 7, 1])
  [MFA Block] Output after attention weighting shape: torch.Size([32, 7, 120])
[STFTnet] Shape after channel projection conv: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Input shape: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Output shape: torch.Size([32, 64, 120])
[STFTnet] Shape after permuting for transformer: torch.Size([32, 120, 64])
  [Transformer Encoder] Input shape (N, L, C): torch.Size([32, 120, 64])
  [Transformer Encoder] Output shape: torch.Size([32, 120, 64])
[STFTnet] Shape after taking last time step: torch.Size([32, 64])
  [Classifier] Input shape: torch.Size([32, 64])
  [Classifier] Output shape: torch.Size([32, 12])

[STFTnet] Initial input shape: torch.Size([32, 120, 7])
[STFTnet] Shape after permuting for conv: torch.Size([32, 7, 120])
  [MFA Block] Input shape: torch.Size([32, 7, 120])
  [MFA Block] After global average pooling shape: torch.Size([32, 7, 1])
  [MFA Block] Attention weights shape: torch.Size([32, 7, 1])
  [MFA Block] Output after attention weighting shape: torch.Size([32, 7, 120])
[STFTnet] Shape after channel projection conv: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Input shape: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Output shape: torch.Size([32, 64, 120])
[STFTnet] Shape after permuting for transformer: torch.Size([32, 120, 64])
  [Transformer Encoder] Input shape (N, L, C): torch.Size([32, 120, 64])
  [Transformer Encoder] Output shape: torch.Size([32, 120, 64])
[STFTnet] Shape after taking last time step: torch.Size([32, 64])
  [Classifier] Input shape: torch.Size([32, 64])
  [Classifier] Output shape: torch.Size([32, 12])

--- Completed a few batches for validation purposes; ending this epoch early. ---

========================================================
Epoch 48, Loss: 1.6226, Accuracy: 1.0000
========================================================


[STFTnet] Initial input shape: torch.Size([32, 120, 7])
[STFTnet] Shape after permuting for conv: torch.Size([32, 7, 120])
  [MFA Block] Input shape: torch.Size([32, 7, 120])
  [MFA Block] After global average pooling shape: torch.Size([32, 7, 1])
  [MFA Block] Attention weights shape: torch.Size([32, 7, 1])
  [MFA Block] Output after attention weighting shape: torch.Size([32, 7, 120])
[STFTnet] Shape after channel projection conv: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Input shape: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Output shape: torch.Size([32, 64, 120])
[STFTnet] Shape after permuting for transformer: torch.Size([32, 120, 64])
  [Transformer Encoder] Input shape (N, L, C): torch.Size([32, 120, 64])
  [Transformer Encoder] Output shape: torch.Size([32, 120, 64])
[STFTnet] Shape after taking last time step: torch.Size([32, 64])
  [Classifier] Input shape: torch.Size([32, 64])
  [Classifier] Output shape: torch.Size([32, 12])

[STFTnet] Initial input shape: torch.Size([32, 120, 7])
[STFTnet] Shape after permuting for conv: torch.Size([32, 7, 120])
  [MFA Block] Input shape: torch.Size([32, 7, 120])
  [MFA Block] After global average pooling shape: torch.Size([32, 7, 1])
  [MFA Block] Attention weights shape: torch.Size([32, 7, 1])
  [MFA Block] Output after attention weighting shape: torch.Size([32, 7, 120])
[STFTnet] Shape after channel projection conv: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Input shape: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Output shape: torch.Size([32, 64, 120])
[STFTnet] Shape after permuting for transformer: torch.Size([32, 120, 64])
  [Transformer Encoder] Input shape (N, L, C): torch.Size([32, 120, 64])
  [Transformer Encoder] Output shape: torch.Size([32, 120, 64])
[STFTnet] Shape after taking last time step: torch.Size([32, 64])
  [Classifier] Input shape: torch.Size([32, 64])
  [Classifier] Output shape: torch.Size([32, 12])

[STFTnet] Initial input shape: torch.Size([32, 120, 7])
[STFTnet] Shape after permuting for conv: torch.Size([32, 7, 120])
  [MFA Block] Input shape: torch.Size([32, 7, 120])
  [MFA Block] After global average pooling shape: torch.Size([32, 7, 1])
  [MFA Block] Attention weights shape: torch.Size([32, 7, 1])
  [MFA Block] Output after attention weighting shape: torch.Size([32, 7, 120])
[STFTnet] Shape after channel projection conv: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Input shape: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Output shape: torch.Size([32, 64, 120])
[STFTnet] Shape after permuting for transformer: torch.Size([32, 120, 64])
  [Transformer Encoder] Input shape (N, L, C): torch.Size([32, 120, 64])
  [Transformer Encoder] Output shape: torch.Size([32, 120, 64])
[STFTnet] Shape after taking last time step: torch.Size([32, 64])
  [Classifier] Input shape: torch.Size([32, 64])
  [Classifier] Output shape: torch.Size([32, 12])

[STFTnet] Initial input shape: torch.Size([32, 120, 7])
[STFTnet] Shape after permuting for conv: torch.Size([32, 7, 120])
  [MFA Block] Input shape: torch.Size([32, 7, 120])
  [MFA Block] After global average pooling shape: torch.Size([32, 7, 1])
  [MFA Block] Attention weights shape: torch.Size([32, 7, 1])
  [MFA Block] Output after attention weighting shape: torch.Size([32, 7, 120])
[STFTnet] Shape after channel projection conv: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Input shape: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Output shape: torch.Size([32, 64, 120])
[STFTnet] Shape after permuting for transformer: torch.Size([32, 120, 64])
  [Transformer Encoder] Input shape (N, L, C): torch.Size([32, 120, 64])
  [Transformer Encoder] Output shape: torch.Size([32, 120, 64])
[STFTnet] Shape after taking last time step: torch.Size([32, 64])
  [Classifier] Input shape: torch.Size([32, 64])
  [Classifier] Output shape: torch.Size([32, 12])

[STFTnet] Initial input shape: torch.Size([32, 120, 7])
[STFTnet] Shape after permuting for conv: torch.Size([32, 7, 120])
  [MFA Block] Input shape: torch.Size([32, 7, 120])
  [MFA Block] After global average pooling shape: torch.Size([32, 7, 1])
  [MFA Block] Attention weights shape: torch.Size([32, 7, 1])
  [MFA Block] Output after attention weighting shape: torch.Size([32, 7, 120])
[STFTnet] Shape after channel projection conv: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Input shape: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Output shape: torch.Size([32, 64, 120])
[STFTnet] Shape after permuting for transformer: torch.Size([32, 120, 64])
  [Transformer Encoder] Input shape (N, L, C): torch.Size([32, 120, 64])
  [Transformer Encoder] Output shape: torch.Size([32, 120, 64])
[STFTnet] Shape after taking last time step: torch.Size([32, 64])
  [Classifier] Input shape: torch.Size([32, 64])
  [Classifier] Output shape: torch.Size([32, 12])

[STFTnet] Initial input shape: torch.Size([32, 120, 7])
[STFTnet] Shape after permuting for conv: torch.Size([32, 7, 120])
  [MFA Block] Input shape: torch.Size([32, 7, 120])
  [MFA Block] After global average pooling shape: torch.Size([32, 7, 1])
  [MFA Block] Attention weights shape: torch.Size([32, 7, 1])
  [MFA Block] Output after attention weighting shape: torch.Size([32, 7, 120])
[STFTnet] Shape after channel projection conv: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Input shape: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Output shape: torch.Size([32, 64, 120])
[STFTnet] Shape after permuting for transformer: torch.Size([32, 120, 64])
  [Transformer Encoder] Input shape (N, L, C): torch.Size([32, 120, 64])
  [Transformer Encoder] Output shape: torch.Size([32, 120, 64])
[STFTnet] Shape after taking last time step: torch.Size([32, 64])
  [Classifier] Input shape: torch.Size([32, 64])
  [Classifier] Output shape: torch.Size([32, 12])

--- Completed a few batches for validation purposes; ending this epoch early. ---

========================================================
Epoch 49, Loss: 1.6224, Accuracy: 1.0000
========================================================


[STFTnet] Initial input shape: torch.Size([32, 120, 7])
[STFTnet] Shape after permuting for conv: torch.Size([32, 7, 120])
  [MFA Block] Input shape: torch.Size([32, 7, 120])
  [MFA Block] After global average pooling shape: torch.Size([32, 7, 1])
  [MFA Block] Attention weights shape: torch.Size([32, 7, 1])
  [MFA Block] Output after attention weighting shape: torch.Size([32, 7, 120])
[STFTnet] Shape after channel projection conv: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Input shape: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Output shape: torch.Size([32, 64, 120])
[STFTnet] Shape after permuting for transformer: torch.Size([32, 120, 64])
  [Transformer Encoder] Input shape (N, L, C): torch.Size([32, 120, 64])
  [Transformer Encoder] Output shape: torch.Size([32, 120, 64])
[STFTnet] Shape after taking last time step: torch.Size([32, 64])
  [Classifier] Input shape: torch.Size([32, 64])
  [Classifier] Output shape: torch.Size([32, 12])

[STFTnet] Initial input shape: torch.Size([32, 120, 7])
[STFTnet] Shape after permuting for conv: torch.Size([32, 7, 120])
  [MFA Block] Input shape: torch.Size([32, 7, 120])
  [MFA Block] After global average pooling shape: torch.Size([32, 7, 1])
  [MFA Block] Attention weights shape: torch.Size([32, 7, 1])
  [MFA Block] Output after attention weighting shape: torch.Size([32, 7, 120])
[STFTnet] Shape after channel projection conv: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Input shape: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Output shape: torch.Size([32, 64, 120])
[STFTnet] Shape after permuting for transformer: torch.Size([32, 120, 64])
  [Transformer Encoder] Input shape (N, L, C): torch.Size([32, 120, 64])
  [Transformer Encoder] Output shape: torch.Size([32, 120, 64])
[STFTnet] Shape after taking last time step: torch.Size([32, 64])
  [Classifier] Input shape: torch.Size([32, 64])
  [Classifier] Output shape: torch.Size([32, 12])

[STFTnet] Initial input shape: torch.Size([32, 120, 7])
[STFTnet] Shape after permuting for conv: torch.Size([32, 7, 120])
  [MFA Block] Input shape: torch.Size([32, 7, 120])
  [MFA Block] After global average pooling shape: torch.Size([32, 7, 1])
  [MFA Block] Attention weights shape: torch.Size([32, 7, 1])
  [MFA Block] Output after attention weighting shape: torch.Size([32, 7, 120])
[STFTnet] Shape after channel projection conv: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Input shape: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Output shape: torch.Size([32, 64, 120])
[STFTnet] Shape after permuting for transformer: torch.Size([32, 120, 64])
  [Transformer Encoder] Input shape (N, L, C): torch.Size([32, 120, 64])
  [Transformer Encoder] Output shape: torch.Size([32, 120, 64])
[STFTnet] Shape after taking last time step: torch.Size([32, 64])
  [Classifier] Input shape: torch.Size([32, 64])
  [Classifier] Output shape: torch.Size([32, 12])

[STFTnet] Initial input shape: torch.Size([32, 120, 7])
[STFTnet] Shape after permuting for conv: torch.Size([32, 7, 120])
  [MFA Block] Input shape: torch.Size([32, 7, 120])
  [MFA Block] After global average pooling shape: torch.Size([32, 7, 1])
  [MFA Block] Attention weights shape: torch.Size([32, 7, 1])
  [MFA Block] Output after attention weighting shape: torch.Size([32, 7, 120])
[STFTnet] Shape after channel projection conv: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Input shape: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Output shape: torch.Size([32, 64, 120])
[STFTnet] Shape after permuting for transformer: torch.Size([32, 120, 64])
  [Transformer Encoder] Input shape (N, L, C): torch.Size([32, 120, 64])
  [Transformer Encoder] Output shape: torch.Size([32, 120, 64])
[STFTnet] Shape after taking last time step: torch.Size([32, 64])
  [Classifier] Input shape: torch.Size([32, 64])
  [Classifier] Output shape: torch.Size([32, 12])

[STFTnet] Initial input shape: torch.Size([32, 120, 7])
[STFTnet] Shape after permuting for conv: torch.Size([32, 7, 120])
  [MFA Block] Input shape: torch.Size([32, 7, 120])
  [MFA Block] After global average pooling shape: torch.Size([32, 7, 1])
  [MFA Block] Attention weights shape: torch.Size([32, 7, 1])
  [MFA Block] Output after attention weighting shape: torch.Size([32, 7, 120])
[STFTnet] Shape after channel projection conv: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Input shape: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Output shape: torch.Size([32, 64, 120])
[STFTnet] Shape after permuting for transformer: torch.Size([32, 120, 64])
  [Transformer Encoder] Input shape (N, L, C): torch.Size([32, 120, 64])
  [Transformer Encoder] Output shape: torch.Size([32, 120, 64])
[STFTnet] Shape after taking last time step: torch.Size([32, 64])
  [Classifier] Input shape: torch.Size([32, 64])
  [Classifier] Output shape: torch.Size([32, 12])

[STFTnet] Initial input shape: torch.Size([32, 120, 7])
[STFTnet] Shape after permuting for conv: torch.Size([32, 7, 120])
  [MFA Block] Input shape: torch.Size([32, 7, 120])
  [MFA Block] After global average pooling shape: torch.Size([32, 7, 1])
  [MFA Block] Attention weights shape: torch.Size([32, 7, 1])
  [MFA Block] Output after attention weighting shape: torch.Size([32, 7, 120])
[STFTnet] Shape after channel projection conv: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Input shape: torch.Size([32, 64, 120])
  [Depthwise Separable Conv] Output shape: torch.Size([32, 64, 120])
[STFTnet] Shape after permuting for transformer: torch.Size([32, 120, 64])
  [Transformer Encoder] Input shape (N, L, C): torch.Size([32, 120, 64])
  [Transformer Encoder] Output shape: torch.Size([32, 120, 64])
[STFTnet] Shape after taking last time step: torch.Size([32, 64])
  [Classifier] Input shape: torch.Size([32, 64])
  [Classifier] Output shape: torch.Size([32, 12])

--- Completed a few batches for validation purposes; ending this epoch early. ---

========================================================
Epoch 120, Loss: 1.6223, Accuracy: 1.0000
========================================================

